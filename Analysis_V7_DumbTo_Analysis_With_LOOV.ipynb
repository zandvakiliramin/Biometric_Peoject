{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba3011c4",
   "metadata": {},
   "source": [
    "## Analysis with \"DumbTo\" Stimulus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9fd8254",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "import numpy as np\n",
    "import numpy.ma as ma\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import plot_precision_recall_curve\n",
    "\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "from itertools import cycle\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    " import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "b58b114b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0834ec61",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sub_DumbTo = pd.read_csv(\"./Data/Subsamples/DumbTo.csv\")\n",
    "\n",
    "# For RPD and SPD we can use the raw or normed versions, which one?\n",
    "related_features = ['Subject_ID',\n",
    "       'FPOGX', 'FPOGY', \n",
    "       'BPOGX', 'BPOGY', \n",
    "       'LPCX', 'LPCY', 'Normed_LPD', 'Normed_LPS', 'RPCX', 'RPCY', 'Normed_RPD', 'Normed_RPS', \n",
    "       'LPUPILD', 'RPUPILD', \n",
    "       'Anger_Evidence', 'Contempt_Evidence', 'Disgust_Evidence', 'Joy_Evidence', 'Fear_Evidence',\n",
    "       'Negative_Evidence', 'Neutral_Evidence', 'Positive_Evidence', 'Sadness_Evidence', 'Surprise_Evidence', \n",
    "       'Normed_Heart_Rate']\n",
    "\n",
    "data_recall = pd.read_csv('./Data/Lab_Recall_Hand_Edited/SuperMovie1516_L.csv')\n",
    "\n",
    "idx_to_col_name = {k: v for k, v in enumerate(related_features[1:])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b926399",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of subjects: 45\n",
      "Min sequence lengh:  9676\n"
     ]
    }
   ],
   "source": [
    "seq_length = sub_DumbTo.groupby(\"Subject_ID\").size().reset_index()\n",
    "seq_length.columns = [\"Subject_ID\", \"Length\"]\n",
    "num_of_subjects = seq_length.shape[0]\n",
    "print(\"Number of subjects:\", num_of_subjects)\n",
    "\n",
    "print(\"Min sequence lengh: \",seq_length.Length.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a727be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove user 344 from seq-length and data (length is outlier)\n",
    "seq_length = seq_length[seq_length.Subject_ID != 344]\n",
    "sub_DumbTo = sub_DumbTo[sub_DumbTo.Subject_ID != 344]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3b4b2f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of subjects: 44\n"
     ]
    }
   ],
   "source": [
    "num_of_subjects = seq_length.shape[0]\n",
    "print(\"Number of subjects:\", num_of_subjects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "74ddbb8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cutoff is : 9744\n"
     ]
    }
   ],
   "source": [
    "cut_off = seq_length.Length.min()\n",
    "print(\"Cutoff is :\", cut_off)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "adf1ed2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sqeuence length after adjustment: 9741.0\n"
     ]
    }
   ],
   "source": [
    "seq_length['Sum_Length'] = seq_length.Length.cumsum() \n",
    "seq_length = seq_length.reset_index()\n",
    "\n",
    "# Adjust the length of all seqs\n",
    "#cut_off = seq_length.Length.min()\n",
    "cut_off = 9741\n",
    "\n",
    "rem_index = np.zeros(sub_DumbTo.shape[0])\n",
    "rem_index[0 : cut_off] = 1  \n",
    "for i in range(0, seq_length.shape[0]-1):\n",
    "    rem_index[seq_length.Sum_Length[i] : seq_length.Sum_Length[i]  + cut_off] = 1\n",
    "    \n",
    "sub_DumbTo[\"Rem_Index\"] = rem_index\n",
    "\n",
    "sub_DumbTo = sub_DumbTo[sub_DumbTo.Rem_Index == 1]\n",
    "\n",
    "print(\"Sqeuence length after adjustment:\", sub_DumbTo.shape[0]/44)\n",
    "\n",
    "sub_DumbTo = sub_DumbTo[related_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d6f1a79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_non_zero_nulls_in_cols = sub_DumbTo.isnull().sum().reset_index()\n",
    "num_non_zero_nulls_in_cols.columns = [\"Column\", \"Num_Nulls\"]\n",
    "num_non_zero_nulls_in_cols = num_non_zero_nulls_in_cols[num_non_zero_nulls_in_cols.Num_Nulls != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aa3a9cc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Column</th>\n",
       "      <th>Num_Nulls</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Normed_LPS</td>\n",
       "      <td>9784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Normed_RPS</td>\n",
       "      <td>9784</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Column  Num_Nulls\n",
       "8   Normed_LPS       9784\n",
       "12  Normed_RPS       9784"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In the data one user has no value for the above columns and the first value of all users is null\n",
    "num_non_zero_nulls_in_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8e4e9060",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence length (cutt_off) equls:  9740\n",
      "Number of Subjects is:  43\n"
     ]
    }
   ],
   "source": [
    "# remove user 345 from seq-length and data (has missing values for 4 columns)\n",
    "seq_length = seq_length[seq_length.Subject_ID != 345]\n",
    "sub_DumbTo = sub_DumbTo[~sub_DumbTo.Normed_LPS.isna()]\n",
    "# We can remove all nulls and make the sequnce one unit smaller\n",
    "sub_DumbTo = sub_DumbTo.dropna()\n",
    "\n",
    "# For the above four columns the first element of each sequence was na, so the sequence length (cut_off) is reduced by 1\n",
    "cut_off = cut_off - 1\n",
    "num_of_subjects = int(sub_DumbTo.shape[0]/(cut_off))\n",
    "print(\"Sequence length (cutt_off) equls: \", cut_off)\n",
    "print(\"Number of Subjects is: \", num_of_subjects)\n",
    "\n",
    "sub_DumbTo = sub_DumbTo.reset_index()\n",
    "sub_DumbTo = sub_DumbTo.drop(columns = [\"index\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5df00fa9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(y): 43\n"
     ]
    }
   ],
   "source": [
    "y = [data_recall[data_recall.ExternalReference == i].DumbTo.tolist()[0] for i in seq_length.Subject_ID]\n",
    "y = [1 if i>5 else 0 for i in y]\n",
    "y = np.array(y)\n",
    "\n",
    "id_to_y = {}\n",
    "for i in seq_length.Subject_ID:\n",
    "    if data_recall[data_recall.ExternalReference == i].DumbTo.tolist()[0] > 5:\n",
    "        id_to_y[i] = 1\n",
    "    else:\n",
    "        id_to_y[i] = 0\n",
    "#id_to_y = {i: data_recall[data_recall.ExternalReference == i].DumbTo.tolist()[0] for i in seq_length.Subject_ID}\n",
    "\n",
    "\n",
    "print('len(y):', len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "92646d1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(y==1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f53d5d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_DumbTo = sub_DumbTo.reset_index()\n",
    "sub_DumbTo[\"index\"] = sub_DumbTo[\"index\"] + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4138e0c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Subject_ID</th>\n",
       "      <th>FPOGX</th>\n",
       "      <th>FPOGY</th>\n",
       "      <th>BPOGX</th>\n",
       "      <th>BPOGY</th>\n",
       "      <th>LPCX</th>\n",
       "      <th>LPCY</th>\n",
       "      <th>Normed_LPD</th>\n",
       "      <th>Normed_LPS</th>\n",
       "      <th>...</th>\n",
       "      <th>Contempt_Evidence</th>\n",
       "      <th>Disgust_Evidence</th>\n",
       "      <th>Joy_Evidence</th>\n",
       "      <th>Fear_Evidence</th>\n",
       "      <th>Negative_Evidence</th>\n",
       "      <th>Neutral_Evidence</th>\n",
       "      <th>Positive_Evidence</th>\n",
       "      <th>Sadness_Evidence</th>\n",
       "      <th>Surprise_Evidence</th>\n",
       "      <th>Normed_Heart_Rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>300</td>\n",
       "      <td>0.68073</td>\n",
       "      <td>0.42222</td>\n",
       "      <td>0.68802</td>\n",
       "      <td>0.39259</td>\n",
       "      <td>0.25017</td>\n",
       "      <td>0.41186</td>\n",
       "      <td>1.808046</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.261916</td>\n",
       "      <td>-1.104235</td>\n",
       "      <td>-3.085957</td>\n",
       "      <td>-0.276813</td>\n",
       "      <td>2.562142</td>\n",
       "      <td>0.683569</td>\n",
       "      <td>-1.535620</td>\n",
       "      <td>1.238940</td>\n",
       "      <td>-0.985122</td>\n",
       "      <td>3.416550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>300</td>\n",
       "      <td>0.68125</td>\n",
       "      <td>0.42685</td>\n",
       "      <td>0.68750</td>\n",
       "      <td>0.49259</td>\n",
       "      <td>0.25011</td>\n",
       "      <td>0.41384</td>\n",
       "      <td>-0.821955</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.261916</td>\n",
       "      <td>-1.104235</td>\n",
       "      <td>-3.085957</td>\n",
       "      <td>-0.276813</td>\n",
       "      <td>2.562142</td>\n",
       "      <td>0.683569</td>\n",
       "      <td>-1.535620</td>\n",
       "      <td>1.238940</td>\n",
       "      <td>-0.985122</td>\n",
       "      <td>3.416550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>300</td>\n",
       "      <td>0.68125</td>\n",
       "      <td>0.43241</td>\n",
       "      <td>0.68594</td>\n",
       "      <td>0.50556</td>\n",
       "      <td>0.25011</td>\n",
       "      <td>0.41384</td>\n",
       "      <td>-0.821955</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.261916</td>\n",
       "      <td>-1.104235</td>\n",
       "      <td>-3.085957</td>\n",
       "      <td>-0.276813</td>\n",
       "      <td>2.562142</td>\n",
       "      <td>0.683569</td>\n",
       "      <td>-1.535620</td>\n",
       "      <td>1.238940</td>\n",
       "      <td>-0.985122</td>\n",
       "      <td>3.416550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>300</td>\n",
       "      <td>0.87396</td>\n",
       "      <td>0.43611</td>\n",
       "      <td>2.58698</td>\n",
       "      <td>0.46574</td>\n",
       "      <td>0.24719</td>\n",
       "      <td>0.40959</td>\n",
       "      <td>-12.431953</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.261916</td>\n",
       "      <td>-1.104235</td>\n",
       "      <td>-3.085957</td>\n",
       "      <td>-0.276813</td>\n",
       "      <td>2.562142</td>\n",
       "      <td>0.683569</td>\n",
       "      <td>-1.535620</td>\n",
       "      <td>1.238940</td>\n",
       "      <td>-0.985122</td>\n",
       "      <td>3.416550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>300</td>\n",
       "      <td>0.87396</td>\n",
       "      <td>0.43611</td>\n",
       "      <td>2.58698</td>\n",
       "      <td>0.46574</td>\n",
       "      <td>0.06935</td>\n",
       "      <td>0.33554</td>\n",
       "      <td>-11.791954</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.261916</td>\n",
       "      <td>-1.104235</td>\n",
       "      <td>-3.085957</td>\n",
       "      <td>-0.276813</td>\n",
       "      <td>2.562142</td>\n",
       "      <td>0.683569</td>\n",
       "      <td>-1.535620</td>\n",
       "      <td>1.238940</td>\n",
       "      <td>-0.985122</td>\n",
       "      <td>3.416550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>418815</th>\n",
       "      <td>418816</td>\n",
       "      <td>365</td>\n",
       "      <td>0.37031</td>\n",
       "      <td>0.39815</td>\n",
       "      <td>0.39792</td>\n",
       "      <td>0.38333</td>\n",
       "      <td>0.26668</td>\n",
       "      <td>0.72261</td>\n",
       "      <td>-2.647524</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.407128</td>\n",
       "      <td>-0.156432</td>\n",
       "      <td>-1.473553</td>\n",
       "      <td>-1.841570</td>\n",
       "      <td>2.932494</td>\n",
       "      <td>-1.194771</td>\n",
       "      <td>-0.932807</td>\n",
       "      <td>-1.932029</td>\n",
       "      <td>-1.989926</td>\n",
       "      <td>-40.433041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>418816</th>\n",
       "      <td>418817</td>\n",
       "      <td>365</td>\n",
       "      <td>0.37187</td>\n",
       "      <td>0.39907</td>\n",
       "      <td>0.39323</td>\n",
       "      <td>0.40648</td>\n",
       "      <td>0.26660</td>\n",
       "      <td>0.72223</td>\n",
       "      <td>-2.797525</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.537363</td>\n",
       "      <td>-0.162792</td>\n",
       "      <td>-1.135028</td>\n",
       "      <td>-1.760911</td>\n",
       "      <td>2.925890</td>\n",
       "      <td>-1.501126</td>\n",
       "      <td>-0.740708</td>\n",
       "      <td>-2.024093</td>\n",
       "      <td>-1.986524</td>\n",
       "      <td>-40.433041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>418817</th>\n",
       "      <td>418818</td>\n",
       "      <td>365</td>\n",
       "      <td>0.37240</td>\n",
       "      <td>0.40093</td>\n",
       "      <td>0.37865</td>\n",
       "      <td>0.42963</td>\n",
       "      <td>0.26636</td>\n",
       "      <td>0.72297</td>\n",
       "      <td>-3.027524</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.537363</td>\n",
       "      <td>-0.162792</td>\n",
       "      <td>-1.135028</td>\n",
       "      <td>-1.760911</td>\n",
       "      <td>2.925890</td>\n",
       "      <td>-1.501126</td>\n",
       "      <td>-0.740708</td>\n",
       "      <td>-2.024093</td>\n",
       "      <td>-1.986524</td>\n",
       "      <td>-40.433041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>418818</th>\n",
       "      <td>418819</td>\n",
       "      <td>365</td>\n",
       "      <td>0.37396</td>\n",
       "      <td>0.39815</td>\n",
       "      <td>0.39010</td>\n",
       "      <td>0.36019</td>\n",
       "      <td>0.26663</td>\n",
       "      <td>0.72167</td>\n",
       "      <td>-3.217525</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.537363</td>\n",
       "      <td>-0.162792</td>\n",
       "      <td>-1.135028</td>\n",
       "      <td>-1.760911</td>\n",
       "      <td>2.925890</td>\n",
       "      <td>-1.501126</td>\n",
       "      <td>-0.740708</td>\n",
       "      <td>-2.024093</td>\n",
       "      <td>-1.986524</td>\n",
       "      <td>-40.433041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>418819</th>\n",
       "      <td>418820</td>\n",
       "      <td>365</td>\n",
       "      <td>0.37448</td>\n",
       "      <td>0.39722</td>\n",
       "      <td>0.38646</td>\n",
       "      <td>0.37500</td>\n",
       "      <td>0.26653</td>\n",
       "      <td>0.72194</td>\n",
       "      <td>-2.657524</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.537363</td>\n",
       "      <td>-0.162792</td>\n",
       "      <td>-1.135028</td>\n",
       "      <td>-1.760911</td>\n",
       "      <td>2.925890</td>\n",
       "      <td>-1.501126</td>\n",
       "      <td>-0.740708</td>\n",
       "      <td>-2.024093</td>\n",
       "      <td>-1.986524</td>\n",
       "      <td>-40.433041</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>418820 rows Ã— 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         index  Subject_ID    FPOGX    FPOGY    BPOGX    BPOGY     LPCX  \\\n",
       "0            1         300  0.68073  0.42222  0.68802  0.39259  0.25017   \n",
       "1            2         300  0.68125  0.42685  0.68750  0.49259  0.25011   \n",
       "2            3         300  0.68125  0.43241  0.68594  0.50556  0.25011   \n",
       "3            4         300  0.87396  0.43611  2.58698  0.46574  0.24719   \n",
       "4            5         300  0.87396  0.43611  2.58698  0.46574  0.06935   \n",
       "...        ...         ...      ...      ...      ...      ...      ...   \n",
       "418815  418816         365  0.37031  0.39815  0.39792  0.38333  0.26668   \n",
       "418816  418817         365  0.37187  0.39907  0.39323  0.40648  0.26660   \n",
       "418817  418818         365  0.37240  0.40093  0.37865  0.42963  0.26636   \n",
       "418818  418819         365  0.37396  0.39815  0.39010  0.36019  0.26663   \n",
       "418819  418820         365  0.37448  0.39722  0.38646  0.37500  0.26653   \n",
       "\n",
       "           LPCY  Normed_LPD  Normed_LPS  ...  Contempt_Evidence  \\\n",
       "0       0.41186    1.808046         0.0  ...          -0.261916   \n",
       "1       0.41384   -0.821955         0.0  ...          -0.261916   \n",
       "2       0.41384   -0.821955         1.0  ...          -0.261916   \n",
       "3       0.40959  -12.431953         0.0  ...          -0.261916   \n",
       "4       0.33554  -11.791954         0.0  ...          -0.261916   \n",
       "...         ...         ...         ...  ...                ...   \n",
       "418815  0.72261   -2.647524         0.0  ...          -0.407128   \n",
       "418816  0.72223   -2.797525         0.0  ...          -0.537363   \n",
       "418817  0.72297   -3.027524         0.0  ...          -0.537363   \n",
       "418818  0.72167   -3.217525         0.0  ...          -0.537363   \n",
       "418819  0.72194   -2.657524         0.0  ...          -0.537363   \n",
       "\n",
       "        Disgust_Evidence  Joy_Evidence  Fear_Evidence  Negative_Evidence  \\\n",
       "0              -1.104235     -3.085957      -0.276813           2.562142   \n",
       "1              -1.104235     -3.085957      -0.276813           2.562142   \n",
       "2              -1.104235     -3.085957      -0.276813           2.562142   \n",
       "3              -1.104235     -3.085957      -0.276813           2.562142   \n",
       "4              -1.104235     -3.085957      -0.276813           2.562142   \n",
       "...                  ...           ...            ...                ...   \n",
       "418815         -0.156432     -1.473553      -1.841570           2.932494   \n",
       "418816         -0.162792     -1.135028      -1.760911           2.925890   \n",
       "418817         -0.162792     -1.135028      -1.760911           2.925890   \n",
       "418818         -0.162792     -1.135028      -1.760911           2.925890   \n",
       "418819         -0.162792     -1.135028      -1.760911           2.925890   \n",
       "\n",
       "        Neutral_Evidence  Positive_Evidence  Sadness_Evidence  \\\n",
       "0               0.683569          -1.535620          1.238940   \n",
       "1               0.683569          -1.535620          1.238940   \n",
       "2               0.683569          -1.535620          1.238940   \n",
       "3               0.683569          -1.535620          1.238940   \n",
       "4               0.683569          -1.535620          1.238940   \n",
       "...                  ...                ...               ...   \n",
       "418815         -1.194771          -0.932807         -1.932029   \n",
       "418816         -1.501126          -0.740708         -2.024093   \n",
       "418817         -1.501126          -0.740708         -2.024093   \n",
       "418818         -1.501126          -0.740708         -2.024093   \n",
       "418819         -1.501126          -0.740708         -2.024093   \n",
       "\n",
       "        Surprise_Evidence  Normed_Heart_Rate  \n",
       "0               -0.985122           3.416550  \n",
       "1               -0.985122           3.416550  \n",
       "2               -0.985122           3.416550  \n",
       "3               -0.985122           3.416550  \n",
       "4               -0.985122           3.416550  \n",
       "...                   ...                ...  \n",
       "418815          -1.989926         -40.433041  \n",
       "418816          -1.986524         -40.433041  \n",
       "418817          -1.986524         -40.433041  \n",
       "418818          -1.986524         -40.433041  \n",
       "418819          -1.986524         -40.433041  \n",
       "\n",
       "[418820 rows x 27 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_DumbTo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c78ae983",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_raw = np.array(sub_DumbTo.iloc[:,2:])\n",
    "x_raw = x_raw.reshape(num_of_subjects, cut_off, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "53438f2d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(43, 9740, 25)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_raw.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c2c20776",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "42 - 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1fded42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_DumbTo = sub_DumbTo.drop(columns=[\"index\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "506c3774",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-81-9a7cab49b54f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     54\u001b[0m     \u001b[0munique_training_subject_ids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0munique_training_subject_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 56\u001b[1;33m     \u001b[0mid_to_y_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0m_id\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0my_res\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0m_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0munique_training_subject_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     57\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mgroup_1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx_train_df_tot\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx_train_df_tot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m4\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-81-9a7cab49b54f>\u001b[0m in \u001b[0;36m<dictcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     54\u001b[0m     \u001b[0munique_training_subject_ids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0munique_training_subject_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 56\u001b[1;33m     \u001b[0mid_to_y_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0m_id\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0my_res\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0m_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0munique_training_subject_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     57\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mgroup_1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx_train_df_tot\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx_train_df_tot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m4\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices"
     ]
    }
   ],
   "source": [
    "\n",
    "for i, y_i in enumerate(y):\n",
    "    mask = np.repeat(True, len(y))\n",
    "    mask[i] = False\n",
    "    \n",
    "    y_train = y[mask]   \n",
    "    x_train = x_raw[mask]\n",
    "    \n",
    "    y_valid = y[i]\n",
    "    x_valid = x_raw[i]\n",
    "    \n",
    "    x_for_smote = np.zeros((x_train.shape[0], x_train.shape[1] * x_train.shape[2]))\n",
    "    for i, seq_feat_mat in enumerate(x_train):\n",
    "        x_for_smote[i] = np.reshape(seq_feat_mat, x_train.shape[1] * x_train.shape[2])\n",
    "        \n",
    "    strategy = {0:12, 1:36} if y_i==1 else {0:11, 1:37}\n",
    "    oversample = SMOTE(sampling_strategy=strategy, random_state=0, k_neighbors=3)\n",
    "    x_res, y_res = oversample.fit_resample(x_for_smote, y_train)\n",
    "    \n",
    "    x_res_added = x_res[42:,]\n",
    "#     y_res_added = y_res[42:,]\n",
    "\n",
    "    x_valid_df = sub_DumbTo.iloc[i : i+cut_off].reset_index()\n",
    "    x_valid_df = x_valid_df.drop(columns=[\"index\"])\n",
    "    x_train_df = sub_DumbTo.drop(sub_DumbTo.index[i : i+cut_off], axis=0).reset_index()\n",
    "    x_train_df = x_train_df.drop(columns=[\"index\"])\n",
    "    \n",
    "    x_added_df = pd.DataFrame(columns=related_features)\n",
    "    id_x_added = {}\n",
    "    for i, item in enumerate(x_res_added):\n",
    "        item = np.reshape(item, (cut_off, -1))\n",
    "        r_id = np.full((cut_off, 1), int(1000 + i))\n",
    "        item =  np.hstack((r_id,item))\n",
    "        item_df = pd.DataFrame(data=item, columns=related_features)\n",
    "        x_added_df = x_added_df.append(item_df)\n",
    "    \n",
    "#     y_added_to_id = {int(i): y_res_added[int(1000 - i)] for i in x_added_df.Subject_ID.tolist()}\n",
    "    \n",
    "#     subject_id_unique = sub_DumbTo.Subject_ID.unique().tolist()\n",
    "#     y_train_to_id = {}\n",
    "#     y_valid_to_id = {}\n",
    "#     for j, subject_id in subject_id_unique:\n",
    "#         if j == i:\n",
    "#             y_valid_to_id[subject_id] = y_i\n",
    "#         else:\n",
    "#             y_train_to_id[subject_id] = \n",
    "#     y_train_to_id = {k: id_to_y[k] for k in subject_id_unique[:42]}\n",
    "#     y_train_to_id.update(y_added_to_id)\n",
    "#     y_valid_to_id = {k: id_to_y[k] for k in subject_id_unique[37:]}\n",
    "\n",
    "    x_train_df_tot = x_train_df.append(x_added_df)\n",
    "    x_train_df_tot = x_train_df_tot.reset_index().drop(columns=[\"index\"]).reset_index()\n",
    "    \n",
    "    unique_training_subject_ids = x_train_df_tot.Subject_ID.unique()\n",
    "    unique_training_subject_ids = np.delete(unique_training_subject_ids, i)\n",
    "    \n",
    "    id_to_y_train = {_id: y_res[i] for _id, i in enumerate(unique_training_subject_ids)}\n",
    "    \n",
    "    group_1 = x_train_df_tot[x_train_df_tot.index % 4 == 0]\n",
    "    group_2 = x_train_df_tot[x_train_df_tot.index % 4 == 1]\n",
    "    group_3 = x_train_df_tot[x_train_df_tot.index % 4 == 2]\n",
    "    group_4 = x_train_df_tot[x_train_df_tot.index % 4 == 3]\n",
    "    \n",
    "    data_train = group_1.append(group_2).append(group_3).append(group_4)\n",
    "    \n",
    "    x_train_augmented = data_train.to_numpy()\n",
    "    \n",
    "    num_train_subj = 42\n",
    "    num_augmented_train_subj = num_train_subj + len(y_res_added)\n",
    "    \n",
    "    x_train_augmented_reshaped = x_train_augmented.reshape(int(num_augmented_train_subj) * 4, -1, len(related_features) + 1)\n",
    "    \n",
    "    np.random.seed(42)\n",
    "    rng = np.random.default_rng()\n",
    "    np.random.shuffle(x_train_augmented_reshaped)\n",
    "    \n",
    "    final_x_train = np.zeros((x_train_augmented_reshaped.shape[0], x_train_augmented_reshaped.shape[1],x_train_augmented_reshaped.shape[2]-2))\n",
    "    final_y_train = np.zeros(x_train_augmented_reshaped.shape[0])\n",
    "    \n",
    "    for i, item in enumerate(x_train_augmented_reshaped):\n",
    "        final_y_train[i] = id_to_y_train[int(item[0][1])]\n",
    "        final_x_train[i] = np.delete(item,np.s_[0:2],axis=1)\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "0bda2869",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "6/6 [==============================] - 131s 22s/step - loss: 2.7312 - accuracy: 0.7031\n",
      "Epoch 2/4\n",
      "6/6 [==============================] - 132s 22s/step - loss: 1.0555 - accuracy: 0.9844\n",
      "Epoch 3/4\n",
      "6/6 [==============================] - 130s 22s/step - loss: 0.9989 - accuracy: 0.9167\n",
      "Epoch 4/4\n",
      "6/6 [==============================] - 131s 22s/step - loss: 0.7832 - accuracy: 0.9896\n",
      "INFO:tensorflow:Assets written to: ./Saved_Models/lstm_loov_0\\assets\n",
      "Epoch 1/4\n",
      "6/6 [==============================] - 127s 21s/step - loss: 3.3201 - accuracy: 0.6406\n",
      "Epoch 2/4\n",
      "6/6 [==============================] - 129s 22s/step - loss: 1.3730 - accuracy: 0.9219\n",
      "Epoch 3/4\n",
      "6/6 [==============================] - 129s 21s/step - loss: 1.1176 - accuracy: 0.9740\n",
      "Epoch 4/4\n",
      "6/6 [==============================] - 128s 21s/step - loss: 0.7503 - accuracy: 0.9635\n",
      "INFO:tensorflow:Assets written to: ./Saved_Models/lstm_loov_1\\assets\n",
      "Epoch 1/4\n",
      "6/6 [==============================] - 135s 22s/step - loss: 3.3679 - accuracy: 0.6875\n",
      "Epoch 2/4\n",
      "6/6 [==============================] - 135s 23s/step - loss: 1.4695 - accuracy: 0.9062\n",
      "Epoch 3/4\n",
      "6/6 [==============================] - 135s 23s/step - loss: 1.1471 - accuracy: 0.9688\n",
      "Epoch 4/4\n",
      "6/6 [==============================] - 136s 23s/step - loss: 0.9016 - accuracy: 0.9583\n",
      "INFO:tensorflow:Assets written to: ./Saved_Models/lstm_loov_2\\assets\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000029F055AAA60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/4\n",
      "6/6 [==============================] - 142s 24s/step - loss: 3.1013 - accuracy: 0.6719\n",
      "Epoch 2/4\n",
      "6/6 [==============================] - 141s 24s/step - loss: 1.4592 - accuracy: 0.8281\n",
      "Epoch 3/4\n",
      "6/6 [==============================] - 142s 24s/step - loss: 1.0273 - accuracy: 0.9896\n",
      "Epoch 4/4\n",
      "6/6 [==============================] - 141s 23s/step - loss: 1.0556 - accuracy: 0.9062\n",
      "INFO:tensorflow:Assets written to: ./Saved_Models/lstm_loov_3\\assets\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000029EF286AAF0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/4\n",
      "6/6 [==============================] - 143s 24s/step - loss: 3.1927 - accuracy: 0.6667\n",
      "Epoch 2/4\n",
      "6/6 [==============================] - 143s 24s/step - loss: 1.5728 - accuracy: 0.8594\n",
      "Epoch 3/4\n",
      "6/6 [==============================] - 143s 24s/step - loss: 1.1684 - accuracy: 0.9583\n",
      "Epoch 4/4\n",
      "6/6 [==============================] - 147s 25s/step - loss: 0.7763 - accuracy: 0.9948\n",
      "INFO:tensorflow:Assets written to: ./Saved_Models/lstm_loov_4\\assets\n",
      "WARNING:tensorflow:7 out of the last 7 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000029EF30DF160> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/4\n",
      "6/6 [==============================] - 148s 25s/step - loss: 3.2550 - accuracy: 0.7292\n",
      "Epoch 2/4\n",
      "6/6 [==============================] - 150s 25s/step - loss: 1.5719 - accuracy: 0.8542\n",
      "Epoch 3/4\n",
      "6/6 [==============================] - 149s 25s/step - loss: 1.3815 - accuracy: 0.9115\n",
      "Epoch 4/4\n",
      "6/6 [==============================] - 148s 25s/step - loss: 1.3245 - accuracy: 0.8229\n",
      "INFO:tensorflow:Assets written to: ./Saved_Models/lstm_loov_5\\assets\n",
      "WARNING:tensorflow:8 out of the last 8 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000029EA52B7550> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/4\n",
      "6/6 [==============================] - 146s 24s/step - loss: 2.7667 - accuracy: 0.6875\n",
      "Epoch 2/4\n",
      "6/6 [==============================] - 147s 25s/step - loss: 1.2508 - accuracy: 0.9323\n",
      "Epoch 3/4\n",
      "6/6 [==============================] - 148s 25s/step - loss: 0.9262 - accuracy: 0.9792\n",
      "Epoch 4/4\n",
      "6/6 [==============================] - 147s 25s/step - loss: 0.7087 - accuracy: 0.9948\n",
      "INFO:tensorflow:Assets written to: ./Saved_Models/lstm_loov_6\\assets\n",
      "WARNING:tensorflow:9 out of the last 9 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000029EF2AEF160> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/4\n",
      "6/6 [==============================] - 150s 25s/step - loss: 4.1153 - accuracy: 0.7604\n",
      "Epoch 2/4\n",
      "6/6 [==============================] - 150s 25s/step - loss: 1.7518 - accuracy: 0.5729\n",
      "Epoch 3/4\n",
      "6/6 [==============================] - 150s 25s/step - loss: 1.4185 - accuracy: 0.9375\n",
      "Epoch 4/4\n",
      "6/6 [==============================] - 151s 25s/step - loss: 0.9521 - accuracy: 0.9583\n",
      "INFO:tensorflow:Assets written to: ./Saved_Models/lstm_loov_7\\assets\n",
      "WARNING:tensorflow:10 out of the last 10 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000029F09E303A0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "6/6 [==============================] - 153s 25s/step - loss: 2.8685 - accuracy: 0.7292\n",
      "Epoch 2/4\n",
      "6/6 [==============================] - 152s 25s/step - loss: 1.6794 - accuracy: 0.8229\n",
      "Epoch 3/4\n",
      "6/6 [==============================] - 152s 25s/step - loss: 1.2406 - accuracy: 0.9219\n",
      "Epoch 4/4\n",
      "6/6 [==============================] - 151s 25s/step - loss: 0.8448 - accuracy: 0.9844\n",
      "INFO:tensorflow:Assets written to: ./Saved_Models/lstm_loov_8\\assets\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000029EB438D550> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/4\n",
      "6/6 [==============================] - 153s 25s/step - loss: 2.2851 - accuracy: 0.8438\n",
      "Epoch 2/4\n",
      "6/6 [==============================] - 153s 26s/step - loss: 0.8448 - accuracy: 0.9896\n",
      "Epoch 3/4\n",
      "6/6 [==============================] - 153s 26s/step - loss: 0.6632 - accuracy: 0.9896\n",
      "Epoch 4/4\n",
      "6/6 [==============================] - 153s 25s/step - loss: 0.4705 - accuracy: 0.9896\n",
      "INFO:tensorflow:Assets written to: ./Saved_Models/lstm_loov_9\\assets\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000029ED5216DC0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/4\n",
      "6/6 [==============================] - 156s 26s/step - loss: 2.9648 - accuracy: 0.7552\n",
      "Epoch 2/4\n",
      "6/6 [==============================] - 157s 26s/step - loss: 1.3292 - accuracy: 0.8906\n",
      "Epoch 3/4\n",
      "6/6 [==============================] - 155s 26s/step - loss: 1.0099 - accuracy: 0.9635\n",
      "Epoch 4/4\n",
      "6/6 [==============================] - 160s 27s/step - loss: 0.9619 - accuracy: 0.9792\n",
      "INFO:tensorflow:Assets written to: ./Saved_Models/lstm_loov_10\\assets\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000029F1E5004C0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/4\n",
      "6/6 [==============================] - 155s 26s/step - loss: 2.5812 - accuracy: 0.6771\n",
      "Epoch 2/4\n",
      "6/6 [==============================] - 158s 26s/step - loss: 1.2254 - accuracy: 0.9010\n",
      "Epoch 3/4\n",
      "6/6 [==============================] - 156s 26s/step - loss: 1.2271 - accuracy: 0.9479\n",
      "Epoch 4/4\n",
      "6/6 [==============================] - 157s 26s/step - loss: 1.0476 - accuracy: 0.9479\n",
      "INFO:tensorflow:Assets written to: ./Saved_Models/lstm_loov_11\\assets\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000029F1F6F8C10> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/4\n",
      "6/6 [==============================] - 160s 27s/step - loss: 3.2025 - accuracy: 0.5938\n",
      "Epoch 2/4\n",
      "6/6 [==============================] - 162s 27s/step - loss: 1.5883 - accuracy: 0.8333\n",
      "Epoch 3/4\n",
      "6/6 [==============================] - 160s 27s/step - loss: 1.1870 - accuracy: 0.9583\n",
      "Epoch 4/4\n",
      "6/6 [==============================] - 160s 27s/step - loss: 0.8260 - accuracy: 0.9948\n",
      "INFO:tensorflow:Assets written to: ./Saved_Models/lstm_loov_12\\assets\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000029E8A6A14C0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/4\n",
      "6/6 [==============================] - 159s 27s/step - loss: 2.9743 - accuracy: 0.6823\n",
      "Epoch 2/4\n",
      "6/6 [==============================] - 159s 27s/step - loss: 1.3408 - accuracy: 0.9479\n",
      "Epoch 3/4\n",
      "6/6 [==============================] - 159s 26s/step - loss: 1.0369 - accuracy: 0.9688\n",
      "Epoch 4/4\n",
      "6/6 [==============================] - 160s 27s/step - loss: 0.7431 - accuracy: 0.9896\n",
      "INFO:tensorflow:Assets written to: ./Saved_Models/lstm_loov_13\\assets\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000029F0E0AF9D0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/4\n",
      "6/6 [==============================] - 158s 26s/step - loss: 3.6877 - accuracy: 0.5729\n",
      "Epoch 2/4\n",
      "6/6 [==============================] - 159s 27s/step - loss: 1.4463 - accuracy: 0.9531\n",
      "Epoch 3/4\n",
      "6/6 [==============================] - 158s 26s/step - loss: 0.9876 - accuracy: 0.9948\n",
      "Epoch 4/4\n",
      "6/6 [==============================] - 159s 26s/step - loss: 0.9025 - accuracy: 0.9635\n",
      "INFO:tensorflow:Assets written to: ./Saved_Models/lstm_loov_14\\assets\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000029EA1449550> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "6/6 [==============================] - 158s 26s/step - loss: 2.9968 - accuracy: 0.7240\n",
      "Epoch 2/4\n",
      "6/6 [==============================] - 157s 26s/step - loss: 1.2937 - accuracy: 0.9375\n",
      "Epoch 3/4\n",
      "6/6 [==============================] - 160s 27s/step - loss: 0.9972 - accuracy: 0.9583\n",
      "Epoch 4/4\n",
      "6/6 [==============================] - 159s 26s/step - loss: 0.7169 - accuracy: 0.9948\n",
      "INFO:tensorflow:Assets written to: ./Saved_Models/lstm_loov_15\\assets\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000029ED4FA1160> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/4\n",
      "6/6 [==============================] - 163s 27s/step - loss: 2.8327 - accuracy: 0.7604\n",
      "Epoch 2/4\n",
      "6/6 [==============================] - 163s 27s/step - loss: 1.0929 - accuracy: 0.9792\n",
      "Epoch 3/4\n",
      "6/6 [==============================] - 162s 27s/step - loss: 1.3295 - accuracy: 0.8906\n",
      "Epoch 4/4\n",
      "6/6 [==============================] - 162s 27s/step - loss: 1.1805 - accuracy: 0.8646\n",
      "INFO:tensorflow:Assets written to: ./Saved_Models/lstm_loov_16\\assets\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000029F211A4040> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/4\n",
      "6/6 [==============================] - 168s 28s/step - loss: 2.8996 - accuracy: 0.7188\n",
      "Epoch 2/4\n",
      "6/6 [==============================] - 168s 28s/step - loss: 1.3373 - accuracy: 0.8958\n",
      "Epoch 3/4\n",
      "6/6 [==============================] - 168s 28s/step - loss: 0.9538 - accuracy: 0.9844\n",
      "Epoch 4/4\n",
      "6/6 [==============================] - 169s 28s/step - loss: 0.8134 - accuracy: 0.9844\n",
      "INFO:tensorflow:Assets written to: ./Saved_Models/lstm_loov_17\\assets\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000029EA010A5E0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/4\n",
      "6/6 [==============================] - 163s 27s/step - loss: 2.8194 - accuracy: 0.7240\n",
      "Epoch 2/4\n",
      "6/6 [==============================] - 164s 27s/step - loss: 1.3644 - accuracy: 0.7917\n",
      "Epoch 3/4\n",
      "6/6 [==============================] - 163s 27s/step - loss: 1.0186 - accuracy: 0.9896\n",
      "Epoch 4/4\n",
      "6/6 [==============================] - 164s 27s/step - loss: 1.0109 - accuracy: 0.9010\n",
      "INFO:tensorflow:Assets written to: ./Saved_Models/lstm_loov_18\\assets\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000029F20682AF0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/4\n",
      "6/6 [==============================] - 169s 28s/step - loss: 2.8934 - accuracy: 0.6875\n",
      "Epoch 2/4\n",
      "6/6 [==============================] - 165s 28s/step - loss: 1.2779 - accuracy: 0.8854\n",
      "Epoch 3/4\n",
      "6/6 [==============================] - 172s 29s/step - loss: 0.9775 - accuracy: 0.9896\n",
      "Epoch 4/4\n",
      "6/6 [==============================] - 167s 28s/step - loss: 0.7868 - accuracy: 0.9896\n",
      "INFO:tensorflow:Assets written to: ./Saved_Models/lstm_loov_19\\assets\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000029F230271F0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/4\n",
      "6/6 [==============================] - 166s 28s/step - loss: 3.0635 - accuracy: 0.7656\n",
      "Epoch 2/4\n",
      "6/6 [==============================] - 166s 28s/step - loss: 1.5251 - accuracy: 0.8542\n",
      "Epoch 3/4\n",
      "6/6 [==============================] - 167s 28s/step - loss: 1.1226 - accuracy: 0.9583\n",
      "Epoch 4/4\n",
      "6/6 [==============================] - 166s 28s/step - loss: 0.8202 - accuracy: 0.9844\n",
      "INFO:tensorflow:Assets written to: ./Saved_Models/lstm_loov_20\\assets\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000029F0E0DA550> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/4\n",
      "6/6 [==============================] - 176s 29s/step - loss: 2.8425 - accuracy: 0.7083\n",
      "Epoch 2/4\n",
      "6/6 [==============================] - 180s 30s/step - loss: 1.6274 - accuracy: 0.8073\n",
      "Epoch 3/4\n",
      "6/6 [==============================] - 178s 30s/step - loss: 1.2767 - accuracy: 0.9375\n",
      "Epoch 4/4\n",
      "6/6 [==============================] - 175s 29s/step - loss: 0.9044 - accuracy: 1.0000\n",
      "INFO:tensorflow:Assets written to: ./Saved_Models/lstm_loov_21\\assets\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000029F24788820> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "6/6 [==============================] - 176s 29s/step - loss: 2.7306 - accuracy: 0.7865\n",
      "Epoch 2/4\n",
      "6/6 [==============================] - 174s 29s/step - loss: 1.6661 - accuracy: 0.7656\n",
      "Epoch 3/4\n",
      "6/6 [==============================] - 174s 29s/step - loss: 1.1549 - accuracy: 0.9479\n",
      "Epoch 4/4\n",
      "6/6 [==============================] - 176s 29s/step - loss: 0.9346 - accuracy: 0.9323\n",
      "INFO:tensorflow:Assets written to: ./Saved_Models/lstm_loov_22\\assets\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000029F26A38550> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/4\n",
      "6/6 [==============================] - 186s 31s/step - loss: 2.8104 - accuracy: 0.7031\n",
      "Epoch 2/4\n",
      "6/6 [==============================] - 185s 31s/step - loss: 1.3194 - accuracy: 0.9635\n",
      "Epoch 3/4\n",
      "6/6 [==============================] - 188s 31s/step - loss: 1.0094 - accuracy: 0.9740\n",
      "Epoch 4/4\n",
      "6/6 [==============================] - 185s 31s/step - loss: 0.7254 - accuracy: 0.9948\n",
      "INFO:tensorflow:Assets written to: ./Saved_Models/lstm_loov_23\\assets\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000029EA0B07F70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/4\n",
      "6/6 [==============================] - 182s 30s/step - loss: 2.8560 - accuracy: 0.7188\n",
      "Epoch 2/4\n",
      "6/6 [==============================] - 181s 30s/step - loss: 1.4052 - accuracy: 0.8438\n",
      "Epoch 3/4\n",
      "6/6 [==============================] - 182s 30s/step - loss: 0.9722 - accuracy: 0.9531\n",
      "Epoch 4/4\n",
      "6/6 [==============================] - 183s 30s/step - loss: 0.7752 - accuracy: 0.9323\n",
      "INFO:tensorflow:Assets written to: ./Saved_Models/lstm_loov_24\\assets\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000029E8A94A550> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/4\n",
      "6/6 [==============================] - 184s 31s/step - loss: 3.6577 - accuracy: 0.7135\n",
      "Epoch 2/4\n",
      "6/6 [==============================] - 185s 31s/step - loss: 1.6766 - accuracy: 0.8542\n",
      "Epoch 3/4\n",
      "6/6 [==============================] - 183s 30s/step - loss: 1.3626 - accuracy: 0.9375\n",
      "Epoch 4/4\n",
      "6/6 [==============================] - 191s 32s/step - loss: 0.8820 - accuracy: 0.9896\n",
      "INFO:tensorflow:Assets written to: ./Saved_Models/lstm_loov_25\\assets\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000029F22366040> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/4\n",
      "6/6 [==============================] - 183s 31s/step - loss: 2.7549 - accuracy: 0.7396\n",
      "Epoch 2/4\n",
      "6/6 [==============================] - 185s 31s/step - loss: 1.1198 - accuracy: 0.9531\n",
      "Epoch 3/4\n",
      "6/6 [==============================] - 186s 31s/step - loss: 0.9540 - accuracy: 0.9427\n",
      "Epoch 4/4\n",
      "6/6 [==============================] - 184s 31s/step - loss: 0.7477 - accuracy: 0.9844\n",
      "INFO:tensorflow:Assets written to: ./Saved_Models/lstm_loov_26\\assets\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000029F1F2F1790> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/4\n",
      "6/6 [==============================] - 187s 31s/step - loss: 2.8309 - accuracy: 0.7865\n",
      "Epoch 2/4\n",
      "6/6 [==============================] - 188s 31s/step - loss: 1.1212 - accuracy: 0.9740\n",
      "Epoch 3/4\n",
      "6/6 [==============================] - 190s 32s/step - loss: 0.8568 - accuracy: 0.9896\n",
      "Epoch 4/4\n",
      "6/6 [==============================] - 191s 32s/step - loss: 0.8837 - accuracy: 0.9792\n",
      "INFO:tensorflow:Assets written to: ./Saved_Models/lstm_loov_27\\assets\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000029F258818B0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/4\n",
      "6/6 [==============================] - 187s 31s/step - loss: 2.6195 - accuracy: 0.7969\n",
      "Epoch 2/4\n",
      "6/6 [==============================] - 188s 31s/step - loss: 1.2013 - accuracy: 0.9115\n",
      "Epoch 3/4\n",
      "6/6 [==============================] - 188s 31s/step - loss: 0.9243 - accuracy: 0.9792\n",
      "Epoch 4/4\n",
      "6/6 [==============================] - 188s 31s/step - loss: 0.6802 - accuracy: 0.9948\n",
      "INFO:tensorflow:Assets written to: ./Saved_Models/lstm_loov_28\\assets\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000029F2A241E50> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "6/6 [==============================] - 188s 31s/step - loss: 3.5902 - accuracy: 0.6562\n",
      "Epoch 2/4\n",
      "6/6 [==============================] - 196s 33s/step - loss: 1.3503 - accuracy: 0.9688\n",
      "Epoch 3/4\n",
      "6/6 [==============================] - 190s 32s/step - loss: 1.3245 - accuracy: 0.9427\n",
      "Epoch 4/4\n",
      "6/6 [==============================] - 190s 32s/step - loss: 0.8919 - accuracy: 0.9740\n",
      "INFO:tensorflow:Assets written to: ./Saved_Models/lstm_loov_29\\assets\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000029F0CB163A0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/4\n",
      "6/6 [==============================] - 193s 32s/step - loss: 2.6662 - accuracy: 0.6719\n",
      "Epoch 2/4\n",
      "6/6 [==============================] - 192s 32s/step - loss: 1.1379 - accuracy: 0.9479\n",
      "Epoch 3/4\n",
      "6/6 [==============================] - 190s 32s/step - loss: 1.0205 - accuracy: 0.9479\n",
      "Epoch 4/4\n",
      "6/6 [==============================] - 191s 32s/step - loss: 0.7203 - accuracy: 0.9948\n",
      "INFO:tensorflow:Assets written to: ./Saved_Models/lstm_loov_30\\assets\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000029F0EA7FA60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/4\n",
      "6/6 [==============================] - 189s 31s/step - loss: 3.3774 - accuracy: 0.7708\n",
      "Epoch 2/4\n",
      "6/6 [==============================] - 190s 32s/step - loss: 1.5955 - accuracy: 0.8906\n",
      "Epoch 3/4\n",
      "6/6 [==============================] - 190s 32s/step - loss: 1.1525 - accuracy: 0.9688\n",
      "Epoch 4/4\n",
      "6/6 [==============================] - 196s 33s/step - loss: 0.7853 - accuracy: 1.0000\n",
      "INFO:tensorflow:Assets written to: ./Saved_Models/lstm_loov_31\\assets\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000029F2BE9F160> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/4\n",
      "6/6 [==============================] - 190s 32s/step - loss: 3.4148 - accuracy: 0.5469\n",
      "Epoch 2/4\n",
      "6/6 [==============================] - 189s 31s/step - loss: 1.5716 - accuracy: 0.9323\n",
      "Epoch 3/4\n",
      "6/6 [==============================] - 190s 32s/step - loss: 1.2473 - accuracy: 0.8594\n",
      "Epoch 4/4\n",
      "6/6 [==============================] - 189s 32s/step - loss: 0.9010 - accuracy: 0.9740\n",
      "INFO:tensorflow:Assets written to: ./Saved_Models/lstm_loov_32\\assets\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000029EFD0504C0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/4\n",
      "6/6 [==============================] - 195s 32s/step - loss: 3.0154 - accuracy: 0.6042\n",
      "Epoch 2/4\n",
      "6/6 [==============================] - 192s 32s/step - loss: 1.4360 - accuracy: 0.8750\n",
      "Epoch 3/4\n",
      "6/6 [==============================] - 195s 32s/step - loss: 1.0449 - accuracy: 0.9583\n",
      "Epoch 4/4\n",
      "6/6 [==============================] - 193s 32s/step - loss: 0.6756 - accuracy: 1.0000\n",
      "INFO:tensorflow:Assets written to: ./Saved_Models/lstm_loov_33\\assets\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000029F26867D30> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/4\n",
      "6/6 [==============================] - 195s 33s/step - loss: 2.5828 - accuracy: 0.6979\n",
      "Epoch 2/4\n",
      "6/6 [==============================] - 195s 32s/step - loss: 1.0282 - accuracy: 0.9740\n",
      "Epoch 3/4\n",
      "6/6 [==============================] - 194s 32s/step - loss: 0.7902 - accuracy: 0.9896\n",
      "Epoch 4/4\n",
      "6/6 [==============================] - 195s 33s/step - loss: 0.5747 - accuracy: 1.0000\n",
      "INFO:tensorflow:Assets written to: ./Saved_Models/lstm_loov_34\\assets\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000029F28332700> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/4\n",
      "6/6 [==============================] - 191s 32s/step - loss: 2.6086 - accuracy: 0.7448\n",
      "Epoch 2/4\n",
      "6/6 [==============================] - 195s 33s/step - loss: 1.0302 - accuracy: 1.0000\n",
      "Epoch 3/4\n",
      "6/6 [==============================] - 195s 32s/step - loss: 1.1295 - accuracy: 0.9219\n",
      "Epoch 4/4\n",
      "6/6 [==============================] - 193s 32s/step - loss: 1.1953 - accuracy: 0.9219\n",
      "INFO:tensorflow:Assets written to: ./Saved_Models/lstm_loov_35\\assets\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000029F0E9C80D0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "6/6 [==============================] - 205s 34s/step - loss: 3.1239 - accuracy: 0.6302\n",
      "Epoch 2/4\n",
      "6/6 [==============================] - 207s 34s/step - loss: 1.6333 - accuracy: 0.9115\n",
      "Epoch 3/4\n",
      "6/6 [==============================] - 202s 34s/step - loss: 1.2292 - accuracy: 0.8958\n",
      "Epoch 4/4\n",
      "6/6 [==============================] - 196s 33s/step - loss: 0.9149 - accuracy: 0.9531\n",
      "INFO:tensorflow:Assets written to: ./Saved_Models/lstm_loov_36\\assets\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000029F2210A160> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/4\n",
      "6/6 [==============================] - 198s 33s/step - loss: 2.7911 - accuracy: 0.7188\n",
      "Epoch 2/4\n",
      "6/6 [==============================] - 197s 33s/step - loss: 1.2006 - accuracy: 0.9583\n",
      "Epoch 3/4\n",
      "6/6 [==============================] - 200s 33s/step - loss: 0.9258 - accuracy: 0.9635\n",
      "Epoch 4/4\n",
      "6/6 [==============================] - 198s 33s/step - loss: 0.9613 - accuracy: 0.9271\n",
      "INFO:tensorflow:Assets written to: ./Saved_Models/lstm_loov_37\\assets\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000029F206D4310> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/4\n",
      "6/6 [==============================] - 204s 34s/step - loss: 3.1437 - accuracy: 0.5885\n",
      "Epoch 2/4\n",
      "6/6 [==============================] - 204s 34s/step - loss: 1.4722 - accuracy: 0.8854\n",
      "Epoch 3/4\n",
      "6/6 [==============================] - 212s 35s/step - loss: 1.0143 - accuracy: 0.9688\n",
      "Epoch 4/4\n",
      "6/6 [==============================] - 212s 35s/step - loss: 0.6679 - accuracy: 1.0000\n",
      "INFO:tensorflow:Assets written to: ./Saved_Models/lstm_loov_38\\assets\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000029F30B5B940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/4\n",
      "6/6 [==============================] - 218s 36s/step - loss: 3.0832 - accuracy: 0.6250\n",
      "Epoch 2/4\n",
      "6/6 [==============================] - 218s 36s/step - loss: 1.4356 - accuracy: 0.9427\n",
      "Epoch 3/4\n",
      "6/6 [==============================] - 221s 37s/step - loss: 1.2177 - accuracy: 0.9062\n",
      "Epoch 4/4\n",
      "6/6 [==============================] - 215s 36s/step - loss: 0.7985 - accuracy: 0.9740\n",
      "INFO:tensorflow:Assets written to: ./Saved_Models/lstm_loov_39\\assets\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000029F324EBDC0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/4\n",
      "6/6 [==============================] - 214s 36s/step - loss: 3.0557 - accuracy: 0.7240\n",
      "Epoch 2/4\n",
      "6/6 [==============================] - 206s 34s/step - loss: 1.4250 - accuracy: 0.8750\n",
      "Epoch 3/4\n",
      "6/6 [==============================] - 207s 34s/step - loss: 1.1130 - accuracy: 0.9531\n",
      "Epoch 4/4\n",
      "6/6 [==============================] - 210s 35s/step - loss: 0.9529 - accuracy: 0.9688\n",
      "INFO:tensorflow:Assets written to: ./Saved_Models/lstm_loov_40\\assets\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000029F23E34280> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/4\n",
      "6/6 [==============================] - 204s 34s/step - loss: 3.8885 - accuracy: 0.6198\n",
      "Epoch 2/4\n",
      "6/6 [==============================] - 211s 35s/step - loss: 1.3450 - accuracy: 0.9427\n",
      "Epoch 3/4\n",
      "6/6 [==============================] - 208s 35s/step - loss: 1.0145 - accuracy: 0.9583\n",
      "Epoch 4/4\n",
      "6/6 [==============================] - 210s 35s/step - loss: 0.7687 - accuracy: 0.9844\n",
      "INFO:tensorflow:Assets written to: ./Saved_Models/lstm_loov_41\\assets\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000029F26A1F550> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/4\n",
      "6/6 [==============================] - 223s 37s/step - loss: 2.4861 - accuracy: 0.7969\n",
      "Epoch 2/4\n",
      "6/6 [==============================] - 250s 42s/step - loss: 1.0597 - accuracy: 0.9740\n",
      "Epoch 3/4\n",
      "6/6 [==============================] - 221s 37s/step - loss: 0.8205 - accuracy: 0.9792\n",
      "Epoch 4/4\n",
      "6/6 [==============================] - 230s 38s/step - loss: 0.7130 - accuracy: 0.9583\n",
      "INFO:tensorflow:Assets written to: ./Saved_Models/lstm_loov_42\\assets\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000029F2666ADC0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    }
   ],
   "source": [
    "final_results = []\n",
    "training_acc = []\n",
    "for i, y_i in enumerate(y):\n",
    "    mask = np.repeat(True, len(y))\n",
    "    mask[i] = False\n",
    "\n",
    "    y_train = y[mask]   \n",
    "    x_train = x_raw[mask]\n",
    "\n",
    "    y_valid = y[i]\n",
    "    x_valid = x_raw[i]\n",
    "\n",
    "    x_for_smote = np.zeros((x_train.shape[0], x_train.shape[1] * x_train.shape[2]))\n",
    "    for j, seq_feat_mat in enumerate(x_train):\n",
    "        x_for_smote[j] = np.reshape(seq_feat_mat, x_train.shape[1] * x_train.shape[2])\n",
    "\n",
    "    strategy = {0:12, 1:36} if y_i==1 else {0:11, 1:37}\n",
    "    oversample = SMOTE(sampling_strategy=strategy, random_state=0, k_neighbors=3)\n",
    "    x_res, y_res = oversample.fit_resample(x_for_smote, y_train)\n",
    "\n",
    "    x_res_added = x_res[42:,]\n",
    "    y_res_added = y_res[42:,]\n",
    "\n",
    "    x_valid_df = sub_DumbTo.iloc[i*cut_off : (i+1)*cut_off].reset_index()\n",
    "    x_valid_df = x_valid_df.drop(columns=[\"index\"])\n",
    "    x_train_df = sub_DumbTo.drop(sub_DumbTo.index[i*cut_off : (i+1)*cut_off], axis=0).reset_index()\n",
    "    x_train_df = x_train_df.drop(columns=[\"index\"])\n",
    "\n",
    "    x_added_df = pd.DataFrame(columns=related_features)\n",
    "    id_x_added = {}\n",
    "\n",
    "    for k, item in enumerate(x_res_added):\n",
    "        item = np.reshape(item, (cut_off, -1))\n",
    "        r_id = np.full((cut_off, 1), int(1000 + k))\n",
    "        item =  np.hstack((r_id,item))\n",
    "        item_df = pd.DataFrame(data=item, columns=related_features)\n",
    "        x_added_df = x_added_df.append(item_df)\n",
    "\n",
    "    x_train_df_tot = x_train_df.append(x_added_df)\n",
    "    x_train_df_tot = x_train_df_tot.reset_index().drop(columns=[\"index\"]).reset_index()\n",
    "\n",
    "    unique_training_subject_ids = x_train_df_tot.Subject_ID.unique()\n",
    "    #unique_training_subject_ids = np.delete(unique_training_subject_ids, i)\n",
    "\n",
    "    id_to_y_train = {_id: y_res[i] for i, _id in enumerate(unique_training_subject_ids)}\n",
    "\n",
    "    group_1 = x_train_df_tot[x_train_df_tot.index % 4 == 0]\n",
    "    group_2 = x_train_df_tot[x_train_df_tot.index % 4 == 1]\n",
    "    group_3 = x_train_df_tot[x_train_df_tot.index % 4 == 2]\n",
    "    group_4 = x_train_df_tot[x_train_df_tot.index % 4 == 3]\n",
    "\n",
    "    data_train = group_1.append(group_2).append(group_3).append(group_4)\n",
    "\n",
    "    x_train_augmented = data_train.to_numpy()\n",
    "\n",
    "    num_train_subj = 42\n",
    "    num_augmented_train_subj = num_train_subj + len(y_res_added)\n",
    "\n",
    "    x_train_augmented_reshaped = x_train_augmented.reshape(int(num_augmented_train_subj) * 4, -1, len(related_features) + 1)\n",
    "\n",
    "    np.random.seed(42)\n",
    "    rng = np.random.default_rng()\n",
    "    np.random.shuffle(x_train_augmented_reshaped)\n",
    "\n",
    "    final_x_train = np.zeros((x_train_augmented_reshaped.shape[0], x_train_augmented_reshaped.shape[1],x_train_augmented_reshaped.shape[2]-2))\n",
    "    final_y_train = np.zeros(x_train_augmented_reshaped.shape[0])\n",
    "\n",
    "    for l, item in enumerate(x_train_augmented_reshaped):\n",
    "        final_y_train[l] = id_to_y_train[int(item[0][1])]\n",
    "        final_x_train[l] = np.delete(item,np.s_[0:2],axis=1)\n",
    "        \n",
    "    special_value = -9999.99\n",
    "    model = keras.Sequential()\n",
    "    model.add(keras.layers.Masking(mask_value=special_value, input_shape=(final_x_train.shape[1], final_x_train.shape[2])))\n",
    "    model.add(keras.layers.LSTM(256, kernel_regularizer=keras.regularizers.l2(l=0.05)))\n",
    "    model.add(keras.layers.Dense(2, kernel_regularizer=keras.regularizers.l2(l=0.05), activation='softmax'))\n",
    "\n",
    "    model.compile(loss='sparse_categorical_crossentropy', \n",
    "                  optimizer=keras.optimizers.Adam(learning_rate=0.01), metrics=['accuracy'])\n",
    "    \n",
    "    weights = {0:4, 1:1}\n",
    "    history = model.fit(x = final_x_train, y = final_y_train, class_weight=weights, epochs = 4)\n",
    "    \n",
    "    training_acc.append(history.history['accuracy'])\n",
    "    \n",
    "    model.save(\"./Saved_Models/lstm_loov_\" + str(i))\n",
    "    \n",
    "    group_1 = x_valid_df[x_valid_df.index % 4 == 0]\n",
    "    group_2 = x_valid_df[x_valid_df.index % 4 == 1]\n",
    "    group_3 = x_valid_df[x_valid_df.index % 4 == 2]\n",
    "    group_4 = x_valid_df[x_valid_df.index % 4 == 3]\n",
    "    \n",
    "    data_valid = group_1.append(group_2).append(group_3).append(group_4)\n",
    "    \n",
    "    x_valid_mat = np.array(data_valid)\n",
    "    x_valid_reshaped = x_valid_mat.reshape(4, -1, len(related_features))\n",
    "    \n",
    "    final_x_valid = np.zeros((x_valid_reshaped.shape[0], x_valid_reshaped.shape[1],x_valid_reshaped.shape[2]-1))\n",
    "    for m, item in enumerate(x_valid_reshaped):\n",
    "        final_x_valid[m] = np.delete(item,np.s_[0:1],axis=1)\n",
    "        \n",
    "    y_pred = model.predict(final_x_valid)\n",
    "    final_results.append(y_pred)\n",
    "    \n",
    "with open(\"loov_v7_results.txt\", \"wb\") as fp:   #Pickling\n",
    "    pickle.dump(final_results, fp)\n",
    "    \n",
    "with open(\"loov_v7_training_acc.txt\", \"wb\") as fp:   #Pickling\n",
    "    pickle.dump(training_acc, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "1e4897ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0])"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "id": "202dec4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 345,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[37]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "95504397",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[-6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "e14a560c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.9983275e-01, 1.6727383e-04],\n",
       "       [9.9983668e-01, 1.6324289e-04],\n",
       "       [9.9984121e-01, 1.5871078e-04],\n",
       "       [9.9982905e-01, 1.7087847e-04]], dtype=float32)"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_results[-6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "id": "9c3804f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0"
      ]
     },
     "execution_count": 358,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_orig[-6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "40814b58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "id": "c3e20f02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0"
      ]
     },
     "execution_count": 356,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_orig[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "3c8932e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.65108216, 0.34891784],\n",
       "       [0.6548481 , 0.34515196],\n",
       "       [0.6349022 , 0.36509788],\n",
       "       [0.6984896 , 0.30151042]], dtype=float32)"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_results[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "60114fce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "d7e8890d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.41581258, 0.5841874 ],\n",
       "       [0.42510757, 0.5748924 ],\n",
       "       [0.3891483 , 0.6108517 ],\n",
       "       [0.35577324, 0.64422673]], dtype=float32)"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_results[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "id": "7763b725",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.0"
      ]
     },
     "execution_count": 357,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_orig[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "05af616b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "8986c988",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[7.0848176e-04, 9.9929154e-01],\n",
       "       [7.1892195e-04, 9.9928111e-01],\n",
       "       [7.2061963e-04, 9.9927944e-01],\n",
       "       [7.2692003e-04, 9.9927312e-01]], dtype=float32)"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_results[9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "e28edb24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[28]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "cc59dd6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.01158977, 0.98841023],\n",
       "       [0.01143121, 0.9885688 ],\n",
       "       [0.01178067, 0.98821926],\n",
       "       [0.01180772, 0.98819226]], dtype=float32)"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_results[28]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "e32c5e12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.042048  , 0.95795196],\n",
       "       [0.04244957, 0.9575504 ],\n",
       "       [0.03884014, 0.9611598 ],\n",
       "       [0.04044715, 0.9595529 ]], dtype=float32)"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_results[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "id": "c690fe2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.042048  , 0.95795196],\n",
       "       [0.04244957, 0.9575504 ],\n",
       "       [0.03884014, 0.9611598 ],\n",
       "       [0.04044715, 0.9595529 ]], dtype=float32)"
      ]
     },
     "execution_count": 350,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_results[42]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "id": "41ab33a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 352,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "id": "66aacdb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 353,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[21]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "id": "f74eb928",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 354,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[18]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "1c7623fe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(final_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "3ab5f88d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "id": "6d79c72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_orig = [data_recall[data_recall.ExternalReference == i].DumbTo.tolist()[0] for i in seq_length.Subject_ID]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "0af74dbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.640625, 0.8854166865348816, 0.9166666865348816]"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history['accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "f045e1b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0])"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "54873d5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(y_train==1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "5b17a615",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(y==1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "fd1096d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "1b2c4e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_i = y[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "4f25446c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "6/6 [==============================] - 98s 16s/step - loss: 2.9571 - accuracy: 0.6667\n",
      "Epoch 2/3\n",
      "6/6 [==============================] - 103s 17s/step - loss: 1.3710 - accuracy: 0.8854\n",
      "Epoch 3/3\n",
      "6/6 [==============================] - 108s 18s/step - loss: 1.0511 - accuracy: 0.9531\n",
      "WARNING:tensorflow:From C:\\Users\\Ramin\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "WARNING:tensorflow:From C:\\Users\\Ramin\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "INFO:tensorflow:Assets written to: ./Saved_Models/lstm_loov_0\\assets\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'x_valid_mat' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-259-606fefce83ad>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[0mdata_valid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgroup_1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgroup_2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgroup_3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgroup_4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 90\u001b[1;33m \u001b[0mx_valid_reshaped\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx_valid_mat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrelated_features\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mitem\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_valid_reshaped\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'x_valid_mat' is not defined"
     ]
    }
   ],
   "source": [
    "mask = np.repeat(True, len(y))\n",
    "mask[i] = False\n",
    "\n",
    "y_train = y[mask]   \n",
    "x_train = x_raw[mask]\n",
    "\n",
    "y_valid = y[i]\n",
    "x_valid = x_raw[i]\n",
    "\n",
    "x_for_smote = np.zeros((x_train.shape[0], x_train.shape[1] * x_train.shape[2]))\n",
    "for j, seq_feat_mat in enumerate(x_train):\n",
    "    x_for_smote[j] = np.reshape(seq_feat_mat, x_train.shape[1] * x_train.shape[2])\n",
    "\n",
    "strategy = {0:12, 1:36} if y_i==1 else {0:11, 1:37}\n",
    "oversample = SMOTE(sampling_strategy=strategy, random_state=0, k_neighbors=3)\n",
    "x_res, y_res = oversample.fit_resample(x_for_smote, y_train)\n",
    "\n",
    "x_res_added = x_res[42:,]\n",
    "y_res_added = y_res[42:,]\n",
    "\n",
    "x_valid_df = sub_DumbTo.iloc[i*cut_off : (i+1)*cut_off].reset_index()\n",
    "x_valid_df = x_valid_df.drop(columns=[\"index\"])\n",
    "x_train_df = sub_DumbTo.drop(sub_DumbTo.index[i*cut_off : (i+1)*cut_off], axis=0).reset_index()\n",
    "x_train_df = x_train_df.drop(columns=[\"index\"])\n",
    "\n",
    "x_added_df = pd.DataFrame(columns=related_features)\n",
    "id_x_added = {}\n",
    "\n",
    "for k, item in enumerate(x_res_added):\n",
    "    item = np.reshape(item, (cut_off, -1))\n",
    "    r_id = np.full((cut_off, 1), int(1000 + k))\n",
    "    item =  np.hstack((r_id,item))\n",
    "    item_df = pd.DataFrame(data=item, columns=related_features)\n",
    "    x_added_df = x_added_df.append(item_df)\n",
    "\n",
    "x_train_df_tot = x_train_df.append(x_added_df)\n",
    "x_train_df_tot = x_train_df_tot.reset_index().drop(columns=[\"index\"]).reset_index()\n",
    "\n",
    "unique_training_subject_ids = x_train_df_tot.Subject_ID.unique()\n",
    "#unique_training_subject_ids = np.delete(unique_training_subject_ids, i)\n",
    "\n",
    "id_to_y_train = {_id: y_res[i] for i, _id in enumerate(unique_training_subject_ids)}\n",
    "\n",
    "group_1 = x_train_df_tot[x_train_df_tot.index % 4 == 0]\n",
    "group_2 = x_train_df_tot[x_train_df_tot.index % 4 == 1]\n",
    "group_3 = x_train_df_tot[x_train_df_tot.index % 4 == 2]\n",
    "group_4 = x_train_df_tot[x_train_df_tot.index % 4 == 3]\n",
    "\n",
    "data_train = group_1.append(group_2).append(group_3).append(group_4)\n",
    "\n",
    "x_train_augmented = data_train.to_numpy()\n",
    "\n",
    "num_train_subj = 42\n",
    "num_augmented_train_subj = num_train_subj + len(y_res_added)\n",
    "\n",
    "x_train_augmented_reshaped = x_train_augmented.reshape(int(num_augmented_train_subj) * 4, -1, len(related_features) + 1)\n",
    "\n",
    "np.random.seed(42)\n",
    "rng = np.random.default_rng()\n",
    "np.random.shuffle(x_train_augmented_reshaped)\n",
    "\n",
    "final_x_train = np.zeros((x_train_augmented_reshaped.shape[0], x_train_augmented_reshaped.shape[1],x_train_augmented_reshaped.shape[2]-2))\n",
    "final_y_train = np.zeros(x_train_augmented_reshaped.shape[0])\n",
    "\n",
    "for l, item in enumerate(x_train_augmented_reshaped):\n",
    "    final_y_train[l] = id_to_y_train[int(item[0][1])]\n",
    "    final_x_train[l] = np.delete(item,np.s_[0:2],axis=1)\n",
    "\n",
    "special_value = -9999.99\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Masking(mask_value=special_value, input_shape=(final_x_train.shape[1], final_x_train.shape[2])))\n",
    "model.add(keras.layers.LSTM(256, kernel_regularizer=keras.regularizers.l2(l=0.05)))\n",
    "model.add(keras.layers.Dense(2, kernel_regularizer=keras.regularizers.l2(l=0.05), activation='softmax'))\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', \n",
    "              optimizer=keras.optimizers.Adam(learning_rate=0.01), metrics=['accuracy'])\n",
    "\n",
    "weights = {0:4, 1:1}\n",
    "history = model.fit(x = final_x_train, y = final_y_train, class_weight=weights, epochs = 3)\n",
    "\n",
    "model.save(\"./Saved_Models/lstm_loov_\" + str(i))\n",
    "\n",
    "group_1 = x_valid_df[x_valid_df.index % 4 == 0]\n",
    "group_2 = x_valid_df[x_valid_df.index % 4 == 1]\n",
    "group_3 = x_valid_df[x_valid_df.index % 4 == 2]\n",
    "group_4 = x_valid_df[x_valid_df.index % 4 == 3]\n",
    "\n",
    "data_valid = group_1.append(group_2).append(group_3).append(group_4)\n",
    "x_valid_mat = np.array(data_valid)\n",
    "\n",
    "x_valid_reshaped = x_valid_mat.reshape(4, -1, len(related_features))\n",
    "\n",
    "final_x_valid = np.zeros((x_valid_reshaped.shape[0], x_valid_reshaped.shape[1],x_valid_reshaped.shape[2]-1))\n",
    "for m, item in enumerate(x_valid_reshaped):\n",
    "    final_x_valid[m] = np.delete(item,np.s_[0:1],axis=1)\n",
    "\n",
    "y_pred = model.predict(final_x_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "421cee26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(related_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "9664e497",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Subject_ID</th>\n",
       "      <th>FPOGX</th>\n",
       "      <th>FPOGY</th>\n",
       "      <th>BPOGX</th>\n",
       "      <th>BPOGY</th>\n",
       "      <th>LPCX</th>\n",
       "      <th>LPCY</th>\n",
       "      <th>Normed_LPD</th>\n",
       "      <th>Normed_LPS</th>\n",
       "      <th>RPCX</th>\n",
       "      <th>...</th>\n",
       "      <th>Contempt_Evidence</th>\n",
       "      <th>Disgust_Evidence</th>\n",
       "      <th>Joy_Evidence</th>\n",
       "      <th>Fear_Evidence</th>\n",
       "      <th>Negative_Evidence</th>\n",
       "      <th>Neutral_Evidence</th>\n",
       "      <th>Positive_Evidence</th>\n",
       "      <th>Sadness_Evidence</th>\n",
       "      <th>Surprise_Evidence</th>\n",
       "      <th>Normed_Heart_Rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>300</td>\n",
       "      <td>0.68073</td>\n",
       "      <td>0.42222</td>\n",
       "      <td>0.68802</td>\n",
       "      <td>0.39259</td>\n",
       "      <td>0.25017</td>\n",
       "      <td>0.41186</td>\n",
       "      <td>1.808046</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.65154</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.261916</td>\n",
       "      <td>-1.104235</td>\n",
       "      <td>-3.085957</td>\n",
       "      <td>-0.276813</td>\n",
       "      <td>2.562142</td>\n",
       "      <td>0.683569</td>\n",
       "      <td>-1.535620</td>\n",
       "      <td>1.238940</td>\n",
       "      <td>-0.985122</td>\n",
       "      <td>3.416550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>300</td>\n",
       "      <td>0.68125</td>\n",
       "      <td>0.42685</td>\n",
       "      <td>0.68750</td>\n",
       "      <td>0.49259</td>\n",
       "      <td>0.25011</td>\n",
       "      <td>0.41384</td>\n",
       "      <td>-0.821955</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.65151</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.261916</td>\n",
       "      <td>-1.104235</td>\n",
       "      <td>-3.085957</td>\n",
       "      <td>-0.276813</td>\n",
       "      <td>2.562142</td>\n",
       "      <td>0.683569</td>\n",
       "      <td>-1.535620</td>\n",
       "      <td>1.238940</td>\n",
       "      <td>-0.985122</td>\n",
       "      <td>3.416550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>300</td>\n",
       "      <td>0.68125</td>\n",
       "      <td>0.43241</td>\n",
       "      <td>0.68594</td>\n",
       "      <td>0.50556</td>\n",
       "      <td>0.25011</td>\n",
       "      <td>0.41384</td>\n",
       "      <td>-0.821955</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.65146</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.261916</td>\n",
       "      <td>-1.104235</td>\n",
       "      <td>-3.085957</td>\n",
       "      <td>-0.276813</td>\n",
       "      <td>2.562142</td>\n",
       "      <td>0.683569</td>\n",
       "      <td>-1.535620</td>\n",
       "      <td>1.238940</td>\n",
       "      <td>-0.985122</td>\n",
       "      <td>3.416550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>300</td>\n",
       "      <td>0.87396</td>\n",
       "      <td>0.43611</td>\n",
       "      <td>2.58698</td>\n",
       "      <td>0.46574</td>\n",
       "      <td>0.24719</td>\n",
       "      <td>0.40959</td>\n",
       "      <td>-12.431953</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.60381</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.261916</td>\n",
       "      <td>-1.104235</td>\n",
       "      <td>-3.085957</td>\n",
       "      <td>-0.276813</td>\n",
       "      <td>2.562142</td>\n",
       "      <td>0.683569</td>\n",
       "      <td>-1.535620</td>\n",
       "      <td>1.238940</td>\n",
       "      <td>-0.985122</td>\n",
       "      <td>3.416550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>300</td>\n",
       "      <td>0.87396</td>\n",
       "      <td>0.43611</td>\n",
       "      <td>2.58698</td>\n",
       "      <td>0.46574</td>\n",
       "      <td>0.06935</td>\n",
       "      <td>0.33554</td>\n",
       "      <td>-11.791954</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.57383</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.261916</td>\n",
       "      <td>-1.104235</td>\n",
       "      <td>-3.085957</td>\n",
       "      <td>-0.276813</td>\n",
       "      <td>2.562142</td>\n",
       "      <td>0.683569</td>\n",
       "      <td>-1.535620</td>\n",
       "      <td>1.238940</td>\n",
       "      <td>-0.985122</td>\n",
       "      <td>3.416550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9735</th>\n",
       "      <td>300</td>\n",
       "      <td>0.32396</td>\n",
       "      <td>0.31667</td>\n",
       "      <td>0.32135</td>\n",
       "      <td>0.34259</td>\n",
       "      <td>0.30110</td>\n",
       "      <td>0.41826</td>\n",
       "      <td>-1.601954</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.70028</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.460892</td>\n",
       "      <td>-0.851117</td>\n",
       "      <td>-2.313840</td>\n",
       "      <td>0.101591</td>\n",
       "      <td>2.638113</td>\n",
       "      <td>-0.595832</td>\n",
       "      <td>-1.327837</td>\n",
       "      <td>0.058980</td>\n",
       "      <td>-0.248784</td>\n",
       "      <td>-0.825851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9736</th>\n",
       "      <td>300</td>\n",
       "      <td>0.32552</td>\n",
       "      <td>0.37037</td>\n",
       "      <td>0.32656</td>\n",
       "      <td>0.42407</td>\n",
       "      <td>0.30108</td>\n",
       "      <td>0.41808</td>\n",
       "      <td>-1.641954</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.70046</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.460892</td>\n",
       "      <td>-0.851117</td>\n",
       "      <td>-2.313840</td>\n",
       "      <td>0.101591</td>\n",
       "      <td>2.638113</td>\n",
       "      <td>-0.595832</td>\n",
       "      <td>-1.327837</td>\n",
       "      <td>0.058980</td>\n",
       "      <td>-0.248784</td>\n",
       "      <td>-0.825851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9737</th>\n",
       "      <td>300</td>\n",
       "      <td>0.32552</td>\n",
       "      <td>0.38704</td>\n",
       "      <td>0.32604</td>\n",
       "      <td>0.41944</td>\n",
       "      <td>0.30101</td>\n",
       "      <td>0.41811</td>\n",
       "      <td>-1.771954</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.70046</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.460892</td>\n",
       "      <td>-0.851117</td>\n",
       "      <td>-2.313840</td>\n",
       "      <td>0.101591</td>\n",
       "      <td>2.638113</td>\n",
       "      <td>-0.595832</td>\n",
       "      <td>-1.327837</td>\n",
       "      <td>0.058980</td>\n",
       "      <td>-0.248784</td>\n",
       "      <td>-0.825851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9738</th>\n",
       "      <td>300</td>\n",
       "      <td>0.32240</td>\n",
       "      <td>0.37870</td>\n",
       "      <td>0.31406</td>\n",
       "      <td>0.35556</td>\n",
       "      <td>0.30071</td>\n",
       "      <td>0.42038</td>\n",
       "      <td>-14.881954</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.70020</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.460892</td>\n",
       "      <td>-0.851117</td>\n",
       "      <td>-2.313840</td>\n",
       "      <td>0.101591</td>\n",
       "      <td>2.638113</td>\n",
       "      <td>-0.595832</td>\n",
       "      <td>-1.327837</td>\n",
       "      <td>0.058980</td>\n",
       "      <td>-0.248784</td>\n",
       "      <td>-0.825851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9739</th>\n",
       "      <td>300</td>\n",
       "      <td>0.32917</td>\n",
       "      <td>0.37870</td>\n",
       "      <td>0.35521</td>\n",
       "      <td>0.37593</td>\n",
       "      <td>0.30110</td>\n",
       "      <td>0.41901</td>\n",
       "      <td>-3.281954</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.70027</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.543311</td>\n",
       "      <td>-0.840156</td>\n",
       "      <td>-2.308342</td>\n",
       "      <td>0.084717</td>\n",
       "      <td>2.636492</td>\n",
       "      <td>-0.689115</td>\n",
       "      <td>-1.316991</td>\n",
       "      <td>0.047449</td>\n",
       "      <td>-0.208052</td>\n",
       "      <td>-0.825851</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9740 rows Ã— 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Subject_ID    FPOGX    FPOGY    BPOGX    BPOGY     LPCX     LPCY  \\\n",
       "0            300  0.68073  0.42222  0.68802  0.39259  0.25017  0.41186   \n",
       "1            300  0.68125  0.42685  0.68750  0.49259  0.25011  0.41384   \n",
       "2            300  0.68125  0.43241  0.68594  0.50556  0.25011  0.41384   \n",
       "3            300  0.87396  0.43611  2.58698  0.46574  0.24719  0.40959   \n",
       "4            300  0.87396  0.43611  2.58698  0.46574  0.06935  0.33554   \n",
       "...          ...      ...      ...      ...      ...      ...      ...   \n",
       "9735         300  0.32396  0.31667  0.32135  0.34259  0.30110  0.41826   \n",
       "9736         300  0.32552  0.37037  0.32656  0.42407  0.30108  0.41808   \n",
       "9737         300  0.32552  0.38704  0.32604  0.41944  0.30101  0.41811   \n",
       "9738         300  0.32240  0.37870  0.31406  0.35556  0.30071  0.42038   \n",
       "9739         300  0.32917  0.37870  0.35521  0.37593  0.30110  0.41901   \n",
       "\n",
       "      Normed_LPD  Normed_LPS     RPCX  ...  Contempt_Evidence  \\\n",
       "0       1.808046         0.0  0.65154  ...          -0.261916   \n",
       "1      -0.821955         0.0  0.65151  ...          -0.261916   \n",
       "2      -0.821955         1.0  0.65146  ...          -0.261916   \n",
       "3     -12.431953         0.0  0.60381  ...          -0.261916   \n",
       "4     -11.791954         0.0  0.57383  ...          -0.261916   \n",
       "...          ...         ...      ...  ...                ...   \n",
       "9735   -1.601954         0.0  0.70028  ...          -0.460892   \n",
       "9736   -1.641954         0.0  0.70046  ...          -0.460892   \n",
       "9737   -1.771954         0.0  0.70046  ...          -0.460892   \n",
       "9738  -14.881954         0.0  0.70020  ...          -0.460892   \n",
       "9739   -3.281954         0.0  0.70027  ...          -0.543311   \n",
       "\n",
       "      Disgust_Evidence  Joy_Evidence  Fear_Evidence  Negative_Evidence  \\\n",
       "0            -1.104235     -3.085957      -0.276813           2.562142   \n",
       "1            -1.104235     -3.085957      -0.276813           2.562142   \n",
       "2            -1.104235     -3.085957      -0.276813           2.562142   \n",
       "3            -1.104235     -3.085957      -0.276813           2.562142   \n",
       "4            -1.104235     -3.085957      -0.276813           2.562142   \n",
       "...                ...           ...            ...                ...   \n",
       "9735         -0.851117     -2.313840       0.101591           2.638113   \n",
       "9736         -0.851117     -2.313840       0.101591           2.638113   \n",
       "9737         -0.851117     -2.313840       0.101591           2.638113   \n",
       "9738         -0.851117     -2.313840       0.101591           2.638113   \n",
       "9739         -0.840156     -2.308342       0.084717           2.636492   \n",
       "\n",
       "      Neutral_Evidence  Positive_Evidence  Sadness_Evidence  \\\n",
       "0             0.683569          -1.535620          1.238940   \n",
       "1             0.683569          -1.535620          1.238940   \n",
       "2             0.683569          -1.535620          1.238940   \n",
       "3             0.683569          -1.535620          1.238940   \n",
       "4             0.683569          -1.535620          1.238940   \n",
       "...                ...                ...               ...   \n",
       "9735         -0.595832          -1.327837          0.058980   \n",
       "9736         -0.595832          -1.327837          0.058980   \n",
       "9737         -0.595832          -1.327837          0.058980   \n",
       "9738         -0.595832          -1.327837          0.058980   \n",
       "9739         -0.689115          -1.316991          0.047449   \n",
       "\n",
       "      Surprise_Evidence  Normed_Heart_Rate  \n",
       "0             -0.985122           3.416550  \n",
       "1             -0.985122           3.416550  \n",
       "2             -0.985122           3.416550  \n",
       "3             -0.985122           3.416550  \n",
       "4             -0.985122           3.416550  \n",
       "...                 ...                ...  \n",
       "9735          -0.248784          -0.825851  \n",
       "9736          -0.248784          -0.825851  \n",
       "9737          -0.248784          -0.825851  \n",
       "9738          -0.248784          -0.825851  \n",
       "9739          -0.208052          -0.825851  \n",
       "\n",
       "[9740 rows x 26 columns]"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_valid_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "4757ae82",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_valid = group_1.append(group_2).append(group_3).append(group_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "1e2160aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Subject_ID</th>\n",
       "      <th>FPOGX</th>\n",
       "      <th>FPOGY</th>\n",
       "      <th>BPOGX</th>\n",
       "      <th>BPOGY</th>\n",
       "      <th>LPCX</th>\n",
       "      <th>LPCY</th>\n",
       "      <th>Normed_LPD</th>\n",
       "      <th>Normed_LPS</th>\n",
       "      <th>RPCX</th>\n",
       "      <th>...</th>\n",
       "      <th>Contempt_Evidence</th>\n",
       "      <th>Disgust_Evidence</th>\n",
       "      <th>Joy_Evidence</th>\n",
       "      <th>Fear_Evidence</th>\n",
       "      <th>Negative_Evidence</th>\n",
       "      <th>Neutral_Evidence</th>\n",
       "      <th>Positive_Evidence</th>\n",
       "      <th>Sadness_Evidence</th>\n",
       "      <th>Surprise_Evidence</th>\n",
       "      <th>Normed_Heart_Rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>300</td>\n",
       "      <td>0.68073</td>\n",
       "      <td>0.42222</td>\n",
       "      <td>0.68802</td>\n",
       "      <td>0.39259</td>\n",
       "      <td>0.25017</td>\n",
       "      <td>0.41186</td>\n",
       "      <td>1.808046</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.65154</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.261916</td>\n",
       "      <td>-1.104235</td>\n",
       "      <td>-3.085957</td>\n",
       "      <td>-0.276813</td>\n",
       "      <td>2.562142</td>\n",
       "      <td>0.683569</td>\n",
       "      <td>-1.535620</td>\n",
       "      <td>1.238940</td>\n",
       "      <td>-0.985122</td>\n",
       "      <td>3.416550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>300</td>\n",
       "      <td>0.87396</td>\n",
       "      <td>0.43611</td>\n",
       "      <td>2.58698</td>\n",
       "      <td>0.46574</td>\n",
       "      <td>0.06935</td>\n",
       "      <td>0.33554</td>\n",
       "      <td>-11.791954</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.57383</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.261916</td>\n",
       "      <td>-1.104235</td>\n",
       "      <td>-3.085957</td>\n",
       "      <td>-0.276813</td>\n",
       "      <td>2.562142</td>\n",
       "      <td>0.683569</td>\n",
       "      <td>-1.535620</td>\n",
       "      <td>1.238940</td>\n",
       "      <td>-0.985122</td>\n",
       "      <td>3.416550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>300</td>\n",
       "      <td>0.17083</td>\n",
       "      <td>0.74444</td>\n",
       "      <td>0.46094</td>\n",
       "      <td>0.91204</td>\n",
       "      <td>0.24177</td>\n",
       "      <td>0.42196</td>\n",
       "      <td>-7.441954</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.64274</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.261916</td>\n",
       "      <td>-1.104235</td>\n",
       "      <td>-3.085957</td>\n",
       "      <td>-0.276813</td>\n",
       "      <td>2.562142</td>\n",
       "      <td>0.683569</td>\n",
       "      <td>-1.535620</td>\n",
       "      <td>1.238940</td>\n",
       "      <td>-0.985122</td>\n",
       "      <td>7.332649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>300</td>\n",
       "      <td>0.45677</td>\n",
       "      <td>0.33796</td>\n",
       "      <td>0.46927</td>\n",
       "      <td>0.40185</td>\n",
       "      <td>0.24185</td>\n",
       "      <td>0.41533</td>\n",
       "      <td>4.888046</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.64258</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.261916</td>\n",
       "      <td>-1.104235</td>\n",
       "      <td>-3.085957</td>\n",
       "      <td>-0.276813</td>\n",
       "      <td>2.562142</td>\n",
       "      <td>0.683569</td>\n",
       "      <td>-1.535620</td>\n",
       "      <td>1.238940</td>\n",
       "      <td>-0.985122</td>\n",
       "      <td>7.332649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>300</td>\n",
       "      <td>0.76250</td>\n",
       "      <td>0.51574</td>\n",
       "      <td>0.43646</td>\n",
       "      <td>0.45463</td>\n",
       "      <td>0.24093</td>\n",
       "      <td>0.41541</td>\n",
       "      <td>3.818047</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.64250</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.261916</td>\n",
       "      <td>-1.104235</td>\n",
       "      <td>-3.085957</td>\n",
       "      <td>-0.276813</td>\n",
       "      <td>2.562142</td>\n",
       "      <td>0.683569</td>\n",
       "      <td>-1.535620</td>\n",
       "      <td>1.238940</td>\n",
       "      <td>-0.985122</td>\n",
       "      <td>7.332649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9723</th>\n",
       "      <td>300</td>\n",
       "      <td>0.57552</td>\n",
       "      <td>0.41389</td>\n",
       "      <td>0.58281</td>\n",
       "      <td>0.39630</td>\n",
       "      <td>0.29422</td>\n",
       "      <td>0.53710</td>\n",
       "      <td>-3.621954</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.70324</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.397472</td>\n",
       "      <td>-0.796294</td>\n",
       "      <td>-2.474787</td>\n",
       "      <td>0.241565</td>\n",
       "      <td>2.842150</td>\n",
       "      <td>-0.665649</td>\n",
       "      <td>-1.442353</td>\n",
       "      <td>-0.069990</td>\n",
       "      <td>-0.163680</td>\n",
       "      <td>-0.825851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9727</th>\n",
       "      <td>300</td>\n",
       "      <td>0.54167</td>\n",
       "      <td>0.65185</td>\n",
       "      <td>0.51667</td>\n",
       "      <td>0.71852</td>\n",
       "      <td>0.53058</td>\n",
       "      <td>0.57326</td>\n",
       "      <td>-10.531954</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.70324</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.440707</td>\n",
       "      <td>-0.859550</td>\n",
       "      <td>-2.378231</td>\n",
       "      <td>0.118651</td>\n",
       "      <td>2.615409</td>\n",
       "      <td>-0.618072</td>\n",
       "      <td>-1.359967</td>\n",
       "      <td>0.008252</td>\n",
       "      <td>-0.192721</td>\n",
       "      <td>-0.825851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9731</th>\n",
       "      <td>300</td>\n",
       "      <td>0.43542</td>\n",
       "      <td>1.23333</td>\n",
       "      <td>0.33542</td>\n",
       "      <td>0.44444</td>\n",
       "      <td>0.30207</td>\n",
       "      <td>0.41953</td>\n",
       "      <td>-4.071954</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.70059</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.476402</td>\n",
       "      <td>-0.818645</td>\n",
       "      <td>-2.341484</td>\n",
       "      <td>0.135853</td>\n",
       "      <td>2.714867</td>\n",
       "      <td>-0.636180</td>\n",
       "      <td>-1.350828</td>\n",
       "      <td>0.038146</td>\n",
       "      <td>-0.237305</td>\n",
       "      <td>-0.825851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9735</th>\n",
       "      <td>300</td>\n",
       "      <td>0.32396</td>\n",
       "      <td>0.31667</td>\n",
       "      <td>0.32135</td>\n",
       "      <td>0.34259</td>\n",
       "      <td>0.30110</td>\n",
       "      <td>0.41826</td>\n",
       "      <td>-1.601954</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.70028</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.460892</td>\n",
       "      <td>-0.851117</td>\n",
       "      <td>-2.313840</td>\n",
       "      <td>0.101591</td>\n",
       "      <td>2.638113</td>\n",
       "      <td>-0.595832</td>\n",
       "      <td>-1.327837</td>\n",
       "      <td>0.058980</td>\n",
       "      <td>-0.248784</td>\n",
       "      <td>-0.825851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9739</th>\n",
       "      <td>300</td>\n",
       "      <td>0.32917</td>\n",
       "      <td>0.37870</td>\n",
       "      <td>0.35521</td>\n",
       "      <td>0.37593</td>\n",
       "      <td>0.30110</td>\n",
       "      <td>0.41901</td>\n",
       "      <td>-3.281954</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.70027</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.543311</td>\n",
       "      <td>-0.840156</td>\n",
       "      <td>-2.308342</td>\n",
       "      <td>0.084717</td>\n",
       "      <td>2.636492</td>\n",
       "      <td>-0.689115</td>\n",
       "      <td>-1.316991</td>\n",
       "      <td>0.047449</td>\n",
       "      <td>-0.208052</td>\n",
       "      <td>-0.825851</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9740 rows Ã— 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Subject_ID    FPOGX    FPOGY    BPOGX    BPOGY     LPCX     LPCY  \\\n",
       "0            300  0.68073  0.42222  0.68802  0.39259  0.25017  0.41186   \n",
       "4            300  0.87396  0.43611  2.58698  0.46574  0.06935  0.33554   \n",
       "8            300  0.17083  0.74444  0.46094  0.91204  0.24177  0.42196   \n",
       "12           300  0.45677  0.33796  0.46927  0.40185  0.24185  0.41533   \n",
       "16           300  0.76250  0.51574  0.43646  0.45463  0.24093  0.41541   \n",
       "...          ...      ...      ...      ...      ...      ...      ...   \n",
       "9723         300  0.57552  0.41389  0.58281  0.39630  0.29422  0.53710   \n",
       "9727         300  0.54167  0.65185  0.51667  0.71852  0.53058  0.57326   \n",
       "9731         300  0.43542  1.23333  0.33542  0.44444  0.30207  0.41953   \n",
       "9735         300  0.32396  0.31667  0.32135  0.34259  0.30110  0.41826   \n",
       "9739         300  0.32917  0.37870  0.35521  0.37593  0.30110  0.41901   \n",
       "\n",
       "      Normed_LPD  Normed_LPS     RPCX  ...  Contempt_Evidence  \\\n",
       "0       1.808046         0.0  0.65154  ...          -0.261916   \n",
       "4     -11.791954         0.0  0.57383  ...          -0.261916   \n",
       "8      -7.441954        -1.0  0.64274  ...          -0.261916   \n",
       "12      4.888046         0.0  0.64258  ...          -0.261916   \n",
       "16      3.818047         1.0  0.64250  ...          -0.261916   \n",
       "...          ...         ...      ...  ...                ...   \n",
       "9723   -3.621954         0.0  0.70324  ...          -0.397472   \n",
       "9727  -10.531954         0.0  0.70324  ...          -0.440707   \n",
       "9731   -4.071954         1.0  0.70059  ...          -0.476402   \n",
       "9735   -1.601954         0.0  0.70028  ...          -0.460892   \n",
       "9739   -3.281954         0.0  0.70027  ...          -0.543311   \n",
       "\n",
       "      Disgust_Evidence  Joy_Evidence  Fear_Evidence  Negative_Evidence  \\\n",
       "0            -1.104235     -3.085957      -0.276813           2.562142   \n",
       "4            -1.104235     -3.085957      -0.276813           2.562142   \n",
       "8            -1.104235     -3.085957      -0.276813           2.562142   \n",
       "12           -1.104235     -3.085957      -0.276813           2.562142   \n",
       "16           -1.104235     -3.085957      -0.276813           2.562142   \n",
       "...                ...           ...            ...                ...   \n",
       "9723         -0.796294     -2.474787       0.241565           2.842150   \n",
       "9727         -0.859550     -2.378231       0.118651           2.615409   \n",
       "9731         -0.818645     -2.341484       0.135853           2.714867   \n",
       "9735         -0.851117     -2.313840       0.101591           2.638113   \n",
       "9739         -0.840156     -2.308342       0.084717           2.636492   \n",
       "\n",
       "      Neutral_Evidence  Positive_Evidence  Sadness_Evidence  \\\n",
       "0             0.683569          -1.535620          1.238940   \n",
       "4             0.683569          -1.535620          1.238940   \n",
       "8             0.683569          -1.535620          1.238940   \n",
       "12            0.683569          -1.535620          1.238940   \n",
       "16            0.683569          -1.535620          1.238940   \n",
       "...                ...                ...               ...   \n",
       "9723         -0.665649          -1.442353         -0.069990   \n",
       "9727         -0.618072          -1.359967          0.008252   \n",
       "9731         -0.636180          -1.350828          0.038146   \n",
       "9735         -0.595832          -1.327837          0.058980   \n",
       "9739         -0.689115          -1.316991          0.047449   \n",
       "\n",
       "      Surprise_Evidence  Normed_Heart_Rate  \n",
       "0             -0.985122           3.416550  \n",
       "4             -0.985122           3.416550  \n",
       "8             -0.985122           7.332649  \n",
       "12            -0.985122           7.332649  \n",
       "16            -0.985122           7.332649  \n",
       "...                 ...                ...  \n",
       "9723          -0.163680          -0.825851  \n",
       "9727          -0.192721          -0.825851  \n",
       "9731          -0.237305          -0.825851  \n",
       "9735          -0.248784          -0.825851  \n",
       "9739          -0.208052          -0.825851  \n",
       "\n",
       "[9740 rows x 26 columns]"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "38ef99cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_valid_mat = np.array(data_valid)\n",
    "\n",
    "x_valid_reshaped = x_valid_mat.reshape(4, -1, len(related_features))\n",
    "\n",
    "final_x_valid = np.zeros((x_valid_reshaped.shape[0], x_valid_reshaped.shape[1],x_valid_reshaped.shape[2]-1))\n",
    "for m, item in enumerate(x_valid_reshaped):\n",
    "    final_x_valid[m] = np.delete(item,np.s_[0:1],axis=1)\n",
    "\n",
    "y_pred = model.predict(final_x_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "ae9c3974",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.0571278 , 0.9428722 ],\n",
       "       [0.05642581, 0.94357425],\n",
       "       [0.05499753, 0.94500244],\n",
       "       [0.03946377, 0.96053624]], dtype=float32)"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "b800cb92",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "7\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "mask = np.repeat(True, len(y))\n",
    "mask[i] = False\n",
    "\n",
    "y_train = y[mask]   \n",
    "x_train = x_raw[mask]\n",
    "\n",
    "y_valid = y[i]\n",
    "x_valid = x_raw[i]\n",
    "\n",
    "print(i)\n",
    "\n",
    "x_for_smote = np.zeros((x_train.shape[0], x_train.shape[1] * x_train.shape[2]))\n",
    "for j, seq_feat_mat in enumerate(x_train):\n",
    "    x_for_smote[j] = np.reshape(seq_feat_mat, x_train.shape[1] * x_train.shape[2])\n",
    "\n",
    "strategy = {0:12, 1:36} if y_i==1 else {0:11, 1:37}\n",
    "oversample = SMOTE(sampling_strategy=strategy, random_state=0, k_neighbors=3)\n",
    "x_res, y_res = oversample.fit_resample(x_for_smote, y_train)\n",
    "\n",
    "x_res_added = x_res[42:,]\n",
    "y_res_added = y_res[42:,]\n",
    "\n",
    "x_valid_df = sub_DumbTo.iloc[i*cut_off : (i+1)*cut_off].reset_index()\n",
    "x_valid_df = x_valid_df.drop(columns=[\"index\"])\n",
    "x_train_df = sub_DumbTo.drop(sub_DumbTo.index[i*cut_off : (i+1)*cut_off], axis=0).reset_index()\n",
    "x_train_df = x_train_df.drop(columns=[\"index\"])\n",
    "\n",
    "print(i)\n",
    "x_added_df = pd.DataFrame(columns=related_features)\n",
    "id_x_added = {}\n",
    "\n",
    "\n",
    "for k, item in enumerate(x_res_added):\n",
    "    item = np.reshape(item, (cut_off, -1))\n",
    "    r_id = np.full((cut_off, 1), int(1000 + k))\n",
    "    item =  np.hstack((r_id,item))\n",
    "    item_df = pd.DataFrame(data=item, columns=related_features)\n",
    "    x_added_df = x_added_df.append(item_df)\n",
    "print(i)\n",
    "x_train_df_tot = x_train_df.append(x_added_df)\n",
    "x_train_df_tot = x_train_df_tot.reset_index().drop(columns=[\"index\"]).reset_index()\n",
    "\n",
    "unique_training_subject_ids = x_train_df_tot.Subject_ID.unique()\n",
    "#unique_training_subject_ids = np.delete(unique_training_subject_ids, i)\n",
    "\n",
    "id_to_y_train = {_id: y_res[i] for i, _id in enumerate(unique_training_subject_ids)}\n",
    "\n",
    "group_1 = x_train_df_tot[x_train_df_tot.index % 4 == 0]\n",
    "group_2 = x_train_df_tot[x_train_df_tot.index % 4 == 1]\n",
    "group_3 = x_train_df_tot[x_train_df_tot.index % 4 == 2]\n",
    "group_4 = x_train_df_tot[x_train_df_tot.index % 4 == 3]\n",
    "\n",
    "data_train = group_1.append(group_2).append(group_3).append(group_4)\n",
    "\n",
    "x_train_augmented = data_train.to_numpy()\n",
    "\n",
    "num_train_subj = 42\n",
    "num_augmented_train_subj = num_train_subj + len(y_res_added)\n",
    "\n",
    "x_train_augmented_reshaped = x_train_augmented.reshape(int(num_augmented_train_subj) * 4, -1, len(related_features) + 1)\n",
    "\n",
    "np.random.seed(42)\n",
    "rng = np.random.default_rng()\n",
    "np.random.shuffle(x_train_augmented_reshaped)\n",
    "\n",
    "final_x_train = np.zeros((x_train_augmented_reshaped.shape[0], x_train_augmented_reshaped.shape[1],x_train_augmented_reshaped.shape[2]-2))\n",
    "final_y_train = np.zeros(x_train_augmented_reshaped.shape[0])\n",
    "\n",
    "for l, item in enumerate(x_train_augmented_reshaped):\n",
    "    final_y_train[l] = id_to_y_train[int(item[0][1])]\n",
    "    final_x_train[l] = np.delete(item,np.s_[0:2],axis=1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "a3a15ef7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(192, 2435, 25)"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "59f46cd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(192,)"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "38af5ff9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9740"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2435 *4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "4f217c57",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Subject_ID</th>\n",
       "      <th>FPOGX</th>\n",
       "      <th>FPOGY</th>\n",
       "      <th>BPOGX</th>\n",
       "      <th>BPOGY</th>\n",
       "      <th>LPCX</th>\n",
       "      <th>LPCY</th>\n",
       "      <th>Normed_LPD</th>\n",
       "      <th>Normed_LPS</th>\n",
       "      <th>RPCX</th>\n",
       "      <th>...</th>\n",
       "      <th>Contempt_Evidence</th>\n",
       "      <th>Disgust_Evidence</th>\n",
       "      <th>Joy_Evidence</th>\n",
       "      <th>Fear_Evidence</th>\n",
       "      <th>Negative_Evidence</th>\n",
       "      <th>Neutral_Evidence</th>\n",
       "      <th>Positive_Evidence</th>\n",
       "      <th>Sadness_Evidence</th>\n",
       "      <th>Surprise_Evidence</th>\n",
       "      <th>Normed_Heart_Rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9740</th>\n",
       "      <td>305</td>\n",
       "      <td>0.48698</td>\n",
       "      <td>0.33519</td>\n",
       "      <td>0.54583</td>\n",
       "      <td>0.37778</td>\n",
       "      <td>0.23043</td>\n",
       "      <td>0.33596</td>\n",
       "      <td>-5.676806</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.63188</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.063742</td>\n",
       "      <td>-1.864838</td>\n",
       "      <td>-0.997349</td>\n",
       "      <td>0.116210</td>\n",
       "      <td>1.854495</td>\n",
       "      <td>-0.902997</td>\n",
       "      <td>-0.385946</td>\n",
       "      <td>0.262224</td>\n",
       "      <td>-1.695621</td>\n",
       "      <td>-0.898216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9741</th>\n",
       "      <td>305</td>\n",
       "      <td>0.51510</td>\n",
       "      <td>0.35648</td>\n",
       "      <td>0.55937</td>\n",
       "      <td>0.40926</td>\n",
       "      <td>0.23018</td>\n",
       "      <td>0.33530</td>\n",
       "      <td>-5.726807</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63201</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.976372</td>\n",
       "      <td>-1.702245</td>\n",
       "      <td>-1.118984</td>\n",
       "      <td>-0.030738</td>\n",
       "      <td>2.210246</td>\n",
       "      <td>-1.007481</td>\n",
       "      <td>-0.508893</td>\n",
       "      <td>0.199464</td>\n",
       "      <td>-1.773442</td>\n",
       "      <td>-0.898216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9742</th>\n",
       "      <td>305</td>\n",
       "      <td>0.54167</td>\n",
       "      <td>0.43148</td>\n",
       "      <td>0.56823</td>\n",
       "      <td>0.50648</td>\n",
       "      <td>0.23082</td>\n",
       "      <td>0.33640</td>\n",
       "      <td>-5.916805</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63248</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.976372</td>\n",
       "      <td>-1.702245</td>\n",
       "      <td>-1.118984</td>\n",
       "      <td>-0.030738</td>\n",
       "      <td>2.210246</td>\n",
       "      <td>-1.007481</td>\n",
       "      <td>-0.508893</td>\n",
       "      <td>0.199464</td>\n",
       "      <td>-1.773442</td>\n",
       "      <td>-0.898216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9743</th>\n",
       "      <td>305</td>\n",
       "      <td>0.54948</td>\n",
       "      <td>0.45926</td>\n",
       "      <td>0.56563</td>\n",
       "      <td>0.51389</td>\n",
       "      <td>0.23084</td>\n",
       "      <td>0.33630</td>\n",
       "      <td>-5.866806</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63243</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.976372</td>\n",
       "      <td>-1.702245</td>\n",
       "      <td>-1.118984</td>\n",
       "      <td>-0.030738</td>\n",
       "      <td>2.210246</td>\n",
       "      <td>-1.007481</td>\n",
       "      <td>-0.508893</td>\n",
       "      <td>0.199464</td>\n",
       "      <td>-1.773442</td>\n",
       "      <td>-0.898216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9744</th>\n",
       "      <td>305</td>\n",
       "      <td>0.65208</td>\n",
       "      <td>0.70926</td>\n",
       "      <td>0.82187</td>\n",
       "      <td>1.10833</td>\n",
       "      <td>0.24621</td>\n",
       "      <td>0.35972</td>\n",
       "      <td>-6.346806</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.64264</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.976372</td>\n",
       "      <td>-1.702245</td>\n",
       "      <td>-1.118984</td>\n",
       "      <td>-0.030738</td>\n",
       "      <td>2.210246</td>\n",
       "      <td>-1.007481</td>\n",
       "      <td>-0.508893</td>\n",
       "      <td>0.199464</td>\n",
       "      <td>-1.773442</td>\n",
       "      <td>-0.898216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19475</th>\n",
       "      <td>305</td>\n",
       "      <td>0.47031</td>\n",
       "      <td>-0.58611</td>\n",
       "      <td>0.46302</td>\n",
       "      <td>-0.59444</td>\n",
       "      <td>0.18997</td>\n",
       "      <td>0.21182</td>\n",
       "      <td>-7.826805</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.59764</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.955762</td>\n",
       "      <td>-1.022377</td>\n",
       "      <td>-1.726208</td>\n",
       "      <td>-0.290350</td>\n",
       "      <td>2.163536</td>\n",
       "      <td>-0.578703</td>\n",
       "      <td>-0.855910</td>\n",
       "      <td>0.324996</td>\n",
       "      <td>-1.221885</td>\n",
       "      <td>2.306587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19476</th>\n",
       "      <td>305</td>\n",
       "      <td>0.47083</td>\n",
       "      <td>-0.60000</td>\n",
       "      <td>0.47083</td>\n",
       "      <td>-0.61389</td>\n",
       "      <td>0.19028</td>\n",
       "      <td>0.21166</td>\n",
       "      <td>-7.646807</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.59855</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.920440</td>\n",
       "      <td>-1.120620</td>\n",
       "      <td>-1.702867</td>\n",
       "      <td>-0.341188</td>\n",
       "      <td>2.049119</td>\n",
       "      <td>-0.529083</td>\n",
       "      <td>-0.824312</td>\n",
       "      <td>0.307541</td>\n",
       "      <td>-1.318939</td>\n",
       "      <td>2.306587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19477</th>\n",
       "      <td>305</td>\n",
       "      <td>0.47552</td>\n",
       "      <td>-0.58889</td>\n",
       "      <td>0.48542</td>\n",
       "      <td>-0.56759</td>\n",
       "      <td>0.19054</td>\n",
       "      <td>0.21180</td>\n",
       "      <td>-7.626806</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.59897</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.920440</td>\n",
       "      <td>-1.120620</td>\n",
       "      <td>-1.702867</td>\n",
       "      <td>-0.341188</td>\n",
       "      <td>2.049119</td>\n",
       "      <td>-0.529083</td>\n",
       "      <td>-0.824312</td>\n",
       "      <td>0.307541</td>\n",
       "      <td>-1.318939</td>\n",
       "      <td>2.306587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19478</th>\n",
       "      <td>305</td>\n",
       "      <td>0.47708</td>\n",
       "      <td>-0.58426</td>\n",
       "      <td>0.48177</td>\n",
       "      <td>-0.57130</td>\n",
       "      <td>0.19049</td>\n",
       "      <td>0.21175</td>\n",
       "      <td>-7.836805</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.59890</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.920440</td>\n",
       "      <td>-1.120620</td>\n",
       "      <td>-1.702867</td>\n",
       "      <td>-0.341188</td>\n",
       "      <td>2.049119</td>\n",
       "      <td>-0.529083</td>\n",
       "      <td>-0.824312</td>\n",
       "      <td>0.307541</td>\n",
       "      <td>-1.318939</td>\n",
       "      <td>2.306587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19479</th>\n",
       "      <td>305</td>\n",
       "      <td>0.47448</td>\n",
       "      <td>-0.58519</td>\n",
       "      <td>0.46302</td>\n",
       "      <td>-0.58889</td>\n",
       "      <td>0.19087</td>\n",
       "      <td>0.21179</td>\n",
       "      <td>-7.426806</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.59950</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.920440</td>\n",
       "      <td>-1.120620</td>\n",
       "      <td>-1.702867</td>\n",
       "      <td>-0.341188</td>\n",
       "      <td>2.049119</td>\n",
       "      <td>-0.529083</td>\n",
       "      <td>-0.824312</td>\n",
       "      <td>0.307541</td>\n",
       "      <td>-1.318939</td>\n",
       "      <td>2.306587</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9740 rows Ã— 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Subject_ID    FPOGX    FPOGY    BPOGX    BPOGY     LPCX     LPCY  \\\n",
       "9740          305  0.48698  0.33519  0.54583  0.37778  0.23043  0.33596   \n",
       "9741          305  0.51510  0.35648  0.55937  0.40926  0.23018  0.33530   \n",
       "9742          305  0.54167  0.43148  0.56823  0.50648  0.23082  0.33640   \n",
       "9743          305  0.54948  0.45926  0.56563  0.51389  0.23084  0.33630   \n",
       "9744          305  0.65208  0.70926  0.82187  1.10833  0.24621  0.35972   \n",
       "...           ...      ...      ...      ...      ...      ...      ...   \n",
       "19475         305  0.47031 -0.58611  0.46302 -0.59444  0.18997  0.21182   \n",
       "19476         305  0.47083 -0.60000  0.47083 -0.61389  0.19028  0.21166   \n",
       "19477         305  0.47552 -0.58889  0.48542 -0.56759  0.19054  0.21180   \n",
       "19478         305  0.47708 -0.58426  0.48177 -0.57130  0.19049  0.21175   \n",
       "19479         305  0.47448 -0.58519  0.46302 -0.58889  0.19087  0.21179   \n",
       "\n",
       "       Normed_LPD  Normed_LPS     RPCX  ...  Contempt_Evidence  \\\n",
       "9740    -5.676806         1.0  0.63188  ...          -1.063742   \n",
       "9741    -5.726807         0.0  0.63201  ...          -0.976372   \n",
       "9742    -5.916805         0.0  0.63248  ...          -0.976372   \n",
       "9743    -5.866806         0.0  0.63243  ...          -0.976372   \n",
       "9744    -6.346806         0.0  0.64264  ...          -0.976372   \n",
       "...           ...         ...      ...  ...                ...   \n",
       "19475   -7.826805         0.0  0.59764  ...          -0.955762   \n",
       "19476   -7.646807         0.0  0.59855  ...          -0.920440   \n",
       "19477   -7.626806         0.0  0.59897  ...          -0.920440   \n",
       "19478   -7.836805         0.0  0.59890  ...          -0.920440   \n",
       "19479   -7.426806         0.0  0.59950  ...          -0.920440   \n",
       "\n",
       "       Disgust_Evidence  Joy_Evidence  Fear_Evidence  Negative_Evidence  \\\n",
       "9740          -1.864838     -0.997349       0.116210           1.854495   \n",
       "9741          -1.702245     -1.118984      -0.030738           2.210246   \n",
       "9742          -1.702245     -1.118984      -0.030738           2.210246   \n",
       "9743          -1.702245     -1.118984      -0.030738           2.210246   \n",
       "9744          -1.702245     -1.118984      -0.030738           2.210246   \n",
       "...                 ...           ...            ...                ...   \n",
       "19475         -1.022377     -1.726208      -0.290350           2.163536   \n",
       "19476         -1.120620     -1.702867      -0.341188           2.049119   \n",
       "19477         -1.120620     -1.702867      -0.341188           2.049119   \n",
       "19478         -1.120620     -1.702867      -0.341188           2.049119   \n",
       "19479         -1.120620     -1.702867      -0.341188           2.049119   \n",
       "\n",
       "       Neutral_Evidence  Positive_Evidence  Sadness_Evidence  \\\n",
       "9740          -0.902997          -0.385946          0.262224   \n",
       "9741          -1.007481          -0.508893          0.199464   \n",
       "9742          -1.007481          -0.508893          0.199464   \n",
       "9743          -1.007481          -0.508893          0.199464   \n",
       "9744          -1.007481          -0.508893          0.199464   \n",
       "...                 ...                ...               ...   \n",
       "19475         -0.578703          -0.855910          0.324996   \n",
       "19476         -0.529083          -0.824312          0.307541   \n",
       "19477         -0.529083          -0.824312          0.307541   \n",
       "19478         -0.529083          -0.824312          0.307541   \n",
       "19479         -0.529083          -0.824312          0.307541   \n",
       "\n",
       "       Surprise_Evidence  Normed_Heart_Rate  \n",
       "9740           -1.695621          -0.898216  \n",
       "9741           -1.773442          -0.898216  \n",
       "9742           -1.773442          -0.898216  \n",
       "9743           -1.773442          -0.898216  \n",
       "9744           -1.773442          -0.898216  \n",
       "...                  ...                ...  \n",
       "19475          -1.221885           2.306587  \n",
       "19476          -1.318939           2.306587  \n",
       "19477          -1.318939           2.306587  \n",
       "19478          -1.318939           2.306587  \n",
       "19479          -1.318939           2.306587  \n",
       "\n",
       "[9740 rows x 26 columns]"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_DumbTo[sub_DumbTo.Subject_ID == 305]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "439d12ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Subject_ID</th>\n",
       "      <th>FPOGX</th>\n",
       "      <th>FPOGY</th>\n",
       "      <th>BPOGX</th>\n",
       "      <th>BPOGY</th>\n",
       "      <th>LPCX</th>\n",
       "      <th>LPCY</th>\n",
       "      <th>Normed_LPD</th>\n",
       "      <th>Normed_LPS</th>\n",
       "      <th>RPCX</th>\n",
       "      <th>...</th>\n",
       "      <th>Contempt_Evidence</th>\n",
       "      <th>Disgust_Evidence</th>\n",
       "      <th>Joy_Evidence</th>\n",
       "      <th>Fear_Evidence</th>\n",
       "      <th>Negative_Evidence</th>\n",
       "      <th>Neutral_Evidence</th>\n",
       "      <th>Positive_Evidence</th>\n",
       "      <th>Sadness_Evidence</th>\n",
       "      <th>Surprise_Evidence</th>\n",
       "      <th>Normed_Heart_Rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>300</td>\n",
       "      <td>0.68073</td>\n",
       "      <td>0.42222</td>\n",
       "      <td>0.68802</td>\n",
       "      <td>0.39259</td>\n",
       "      <td>0.25017</td>\n",
       "      <td>0.41186</td>\n",
       "      <td>1.808046</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.65154</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.261916</td>\n",
       "      <td>-1.104235</td>\n",
       "      <td>-3.085957</td>\n",
       "      <td>-0.276813</td>\n",
       "      <td>2.562142</td>\n",
       "      <td>0.683569</td>\n",
       "      <td>-1.535620</td>\n",
       "      <td>1.238940</td>\n",
       "      <td>-0.985122</td>\n",
       "      <td>3.416550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>300</td>\n",
       "      <td>0.68125</td>\n",
       "      <td>0.42685</td>\n",
       "      <td>0.68750</td>\n",
       "      <td>0.49259</td>\n",
       "      <td>0.25011</td>\n",
       "      <td>0.41384</td>\n",
       "      <td>-0.821955</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.65151</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.261916</td>\n",
       "      <td>-1.104235</td>\n",
       "      <td>-3.085957</td>\n",
       "      <td>-0.276813</td>\n",
       "      <td>2.562142</td>\n",
       "      <td>0.683569</td>\n",
       "      <td>-1.535620</td>\n",
       "      <td>1.238940</td>\n",
       "      <td>-0.985122</td>\n",
       "      <td>3.416550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>300</td>\n",
       "      <td>0.68125</td>\n",
       "      <td>0.43241</td>\n",
       "      <td>0.68594</td>\n",
       "      <td>0.50556</td>\n",
       "      <td>0.25011</td>\n",
       "      <td>0.41384</td>\n",
       "      <td>-0.821955</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.65146</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.261916</td>\n",
       "      <td>-1.104235</td>\n",
       "      <td>-3.085957</td>\n",
       "      <td>-0.276813</td>\n",
       "      <td>2.562142</td>\n",
       "      <td>0.683569</td>\n",
       "      <td>-1.535620</td>\n",
       "      <td>1.238940</td>\n",
       "      <td>-0.985122</td>\n",
       "      <td>3.416550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>300</td>\n",
       "      <td>0.87396</td>\n",
       "      <td>0.43611</td>\n",
       "      <td>2.58698</td>\n",
       "      <td>0.46574</td>\n",
       "      <td>0.24719</td>\n",
       "      <td>0.40959</td>\n",
       "      <td>-12.431953</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.60381</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.261916</td>\n",
       "      <td>-1.104235</td>\n",
       "      <td>-3.085957</td>\n",
       "      <td>-0.276813</td>\n",
       "      <td>2.562142</td>\n",
       "      <td>0.683569</td>\n",
       "      <td>-1.535620</td>\n",
       "      <td>1.238940</td>\n",
       "      <td>-0.985122</td>\n",
       "      <td>3.416550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>300</td>\n",
       "      <td>0.87396</td>\n",
       "      <td>0.43611</td>\n",
       "      <td>2.58698</td>\n",
       "      <td>0.46574</td>\n",
       "      <td>0.06935</td>\n",
       "      <td>0.33554</td>\n",
       "      <td>-11.791954</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.57383</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.261916</td>\n",
       "      <td>-1.104235</td>\n",
       "      <td>-3.085957</td>\n",
       "      <td>-0.276813</td>\n",
       "      <td>2.562142</td>\n",
       "      <td>0.683569</td>\n",
       "      <td>-1.535620</td>\n",
       "      <td>1.238940</td>\n",
       "      <td>-0.985122</td>\n",
       "      <td>3.416550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>418815</th>\n",
       "      <td>365</td>\n",
       "      <td>0.37031</td>\n",
       "      <td>0.39815</td>\n",
       "      <td>0.39792</td>\n",
       "      <td>0.38333</td>\n",
       "      <td>0.26668</td>\n",
       "      <td>0.72261</td>\n",
       "      <td>-2.647524</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.60355</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.407128</td>\n",
       "      <td>-0.156432</td>\n",
       "      <td>-1.473553</td>\n",
       "      <td>-1.841570</td>\n",
       "      <td>2.932494</td>\n",
       "      <td>-1.194771</td>\n",
       "      <td>-0.932807</td>\n",
       "      <td>-1.932029</td>\n",
       "      <td>-1.989926</td>\n",
       "      <td>-40.433041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>418816</th>\n",
       "      <td>365</td>\n",
       "      <td>0.37187</td>\n",
       "      <td>0.39907</td>\n",
       "      <td>0.39323</td>\n",
       "      <td>0.40648</td>\n",
       "      <td>0.26660</td>\n",
       "      <td>0.72223</td>\n",
       "      <td>-2.797525</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.60344</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.537363</td>\n",
       "      <td>-0.162792</td>\n",
       "      <td>-1.135028</td>\n",
       "      <td>-1.760911</td>\n",
       "      <td>2.925890</td>\n",
       "      <td>-1.501126</td>\n",
       "      <td>-0.740708</td>\n",
       "      <td>-2.024093</td>\n",
       "      <td>-1.986524</td>\n",
       "      <td>-40.433041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>418817</th>\n",
       "      <td>365</td>\n",
       "      <td>0.37240</td>\n",
       "      <td>0.40093</td>\n",
       "      <td>0.37865</td>\n",
       "      <td>0.42963</td>\n",
       "      <td>0.26636</td>\n",
       "      <td>0.72297</td>\n",
       "      <td>-3.027524</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.60332</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.537363</td>\n",
       "      <td>-0.162792</td>\n",
       "      <td>-1.135028</td>\n",
       "      <td>-1.760911</td>\n",
       "      <td>2.925890</td>\n",
       "      <td>-1.501126</td>\n",
       "      <td>-0.740708</td>\n",
       "      <td>-2.024093</td>\n",
       "      <td>-1.986524</td>\n",
       "      <td>-40.433041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>418818</th>\n",
       "      <td>365</td>\n",
       "      <td>0.37396</td>\n",
       "      <td>0.39815</td>\n",
       "      <td>0.39010</td>\n",
       "      <td>0.36019</td>\n",
       "      <td>0.26663</td>\n",
       "      <td>0.72167</td>\n",
       "      <td>-3.217525</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.60324</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.537363</td>\n",
       "      <td>-0.162792</td>\n",
       "      <td>-1.135028</td>\n",
       "      <td>-1.760911</td>\n",
       "      <td>2.925890</td>\n",
       "      <td>-1.501126</td>\n",
       "      <td>-0.740708</td>\n",
       "      <td>-2.024093</td>\n",
       "      <td>-1.986524</td>\n",
       "      <td>-40.433041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>418819</th>\n",
       "      <td>365</td>\n",
       "      <td>0.37448</td>\n",
       "      <td>0.39722</td>\n",
       "      <td>0.38646</td>\n",
       "      <td>0.37500</td>\n",
       "      <td>0.26653</td>\n",
       "      <td>0.72194</td>\n",
       "      <td>-2.657524</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.60324</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.537363</td>\n",
       "      <td>-0.162792</td>\n",
       "      <td>-1.135028</td>\n",
       "      <td>-1.760911</td>\n",
       "      <td>2.925890</td>\n",
       "      <td>-1.501126</td>\n",
       "      <td>-0.740708</td>\n",
       "      <td>-2.024093</td>\n",
       "      <td>-1.986524</td>\n",
       "      <td>-40.433041</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>418820 rows Ã— 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Subject_ID    FPOGX    FPOGY    BPOGX    BPOGY     LPCX     LPCY  \\\n",
       "0              300  0.68073  0.42222  0.68802  0.39259  0.25017  0.41186   \n",
       "1              300  0.68125  0.42685  0.68750  0.49259  0.25011  0.41384   \n",
       "2              300  0.68125  0.43241  0.68594  0.50556  0.25011  0.41384   \n",
       "3              300  0.87396  0.43611  2.58698  0.46574  0.24719  0.40959   \n",
       "4              300  0.87396  0.43611  2.58698  0.46574  0.06935  0.33554   \n",
       "...            ...      ...      ...      ...      ...      ...      ...   \n",
       "418815         365  0.37031  0.39815  0.39792  0.38333  0.26668  0.72261   \n",
       "418816         365  0.37187  0.39907  0.39323  0.40648  0.26660  0.72223   \n",
       "418817         365  0.37240  0.40093  0.37865  0.42963  0.26636  0.72297   \n",
       "418818         365  0.37396  0.39815  0.39010  0.36019  0.26663  0.72167   \n",
       "418819         365  0.37448  0.39722  0.38646  0.37500  0.26653  0.72194   \n",
       "\n",
       "        Normed_LPD  Normed_LPS     RPCX  ...  Contempt_Evidence  \\\n",
       "0         1.808046         0.0  0.65154  ...          -0.261916   \n",
       "1        -0.821955         0.0  0.65151  ...          -0.261916   \n",
       "2        -0.821955         1.0  0.65146  ...          -0.261916   \n",
       "3       -12.431953         0.0  0.60381  ...          -0.261916   \n",
       "4       -11.791954         0.0  0.57383  ...          -0.261916   \n",
       "...            ...         ...      ...  ...                ...   \n",
       "418815   -2.647524         0.0  0.60355  ...          -0.407128   \n",
       "418816   -2.797525         0.0  0.60344  ...          -0.537363   \n",
       "418817   -3.027524         0.0  0.60332  ...          -0.537363   \n",
       "418818   -3.217525         0.0  0.60324  ...          -0.537363   \n",
       "418819   -2.657524         0.0  0.60324  ...          -0.537363   \n",
       "\n",
       "        Disgust_Evidence  Joy_Evidence  Fear_Evidence  Negative_Evidence  \\\n",
       "0              -1.104235     -3.085957      -0.276813           2.562142   \n",
       "1              -1.104235     -3.085957      -0.276813           2.562142   \n",
       "2              -1.104235     -3.085957      -0.276813           2.562142   \n",
       "3              -1.104235     -3.085957      -0.276813           2.562142   \n",
       "4              -1.104235     -3.085957      -0.276813           2.562142   \n",
       "...                  ...           ...            ...                ...   \n",
       "418815         -0.156432     -1.473553      -1.841570           2.932494   \n",
       "418816         -0.162792     -1.135028      -1.760911           2.925890   \n",
       "418817         -0.162792     -1.135028      -1.760911           2.925890   \n",
       "418818         -0.162792     -1.135028      -1.760911           2.925890   \n",
       "418819         -0.162792     -1.135028      -1.760911           2.925890   \n",
       "\n",
       "        Neutral_Evidence  Positive_Evidence  Sadness_Evidence  \\\n",
       "0               0.683569          -1.535620          1.238940   \n",
       "1               0.683569          -1.535620          1.238940   \n",
       "2               0.683569          -1.535620          1.238940   \n",
       "3               0.683569          -1.535620          1.238940   \n",
       "4               0.683569          -1.535620          1.238940   \n",
       "...                  ...                ...               ...   \n",
       "418815         -1.194771          -0.932807         -1.932029   \n",
       "418816         -1.501126          -0.740708         -2.024093   \n",
       "418817         -1.501126          -0.740708         -2.024093   \n",
       "418818         -1.501126          -0.740708         -2.024093   \n",
       "418819         -1.501126          -0.740708         -2.024093   \n",
       "\n",
       "        Surprise_Evidence  Normed_Heart_Rate  \n",
       "0               -0.985122           3.416550  \n",
       "1               -0.985122           3.416550  \n",
       "2               -0.985122           3.416550  \n",
       "3               -0.985122           3.416550  \n",
       "4               -0.985122           3.416550  \n",
       "...                   ...                ...  \n",
       "418815          -1.989926         -40.433041  \n",
       "418816          -1.986524         -40.433041  \n",
       "418817          -1.986524         -40.433041  \n",
       "418818          -1.986524         -40.433041  \n",
       "418819          -1.986524         -40.433041  \n",
       "\n",
       "[418820 rows x 26 columns]"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_DumbTo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "1b3f5f77",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 305.,  306.,  307.,  308.,  309.,  311.,  313.,  319.,  321.,\n",
       "        323.,  324.,  325.,  326.,  327.,  329.,  330.,  331.,  332.,\n",
       "        333.,  334.,  335.,  337.,  338.,  339.,  341.,  342.,  346.,\n",
       "        347.,  349.,  350.,  351.,  352.,  353.,  354.,  355.,  356.,\n",
       "        357.,  358.,  360.,  361.,  363.,  365., 1000., 1001., 1002.,\n",
       "       1003., 1004., 1005.])"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_df_tot.Subject_ID.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "31a86ebf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 306.,  307.,  308.,  309.,  311.,  313.,  319.,  321.,  323.,\n",
       "        324.,  325.,  326.,  327.,  329.,  330.,  331.,  332.,  333.,\n",
       "        334.,  335.,  337.,  338.,  339.,  341.,  342.,  346.,  347.,\n",
       "        349.,  350.,  351.,  352.,  353.,  354.,  355.,  356.,  357.,\n",
       "        358.,  360.,  361.,  363.,  365., 1000., 1001., 1002., 1003.,\n",
       "       1004., 1005.])"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_training_subject_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "79d0cd21",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 70 is out of bounds for axis 0 with size 48",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-127-ee538478fb9d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0munique_training_subject_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdelete\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\tf\\lib\\site-packages\\numpy\\lib\\function_base.py\u001b[0m in \u001b[0;36mdelete\u001b[1;34m(arr, obj, axis)\u001b[0m\n\u001b[0;32m   4371\u001b[0m         \u001b[0mobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4372\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;33m-\u001b[0m\u001b[0mN\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mobj\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0mN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4373\u001b[1;33m             raise IndexError(\n\u001b[0m\u001b[0;32m   4374\u001b[0m                 \u001b[1;34m\"index %i is out of bounds for axis %i with \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4375\u001b[0m                 \"size %i\" % (obj, axis, N))\n",
      "\u001b[1;31mIndexError\u001b[0m: index 70 is out of bounds for axis 0 with size 48"
     ]
    }
   ],
   "source": [
    "np.delete(unique_training_subject_ids, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "c726b8b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "327ecd7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42, 9740, 25)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d2ad01da",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = ma.masked_array(y, mask=mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "70729294",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y[mask])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fedbae72",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_raw[:37]\n",
    "y_train = y[:37]\n",
    "x_valid = x_raw[37:]\n",
    "y_valid = y[37:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "114b0e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_for_smote = np.zeros((x_train.shape[0], x_train.shape[1] * x_train.shape[2]))\n",
    "\n",
    "for i, seq_feat_mat in enumerate(x_train):\n",
    "    x_for_smote[i] = np.reshape(seq_feat_mat, x_train.shape[1] * x_train.shape[2]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "80e42a6d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37, 243500)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_for_smote.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "33ab6e28",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(y_train==1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "81e1d71a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(y_valid==0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6f89f96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "strategy = {0:12, 1:33}\n",
    "oversample = SMOTE(sampling_strategy=strategy, random_state=0, k_neighbors=3)\n",
    "x_res, y_res = oversample.fit_resample(x_for_smote, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "50ea933d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45, 243500)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ea800016",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f59dd495",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_res_added = x_res[37:,]\n",
    "y_res_added = y_res[37:,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a29e8bbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Subject_ID</th>\n",
       "      <th>FPOGX</th>\n",
       "      <th>FPOGY</th>\n",
       "      <th>BPOGX</th>\n",
       "      <th>BPOGY</th>\n",
       "      <th>LPCX</th>\n",
       "      <th>LPCY</th>\n",
       "      <th>Normed_LPD</th>\n",
       "      <th>Normed_LPS</th>\n",
       "      <th>...</th>\n",
       "      <th>Contempt_Evidence</th>\n",
       "      <th>Disgust_Evidence</th>\n",
       "      <th>Joy_Evidence</th>\n",
       "      <th>Fear_Evidence</th>\n",
       "      <th>Negative_Evidence</th>\n",
       "      <th>Neutral_Evidence</th>\n",
       "      <th>Positive_Evidence</th>\n",
       "      <th>Sadness_Evidence</th>\n",
       "      <th>Surprise_Evidence</th>\n",
       "      <th>Normed_Heart_Rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>300</td>\n",
       "      <td>0.68073</td>\n",
       "      <td>0.42222</td>\n",
       "      <td>0.68802</td>\n",
       "      <td>0.39259</td>\n",
       "      <td>0.25017</td>\n",
       "      <td>0.41186</td>\n",
       "      <td>1.808046</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.261916</td>\n",
       "      <td>-1.104235</td>\n",
       "      <td>-3.085957</td>\n",
       "      <td>-0.276813</td>\n",
       "      <td>2.562142</td>\n",
       "      <td>0.683569</td>\n",
       "      <td>-1.535620</td>\n",
       "      <td>1.238940</td>\n",
       "      <td>-0.985122</td>\n",
       "      <td>3.416550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>300</td>\n",
       "      <td>0.68125</td>\n",
       "      <td>0.42685</td>\n",
       "      <td>0.68750</td>\n",
       "      <td>0.49259</td>\n",
       "      <td>0.25011</td>\n",
       "      <td>0.41384</td>\n",
       "      <td>-0.821955</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.261916</td>\n",
       "      <td>-1.104235</td>\n",
       "      <td>-3.085957</td>\n",
       "      <td>-0.276813</td>\n",
       "      <td>2.562142</td>\n",
       "      <td>0.683569</td>\n",
       "      <td>-1.535620</td>\n",
       "      <td>1.238940</td>\n",
       "      <td>-0.985122</td>\n",
       "      <td>3.416550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>300</td>\n",
       "      <td>0.68125</td>\n",
       "      <td>0.43241</td>\n",
       "      <td>0.68594</td>\n",
       "      <td>0.50556</td>\n",
       "      <td>0.25011</td>\n",
       "      <td>0.41384</td>\n",
       "      <td>-0.821955</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.261916</td>\n",
       "      <td>-1.104235</td>\n",
       "      <td>-3.085957</td>\n",
       "      <td>-0.276813</td>\n",
       "      <td>2.562142</td>\n",
       "      <td>0.683569</td>\n",
       "      <td>-1.535620</td>\n",
       "      <td>1.238940</td>\n",
       "      <td>-0.985122</td>\n",
       "      <td>3.416550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>300</td>\n",
       "      <td>0.87396</td>\n",
       "      <td>0.43611</td>\n",
       "      <td>2.58698</td>\n",
       "      <td>0.46574</td>\n",
       "      <td>0.24719</td>\n",
       "      <td>0.40959</td>\n",
       "      <td>-12.431953</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.261916</td>\n",
       "      <td>-1.104235</td>\n",
       "      <td>-3.085957</td>\n",
       "      <td>-0.276813</td>\n",
       "      <td>2.562142</td>\n",
       "      <td>0.683569</td>\n",
       "      <td>-1.535620</td>\n",
       "      <td>1.238940</td>\n",
       "      <td>-0.985122</td>\n",
       "      <td>3.416550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>300</td>\n",
       "      <td>0.87396</td>\n",
       "      <td>0.43611</td>\n",
       "      <td>2.58698</td>\n",
       "      <td>0.46574</td>\n",
       "      <td>0.06935</td>\n",
       "      <td>0.33554</td>\n",
       "      <td>-11.791954</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.261916</td>\n",
       "      <td>-1.104235</td>\n",
       "      <td>-3.085957</td>\n",
       "      <td>-0.276813</td>\n",
       "      <td>2.562142</td>\n",
       "      <td>0.683569</td>\n",
       "      <td>-1.535620</td>\n",
       "      <td>1.238940</td>\n",
       "      <td>-0.985122</td>\n",
       "      <td>3.416550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>418815</th>\n",
       "      <td>418816</td>\n",
       "      <td>365</td>\n",
       "      <td>0.37031</td>\n",
       "      <td>0.39815</td>\n",
       "      <td>0.39792</td>\n",
       "      <td>0.38333</td>\n",
       "      <td>0.26668</td>\n",
       "      <td>0.72261</td>\n",
       "      <td>-2.647524</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.407128</td>\n",
       "      <td>-0.156432</td>\n",
       "      <td>-1.473553</td>\n",
       "      <td>-1.841570</td>\n",
       "      <td>2.932494</td>\n",
       "      <td>-1.194771</td>\n",
       "      <td>-0.932807</td>\n",
       "      <td>-1.932029</td>\n",
       "      <td>-1.989926</td>\n",
       "      <td>-40.433041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>418816</th>\n",
       "      <td>418817</td>\n",
       "      <td>365</td>\n",
       "      <td>0.37187</td>\n",
       "      <td>0.39907</td>\n",
       "      <td>0.39323</td>\n",
       "      <td>0.40648</td>\n",
       "      <td>0.26660</td>\n",
       "      <td>0.72223</td>\n",
       "      <td>-2.797525</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.537363</td>\n",
       "      <td>-0.162792</td>\n",
       "      <td>-1.135028</td>\n",
       "      <td>-1.760911</td>\n",
       "      <td>2.925890</td>\n",
       "      <td>-1.501126</td>\n",
       "      <td>-0.740708</td>\n",
       "      <td>-2.024093</td>\n",
       "      <td>-1.986524</td>\n",
       "      <td>-40.433041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>418817</th>\n",
       "      <td>418818</td>\n",
       "      <td>365</td>\n",
       "      <td>0.37240</td>\n",
       "      <td>0.40093</td>\n",
       "      <td>0.37865</td>\n",
       "      <td>0.42963</td>\n",
       "      <td>0.26636</td>\n",
       "      <td>0.72297</td>\n",
       "      <td>-3.027524</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.537363</td>\n",
       "      <td>-0.162792</td>\n",
       "      <td>-1.135028</td>\n",
       "      <td>-1.760911</td>\n",
       "      <td>2.925890</td>\n",
       "      <td>-1.501126</td>\n",
       "      <td>-0.740708</td>\n",
       "      <td>-2.024093</td>\n",
       "      <td>-1.986524</td>\n",
       "      <td>-40.433041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>418818</th>\n",
       "      <td>418819</td>\n",
       "      <td>365</td>\n",
       "      <td>0.37396</td>\n",
       "      <td>0.39815</td>\n",
       "      <td>0.39010</td>\n",
       "      <td>0.36019</td>\n",
       "      <td>0.26663</td>\n",
       "      <td>0.72167</td>\n",
       "      <td>-3.217525</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.537363</td>\n",
       "      <td>-0.162792</td>\n",
       "      <td>-1.135028</td>\n",
       "      <td>-1.760911</td>\n",
       "      <td>2.925890</td>\n",
       "      <td>-1.501126</td>\n",
       "      <td>-0.740708</td>\n",
       "      <td>-2.024093</td>\n",
       "      <td>-1.986524</td>\n",
       "      <td>-40.433041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>418819</th>\n",
       "      <td>418820</td>\n",
       "      <td>365</td>\n",
       "      <td>0.37448</td>\n",
       "      <td>0.39722</td>\n",
       "      <td>0.38646</td>\n",
       "      <td>0.37500</td>\n",
       "      <td>0.26653</td>\n",
       "      <td>0.72194</td>\n",
       "      <td>-2.657524</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.537363</td>\n",
       "      <td>-0.162792</td>\n",
       "      <td>-1.135028</td>\n",
       "      <td>-1.760911</td>\n",
       "      <td>2.925890</td>\n",
       "      <td>-1.501126</td>\n",
       "      <td>-0.740708</td>\n",
       "      <td>-2.024093</td>\n",
       "      <td>-1.986524</td>\n",
       "      <td>-40.433041</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>418820 rows Ã— 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         index  Subject_ID    FPOGX    FPOGY    BPOGX    BPOGY     LPCX  \\\n",
       "0            1         300  0.68073  0.42222  0.68802  0.39259  0.25017   \n",
       "1            2         300  0.68125  0.42685  0.68750  0.49259  0.25011   \n",
       "2            3         300  0.68125  0.43241  0.68594  0.50556  0.25011   \n",
       "3            4         300  0.87396  0.43611  2.58698  0.46574  0.24719   \n",
       "4            5         300  0.87396  0.43611  2.58698  0.46574  0.06935   \n",
       "...        ...         ...      ...      ...      ...      ...      ...   \n",
       "418815  418816         365  0.37031  0.39815  0.39792  0.38333  0.26668   \n",
       "418816  418817         365  0.37187  0.39907  0.39323  0.40648  0.26660   \n",
       "418817  418818         365  0.37240  0.40093  0.37865  0.42963  0.26636   \n",
       "418818  418819         365  0.37396  0.39815  0.39010  0.36019  0.26663   \n",
       "418819  418820         365  0.37448  0.39722  0.38646  0.37500  0.26653   \n",
       "\n",
       "           LPCY  Normed_LPD  Normed_LPS  ...  Contempt_Evidence  \\\n",
       "0       0.41186    1.808046         0.0  ...          -0.261916   \n",
       "1       0.41384   -0.821955         0.0  ...          -0.261916   \n",
       "2       0.41384   -0.821955         1.0  ...          -0.261916   \n",
       "3       0.40959  -12.431953         0.0  ...          -0.261916   \n",
       "4       0.33554  -11.791954         0.0  ...          -0.261916   \n",
       "...         ...         ...         ...  ...                ...   \n",
       "418815  0.72261   -2.647524         0.0  ...          -0.407128   \n",
       "418816  0.72223   -2.797525         0.0  ...          -0.537363   \n",
       "418817  0.72297   -3.027524         0.0  ...          -0.537363   \n",
       "418818  0.72167   -3.217525         0.0  ...          -0.537363   \n",
       "418819  0.72194   -2.657524         0.0  ...          -0.537363   \n",
       "\n",
       "        Disgust_Evidence  Joy_Evidence  Fear_Evidence  Negative_Evidence  \\\n",
       "0              -1.104235     -3.085957      -0.276813           2.562142   \n",
       "1              -1.104235     -3.085957      -0.276813           2.562142   \n",
       "2              -1.104235     -3.085957      -0.276813           2.562142   \n",
       "3              -1.104235     -3.085957      -0.276813           2.562142   \n",
       "4              -1.104235     -3.085957      -0.276813           2.562142   \n",
       "...                  ...           ...            ...                ...   \n",
       "418815         -0.156432     -1.473553      -1.841570           2.932494   \n",
       "418816         -0.162792     -1.135028      -1.760911           2.925890   \n",
       "418817         -0.162792     -1.135028      -1.760911           2.925890   \n",
       "418818         -0.162792     -1.135028      -1.760911           2.925890   \n",
       "418819         -0.162792     -1.135028      -1.760911           2.925890   \n",
       "\n",
       "        Neutral_Evidence  Positive_Evidence  Sadness_Evidence  \\\n",
       "0               0.683569          -1.535620          1.238940   \n",
       "1               0.683569          -1.535620          1.238940   \n",
       "2               0.683569          -1.535620          1.238940   \n",
       "3               0.683569          -1.535620          1.238940   \n",
       "4               0.683569          -1.535620          1.238940   \n",
       "...                  ...                ...               ...   \n",
       "418815         -1.194771          -0.932807         -1.932029   \n",
       "418816         -1.501126          -0.740708         -2.024093   \n",
       "418817         -1.501126          -0.740708         -2.024093   \n",
       "418818         -1.501126          -0.740708         -2.024093   \n",
       "418819         -1.501126          -0.740708         -2.024093   \n",
       "\n",
       "        Surprise_Evidence  Normed_Heart_Rate  \n",
       "0               -0.985122           3.416550  \n",
       "1               -0.985122           3.416550  \n",
       "2               -0.985122           3.416550  \n",
       "3               -0.985122           3.416550  \n",
       "4               -0.985122           3.416550  \n",
       "...                   ...                ...  \n",
       "418815          -1.989926         -40.433041  \n",
       "418816          -1.986524         -40.433041  \n",
       "418817          -1.986524         -40.433041  \n",
       "418818          -1.986524         -40.433041  \n",
       "418819          -1.986524         -40.433041  \n",
       "\n",
       "[418820 rows x 27 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_DumbTo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7e234fc1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_added_df = pd.DataFrame(columns=related_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "807442bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(related_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a2cf818f",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_x_added = {}\n",
    "for i, item in enumerate(x_res_added):\n",
    "    item = np.reshape(item, (cut_off, -1))\n",
    "    r_id = np.full((cut_off, 1), int(1000 + i))\n",
    "    item =  np.hstack((r_id,item))\n",
    "    item_df = pd.DataFrame(data=item, columns=related_features)\n",
    "    \n",
    "    x_added_df = x_added_df.append(item_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fe65ea93",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Subject_ID</th>\n",
       "      <th>FPOGX</th>\n",
       "      <th>FPOGY</th>\n",
       "      <th>BPOGX</th>\n",
       "      <th>BPOGY</th>\n",
       "      <th>LPCX</th>\n",
       "      <th>LPCY</th>\n",
       "      <th>Normed_LPD</th>\n",
       "      <th>Normed_LPS</th>\n",
       "      <th>RPCX</th>\n",
       "      <th>...</th>\n",
       "      <th>Contempt_Evidence</th>\n",
       "      <th>Disgust_Evidence</th>\n",
       "      <th>Joy_Evidence</th>\n",
       "      <th>Fear_Evidence</th>\n",
       "      <th>Negative_Evidence</th>\n",
       "      <th>Neutral_Evidence</th>\n",
       "      <th>Positive_Evidence</th>\n",
       "      <th>Sadness_Evidence</th>\n",
       "      <th>Surprise_Evidence</th>\n",
       "      <th>Normed_Heart_Rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.516881</td>\n",
       "      <td>0.906728</td>\n",
       "      <td>0.516545</td>\n",
       "      <td>1.003612</td>\n",
       "      <td>0.188472</td>\n",
       "      <td>0.383550</td>\n",
       "      <td>-1.872127</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.583692</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.765096</td>\n",
       "      <td>-0.136646</td>\n",
       "      <td>-1.863887</td>\n",
       "      <td>-0.621926</td>\n",
       "      <td>1.232283</td>\n",
       "      <td>0.682812</td>\n",
       "      <td>-0.833982</td>\n",
       "      <td>0.245861</td>\n",
       "      <td>-0.590043</td>\n",
       "      <td>-4.388696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.441531</td>\n",
       "      <td>1.080163</td>\n",
       "      <td>0.291496</td>\n",
       "      <td>1.425839</td>\n",
       "      <td>0.445332</td>\n",
       "      <td>0.390002</td>\n",
       "      <td>1.008562</td>\n",
       "      <td>0.645894</td>\n",
       "      <td>0.579850</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.593562</td>\n",
       "      <td>-0.144992</td>\n",
       "      <td>-2.088141</td>\n",
       "      <td>-0.390919</td>\n",
       "      <td>1.472481</td>\n",
       "      <td>0.655985</td>\n",
       "      <td>-1.042419</td>\n",
       "      <td>0.236445</td>\n",
       "      <td>-0.560105</td>\n",
       "      <td>-4.388696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.094191</td>\n",
       "      <td>0.887504</td>\n",
       "      <td>0.291496</td>\n",
       "      <td>1.425839</td>\n",
       "      <td>0.197567</td>\n",
       "      <td>0.483405</td>\n",
       "      <td>-11.005069</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.579850</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.593562</td>\n",
       "      <td>-0.144992</td>\n",
       "      <td>-2.088141</td>\n",
       "      <td>-0.390919</td>\n",
       "      <td>1.472481</td>\n",
       "      <td>0.655985</td>\n",
       "      <td>-1.042419</td>\n",
       "      <td>0.236445</td>\n",
       "      <td>-0.560105</td>\n",
       "      <td>-4.388696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.197514</td>\n",
       "      <td>1.262381</td>\n",
       "      <td>0.300834</td>\n",
       "      <td>1.637585</td>\n",
       "      <td>0.179599</td>\n",
       "      <td>0.408848</td>\n",
       "      <td>-5.008056</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.570405</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.593562</td>\n",
       "      <td>-0.144992</td>\n",
       "      <td>-2.088141</td>\n",
       "      <td>-0.390919</td>\n",
       "      <td>1.472481</td>\n",
       "      <td>0.655985</td>\n",
       "      <td>-1.042419</td>\n",
       "      <td>0.236445</td>\n",
       "      <td>-0.560105</td>\n",
       "      <td>-4.388696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1000.0</td>\n",
       "      <td>0.156138</td>\n",
       "      <td>1.335348</td>\n",
       "      <td>0.227164</td>\n",
       "      <td>1.644166</td>\n",
       "      <td>0.179822</td>\n",
       "      <td>0.408904</td>\n",
       "      <td>-5.128453</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.561112</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.593562</td>\n",
       "      <td>-0.144992</td>\n",
       "      <td>-2.088141</td>\n",
       "      <td>-0.390919</td>\n",
       "      <td>1.472481</td>\n",
       "      <td>0.655985</td>\n",
       "      <td>-1.042419</td>\n",
       "      <td>0.236445</td>\n",
       "      <td>-0.560105</td>\n",
       "      <td>-4.388696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9735</th>\n",
       "      <td>1007.0</td>\n",
       "      <td>0.444660</td>\n",
       "      <td>0.359526</td>\n",
       "      <td>0.698782</td>\n",
       "      <td>0.432274</td>\n",
       "      <td>0.407118</td>\n",
       "      <td>0.717636</td>\n",
       "      <td>-5.347173</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.727668</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.095808</td>\n",
       "      <td>-0.744811</td>\n",
       "      <td>-1.386102</td>\n",
       "      <td>-0.109323</td>\n",
       "      <td>0.396394</td>\n",
       "      <td>-0.211663</td>\n",
       "      <td>-0.469004</td>\n",
       "      <td>-0.248212</td>\n",
       "      <td>-0.755508</td>\n",
       "      <td>36.840069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9736</th>\n",
       "      <td>1007.0</td>\n",
       "      <td>0.694382</td>\n",
       "      <td>0.500890</td>\n",
       "      <td>0.716677</td>\n",
       "      <td>0.865497</td>\n",
       "      <td>0.407701</td>\n",
       "      <td>0.710676</td>\n",
       "      <td>-6.512259</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.642196</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.095808</td>\n",
       "      <td>-0.744811</td>\n",
       "      <td>-1.386102</td>\n",
       "      <td>-0.109323</td>\n",
       "      <td>0.396394</td>\n",
       "      <td>-0.211663</td>\n",
       "      <td>-0.469004</td>\n",
       "      <td>-0.248212</td>\n",
       "      <td>-0.755508</td>\n",
       "      <td>36.840069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9737</th>\n",
       "      <td>1007.0</td>\n",
       "      <td>0.706304</td>\n",
       "      <td>0.504890</td>\n",
       "      <td>0.716677</td>\n",
       "      <td>0.865497</td>\n",
       "      <td>0.497037</td>\n",
       "      <td>0.785391</td>\n",
       "      <td>-6.250959</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.642196</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.095808</td>\n",
       "      <td>-0.744811</td>\n",
       "      <td>-1.386102</td>\n",
       "      <td>-0.109323</td>\n",
       "      <td>0.396394</td>\n",
       "      <td>-0.211663</td>\n",
       "      <td>-0.469004</td>\n",
       "      <td>-0.248212</td>\n",
       "      <td>-0.755508</td>\n",
       "      <td>36.840069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9738</th>\n",
       "      <td>1007.0</td>\n",
       "      <td>0.735776</td>\n",
       "      <td>0.518086</td>\n",
       "      <td>0.792718</td>\n",
       "      <td>0.901090</td>\n",
       "      <td>0.404317</td>\n",
       "      <td>0.724950</td>\n",
       "      <td>-6.724377</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.642066</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.089166</td>\n",
       "      <td>-0.873999</td>\n",
       "      <td>-1.361231</td>\n",
       "      <td>-0.214787</td>\n",
       "      <td>0.020447</td>\n",
       "      <td>-0.107674</td>\n",
       "      <td>-0.310357</td>\n",
       "      <td>0.010008</td>\n",
       "      <td>-0.636303</td>\n",
       "      <td>36.840069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9739</th>\n",
       "      <td>1007.0</td>\n",
       "      <td>0.757953</td>\n",
       "      <td>0.537008</td>\n",
       "      <td>0.818161</td>\n",
       "      <td>0.716998</td>\n",
       "      <td>0.404227</td>\n",
       "      <td>0.679830</td>\n",
       "      <td>-6.626922</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.727611</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.089166</td>\n",
       "      <td>-0.873999</td>\n",
       "      <td>-1.361231</td>\n",
       "      <td>-0.214787</td>\n",
       "      <td>0.020447</td>\n",
       "      <td>-0.107674</td>\n",
       "      <td>-0.310357</td>\n",
       "      <td>0.010008</td>\n",
       "      <td>-0.636303</td>\n",
       "      <td>36.840069</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>77920 rows Ã— 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Subject_ID     FPOGX     FPOGY     BPOGX     BPOGY      LPCX      LPCY  \\\n",
       "0         1000.0  0.516881  0.906728  0.516545  1.003612  0.188472  0.383550   \n",
       "1         1000.0  0.441531  1.080163  0.291496  1.425839  0.445332  0.390002   \n",
       "2         1000.0  0.094191  0.887504  0.291496  1.425839  0.197567  0.483405   \n",
       "3         1000.0  0.197514  1.262381  0.300834  1.637585  0.179599  0.408848   \n",
       "4         1000.0  0.156138  1.335348  0.227164  1.644166  0.179822  0.408904   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "9735      1007.0  0.444660  0.359526  0.698782  0.432274  0.407118  0.717636   \n",
       "9736      1007.0  0.694382  0.500890  0.716677  0.865497  0.407701  0.710676   \n",
       "9737      1007.0  0.706304  0.504890  0.716677  0.865497  0.497037  0.785391   \n",
       "9738      1007.0  0.735776  0.518086  0.792718  0.901090  0.404317  0.724950   \n",
       "9739      1007.0  0.757953  0.537008  0.818161  0.716998  0.404227  0.679830   \n",
       "\n",
       "      Normed_LPD  Normed_LPS      RPCX  ...  Contempt_Evidence  \\\n",
       "0      -1.872127    0.000000  0.583692  ...          -0.765096   \n",
       "1       1.008562    0.645894  0.579850  ...          -0.593562   \n",
       "2     -11.005069    0.000000  0.579850  ...          -0.593562   \n",
       "3      -5.008056    0.000000  0.570405  ...          -0.593562   \n",
       "4      -5.128453    0.000000  0.561112  ...          -0.593562   \n",
       "...          ...         ...       ...  ...                ...   \n",
       "9735   -5.347173    0.000000  0.727668  ...          -1.095808   \n",
       "9736   -6.512259    0.000000  0.642196  ...          -1.095808   \n",
       "9737   -6.250959    0.000000  0.642196  ...          -1.095808   \n",
       "9738   -6.724377    0.000000  0.642066  ...          -1.089166   \n",
       "9739   -6.626922    0.000000  0.727611  ...          -1.089166   \n",
       "\n",
       "      Disgust_Evidence  Joy_Evidence  Fear_Evidence  Negative_Evidence  \\\n",
       "0            -0.136646     -1.863887      -0.621926           1.232283   \n",
       "1            -0.144992     -2.088141      -0.390919           1.472481   \n",
       "2            -0.144992     -2.088141      -0.390919           1.472481   \n",
       "3            -0.144992     -2.088141      -0.390919           1.472481   \n",
       "4            -0.144992     -2.088141      -0.390919           1.472481   \n",
       "...                ...           ...            ...                ...   \n",
       "9735         -0.744811     -1.386102      -0.109323           0.396394   \n",
       "9736         -0.744811     -1.386102      -0.109323           0.396394   \n",
       "9737         -0.744811     -1.386102      -0.109323           0.396394   \n",
       "9738         -0.873999     -1.361231      -0.214787           0.020447   \n",
       "9739         -0.873999     -1.361231      -0.214787           0.020447   \n",
       "\n",
       "      Neutral_Evidence  Positive_Evidence  Sadness_Evidence  \\\n",
       "0             0.682812          -0.833982          0.245861   \n",
       "1             0.655985          -1.042419          0.236445   \n",
       "2             0.655985          -1.042419          0.236445   \n",
       "3             0.655985          -1.042419          0.236445   \n",
       "4             0.655985          -1.042419          0.236445   \n",
       "...                ...                ...               ...   \n",
       "9735         -0.211663          -0.469004         -0.248212   \n",
       "9736         -0.211663          -0.469004         -0.248212   \n",
       "9737         -0.211663          -0.469004         -0.248212   \n",
       "9738         -0.107674          -0.310357          0.010008   \n",
       "9739         -0.107674          -0.310357          0.010008   \n",
       "\n",
       "      Surprise_Evidence  Normed_Heart_Rate  \n",
       "0             -0.590043          -4.388696  \n",
       "1             -0.560105          -4.388696  \n",
       "2             -0.560105          -4.388696  \n",
       "3             -0.560105          -4.388696  \n",
       "4             -0.560105          -4.388696  \n",
       "...                 ...                ...  \n",
       "9735          -0.755508          36.840069  \n",
       "9736          -0.755508          36.840069  \n",
       "9737          -0.755508          36.840069  \n",
       "9738          -0.636303          36.840069  \n",
       "9739          -0.636303          36.840069  \n",
       "\n",
       "[77920 rows x 26 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_added_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9764b1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_added_to_id = {int(i): y_res_added[int(1000 - i)] for i in x_added_df.Subject_ID.tolist()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7de62904",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1000: 0, 1001: 0, 1002: 0, 1003: 0, 1004: 0, 1005: 0, 1006: 0, 1007: 0}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_added_to_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fa7ee910",
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_id_unique = sub_DumbTo.Subject_ID.unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "342f0220",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_to_id = {k: id_to_y[k] for k in subject_id_unique[:37]}\n",
    "y_train_to_id.update(y_added_to_id)\n",
    "y_valid_to_id = {k: id_to_y[k] for k in subject_id_unique[37:]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9def44f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{357: 0, 358: 1, 360: 1, 361: 1, 363: 1, 365: 0}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_valid_to_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cb37a065",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_df = sub_DumbTo.iloc[:37*cut_off]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cfd58296",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_valid_df = sub_DumbTo.iloc[37*cut_off:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f7fefb2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_df = x_train_df.drop(columns=[\"index\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "65208ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_df_tot = x_train_df.append(x_added_df)\n",
    "x_train_df_tot = x_train_df_tot.reset_index().drop(columns=[\"index\"]).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "78e731dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Subject_ID</th>\n",
       "      <th>FPOGX</th>\n",
       "      <th>FPOGY</th>\n",
       "      <th>BPOGX</th>\n",
       "      <th>BPOGY</th>\n",
       "      <th>LPCX</th>\n",
       "      <th>LPCY</th>\n",
       "      <th>Normed_LPD</th>\n",
       "      <th>Normed_LPS</th>\n",
       "      <th>...</th>\n",
       "      <th>Contempt_Evidence</th>\n",
       "      <th>Disgust_Evidence</th>\n",
       "      <th>Joy_Evidence</th>\n",
       "      <th>Fear_Evidence</th>\n",
       "      <th>Negative_Evidence</th>\n",
       "      <th>Neutral_Evidence</th>\n",
       "      <th>Positive_Evidence</th>\n",
       "      <th>Sadness_Evidence</th>\n",
       "      <th>Surprise_Evidence</th>\n",
       "      <th>Normed_Heart_Rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>300.0</td>\n",
       "      <td>0.680730</td>\n",
       "      <td>0.422220</td>\n",
       "      <td>0.688020</td>\n",
       "      <td>0.392590</td>\n",
       "      <td>0.250170</td>\n",
       "      <td>0.411860</td>\n",
       "      <td>1.808046</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.261916</td>\n",
       "      <td>-1.104235</td>\n",
       "      <td>-3.085957</td>\n",
       "      <td>-0.276813</td>\n",
       "      <td>2.562142</td>\n",
       "      <td>0.683569</td>\n",
       "      <td>-1.535620</td>\n",
       "      <td>1.238940</td>\n",
       "      <td>-0.985122</td>\n",
       "      <td>3.416550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>300.0</td>\n",
       "      <td>0.681250</td>\n",
       "      <td>0.426850</td>\n",
       "      <td>0.687500</td>\n",
       "      <td>0.492590</td>\n",
       "      <td>0.250110</td>\n",
       "      <td>0.413840</td>\n",
       "      <td>-0.821955</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.261916</td>\n",
       "      <td>-1.104235</td>\n",
       "      <td>-3.085957</td>\n",
       "      <td>-0.276813</td>\n",
       "      <td>2.562142</td>\n",
       "      <td>0.683569</td>\n",
       "      <td>-1.535620</td>\n",
       "      <td>1.238940</td>\n",
       "      <td>-0.985122</td>\n",
       "      <td>3.416550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>300.0</td>\n",
       "      <td>0.681250</td>\n",
       "      <td>0.432410</td>\n",
       "      <td>0.685940</td>\n",
       "      <td>0.505560</td>\n",
       "      <td>0.250110</td>\n",
       "      <td>0.413840</td>\n",
       "      <td>-0.821955</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.261916</td>\n",
       "      <td>-1.104235</td>\n",
       "      <td>-3.085957</td>\n",
       "      <td>-0.276813</td>\n",
       "      <td>2.562142</td>\n",
       "      <td>0.683569</td>\n",
       "      <td>-1.535620</td>\n",
       "      <td>1.238940</td>\n",
       "      <td>-0.985122</td>\n",
       "      <td>3.416550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>300.0</td>\n",
       "      <td>0.873960</td>\n",
       "      <td>0.436110</td>\n",
       "      <td>2.586980</td>\n",
       "      <td>0.465740</td>\n",
       "      <td>0.247190</td>\n",
       "      <td>0.409590</td>\n",
       "      <td>-12.431953</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.261916</td>\n",
       "      <td>-1.104235</td>\n",
       "      <td>-3.085957</td>\n",
       "      <td>-0.276813</td>\n",
       "      <td>2.562142</td>\n",
       "      <td>0.683569</td>\n",
       "      <td>-1.535620</td>\n",
       "      <td>1.238940</td>\n",
       "      <td>-0.985122</td>\n",
       "      <td>3.416550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>300.0</td>\n",
       "      <td>0.873960</td>\n",
       "      <td>0.436110</td>\n",
       "      <td>2.586980</td>\n",
       "      <td>0.465740</td>\n",
       "      <td>0.069350</td>\n",
       "      <td>0.335540</td>\n",
       "      <td>-11.791954</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.261916</td>\n",
       "      <td>-1.104235</td>\n",
       "      <td>-3.085957</td>\n",
       "      <td>-0.276813</td>\n",
       "      <td>2.562142</td>\n",
       "      <td>0.683569</td>\n",
       "      <td>-1.535620</td>\n",
       "      <td>1.238940</td>\n",
       "      <td>-0.985122</td>\n",
       "      <td>3.416550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>438295</th>\n",
       "      <td>438295</td>\n",
       "      <td>1007.0</td>\n",
       "      <td>0.444660</td>\n",
       "      <td>0.359526</td>\n",
       "      <td>0.698782</td>\n",
       "      <td>0.432274</td>\n",
       "      <td>0.407118</td>\n",
       "      <td>0.717636</td>\n",
       "      <td>-5.347173</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.095808</td>\n",
       "      <td>-0.744811</td>\n",
       "      <td>-1.386102</td>\n",
       "      <td>-0.109323</td>\n",
       "      <td>0.396394</td>\n",
       "      <td>-0.211663</td>\n",
       "      <td>-0.469004</td>\n",
       "      <td>-0.248212</td>\n",
       "      <td>-0.755508</td>\n",
       "      <td>36.840069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>438296</th>\n",
       "      <td>438296</td>\n",
       "      <td>1007.0</td>\n",
       "      <td>0.694382</td>\n",
       "      <td>0.500890</td>\n",
       "      <td>0.716677</td>\n",
       "      <td>0.865497</td>\n",
       "      <td>0.407701</td>\n",
       "      <td>0.710676</td>\n",
       "      <td>-6.512259</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.095808</td>\n",
       "      <td>-0.744811</td>\n",
       "      <td>-1.386102</td>\n",
       "      <td>-0.109323</td>\n",
       "      <td>0.396394</td>\n",
       "      <td>-0.211663</td>\n",
       "      <td>-0.469004</td>\n",
       "      <td>-0.248212</td>\n",
       "      <td>-0.755508</td>\n",
       "      <td>36.840069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>438297</th>\n",
       "      <td>438297</td>\n",
       "      <td>1007.0</td>\n",
       "      <td>0.706304</td>\n",
       "      <td>0.504890</td>\n",
       "      <td>0.716677</td>\n",
       "      <td>0.865497</td>\n",
       "      <td>0.497037</td>\n",
       "      <td>0.785391</td>\n",
       "      <td>-6.250959</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.095808</td>\n",
       "      <td>-0.744811</td>\n",
       "      <td>-1.386102</td>\n",
       "      <td>-0.109323</td>\n",
       "      <td>0.396394</td>\n",
       "      <td>-0.211663</td>\n",
       "      <td>-0.469004</td>\n",
       "      <td>-0.248212</td>\n",
       "      <td>-0.755508</td>\n",
       "      <td>36.840069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>438298</th>\n",
       "      <td>438298</td>\n",
       "      <td>1007.0</td>\n",
       "      <td>0.735776</td>\n",
       "      <td>0.518086</td>\n",
       "      <td>0.792718</td>\n",
       "      <td>0.901090</td>\n",
       "      <td>0.404317</td>\n",
       "      <td>0.724950</td>\n",
       "      <td>-6.724377</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.089166</td>\n",
       "      <td>-0.873999</td>\n",
       "      <td>-1.361231</td>\n",
       "      <td>-0.214787</td>\n",
       "      <td>0.020447</td>\n",
       "      <td>-0.107674</td>\n",
       "      <td>-0.310357</td>\n",
       "      <td>0.010008</td>\n",
       "      <td>-0.636303</td>\n",
       "      <td>36.840069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>438299</th>\n",
       "      <td>438299</td>\n",
       "      <td>1007.0</td>\n",
       "      <td>0.757953</td>\n",
       "      <td>0.537008</td>\n",
       "      <td>0.818161</td>\n",
       "      <td>0.716998</td>\n",
       "      <td>0.404227</td>\n",
       "      <td>0.679830</td>\n",
       "      <td>-6.626922</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.089166</td>\n",
       "      <td>-0.873999</td>\n",
       "      <td>-1.361231</td>\n",
       "      <td>-0.214787</td>\n",
       "      <td>0.020447</td>\n",
       "      <td>-0.107674</td>\n",
       "      <td>-0.310357</td>\n",
       "      <td>0.010008</td>\n",
       "      <td>-0.636303</td>\n",
       "      <td>36.840069</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>438300 rows Ã— 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         index  Subject_ID     FPOGX     FPOGY     BPOGX     BPOGY      LPCX  \\\n",
       "0            0       300.0  0.680730  0.422220  0.688020  0.392590  0.250170   \n",
       "1            1       300.0  0.681250  0.426850  0.687500  0.492590  0.250110   \n",
       "2            2       300.0  0.681250  0.432410  0.685940  0.505560  0.250110   \n",
       "3            3       300.0  0.873960  0.436110  2.586980  0.465740  0.247190   \n",
       "4            4       300.0  0.873960  0.436110  2.586980  0.465740  0.069350   \n",
       "...        ...         ...       ...       ...       ...       ...       ...   \n",
       "438295  438295      1007.0  0.444660  0.359526  0.698782  0.432274  0.407118   \n",
       "438296  438296      1007.0  0.694382  0.500890  0.716677  0.865497  0.407701   \n",
       "438297  438297      1007.0  0.706304  0.504890  0.716677  0.865497  0.497037   \n",
       "438298  438298      1007.0  0.735776  0.518086  0.792718  0.901090  0.404317   \n",
       "438299  438299      1007.0  0.757953  0.537008  0.818161  0.716998  0.404227   \n",
       "\n",
       "            LPCY  Normed_LPD  Normed_LPS  ...  Contempt_Evidence  \\\n",
       "0       0.411860    1.808046         0.0  ...          -0.261916   \n",
       "1       0.413840   -0.821955         0.0  ...          -0.261916   \n",
       "2       0.413840   -0.821955         1.0  ...          -0.261916   \n",
       "3       0.409590  -12.431953         0.0  ...          -0.261916   \n",
       "4       0.335540  -11.791954         0.0  ...          -0.261916   \n",
       "...          ...         ...         ...  ...                ...   \n",
       "438295  0.717636   -5.347173         0.0  ...          -1.095808   \n",
       "438296  0.710676   -6.512259         0.0  ...          -1.095808   \n",
       "438297  0.785391   -6.250959         0.0  ...          -1.095808   \n",
       "438298  0.724950   -6.724377         0.0  ...          -1.089166   \n",
       "438299  0.679830   -6.626922         0.0  ...          -1.089166   \n",
       "\n",
       "        Disgust_Evidence  Joy_Evidence  Fear_Evidence  Negative_Evidence  \\\n",
       "0              -1.104235     -3.085957      -0.276813           2.562142   \n",
       "1              -1.104235     -3.085957      -0.276813           2.562142   \n",
       "2              -1.104235     -3.085957      -0.276813           2.562142   \n",
       "3              -1.104235     -3.085957      -0.276813           2.562142   \n",
       "4              -1.104235     -3.085957      -0.276813           2.562142   \n",
       "...                  ...           ...            ...                ...   \n",
       "438295         -0.744811     -1.386102      -0.109323           0.396394   \n",
       "438296         -0.744811     -1.386102      -0.109323           0.396394   \n",
       "438297         -0.744811     -1.386102      -0.109323           0.396394   \n",
       "438298         -0.873999     -1.361231      -0.214787           0.020447   \n",
       "438299         -0.873999     -1.361231      -0.214787           0.020447   \n",
       "\n",
       "        Neutral_Evidence  Positive_Evidence  Sadness_Evidence  \\\n",
       "0               0.683569          -1.535620          1.238940   \n",
       "1               0.683569          -1.535620          1.238940   \n",
       "2               0.683569          -1.535620          1.238940   \n",
       "3               0.683569          -1.535620          1.238940   \n",
       "4               0.683569          -1.535620          1.238940   \n",
       "...                  ...                ...               ...   \n",
       "438295         -0.211663          -0.469004         -0.248212   \n",
       "438296         -0.211663          -0.469004         -0.248212   \n",
       "438297         -0.211663          -0.469004         -0.248212   \n",
       "438298         -0.107674          -0.310357          0.010008   \n",
       "438299         -0.107674          -0.310357          0.010008   \n",
       "\n",
       "        Surprise_Evidence  Normed_Heart_Rate  \n",
       "0               -0.985122           3.416550  \n",
       "1               -0.985122           3.416550  \n",
       "2               -0.985122           3.416550  \n",
       "3               -0.985122           3.416550  \n",
       "4               -0.985122           3.416550  \n",
       "...                   ...                ...  \n",
       "438295          -0.755508          36.840069  \n",
       "438296          -0.755508          36.840069  \n",
       "438297          -0.755508          36.840069  \n",
       "438298          -0.636303          36.840069  \n",
       "438299          -0.636303          36.840069  \n",
       "\n",
       "[438300 rows x 27 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_df_tot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "413d5d5b",
   "metadata": {},
   "source": [
    "## Augment the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "cd278e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "related_features.insert(1,\"index\")\n",
    "data_divided = pd.DataFrame(columns = related_features)\n",
    "related_features.remove(\"index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8dd5f33b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Subject_ID</th>\n",
       "      <th>index</th>\n",
       "      <th>FPOGX</th>\n",
       "      <th>FPOGY</th>\n",
       "      <th>BPOGX</th>\n",
       "      <th>BPOGY</th>\n",
       "      <th>LPCX</th>\n",
       "      <th>LPCY</th>\n",
       "      <th>Normed_LPD</th>\n",
       "      <th>Normed_LPS</th>\n",
       "      <th>...</th>\n",
       "      <th>Contempt_Evidence</th>\n",
       "      <th>Disgust_Evidence</th>\n",
       "      <th>Joy_Evidence</th>\n",
       "      <th>Fear_Evidence</th>\n",
       "      <th>Negative_Evidence</th>\n",
       "      <th>Neutral_Evidence</th>\n",
       "      <th>Positive_Evidence</th>\n",
       "      <th>Sadness_Evidence</th>\n",
       "      <th>Surprise_Evidence</th>\n",
       "      <th>Normed_Heart_Rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows Ã— 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Subject_ID, index, FPOGX, FPOGY, BPOGX, BPOGY, LPCX, LPCY, Normed_LPD, Normed_LPS, RPCX, RPCY, Normed_RPD, Normed_RPS, LPUPILD, RPUPILD, Anger_Evidence, Contempt_Evidence, Disgust_Evidence, Joy_Evidence, Fear_Evidence, Negative_Evidence, Neutral_Evidence, Positive_Evidence, Sadness_Evidence, Surprise_Evidence, Normed_Heart_Rate]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 27 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_divided"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5d08b27f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Subject_ID</th>\n",
       "      <th>FPOGX</th>\n",
       "      <th>FPOGY</th>\n",
       "      <th>BPOGX</th>\n",
       "      <th>BPOGY</th>\n",
       "      <th>LPCX</th>\n",
       "      <th>LPCY</th>\n",
       "      <th>Normed_LPD</th>\n",
       "      <th>Normed_LPS</th>\n",
       "      <th>...</th>\n",
       "      <th>Contempt_Evidence</th>\n",
       "      <th>Disgust_Evidence</th>\n",
       "      <th>Joy_Evidence</th>\n",
       "      <th>Fear_Evidence</th>\n",
       "      <th>Negative_Evidence</th>\n",
       "      <th>Neutral_Evidence</th>\n",
       "      <th>Positive_Evidence</th>\n",
       "      <th>Sadness_Evidence</th>\n",
       "      <th>Surprise_Evidence</th>\n",
       "      <th>Normed_Heart_Rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>300.0</td>\n",
       "      <td>0.680730</td>\n",
       "      <td>0.422220</td>\n",
       "      <td>0.688020</td>\n",
       "      <td>0.392590</td>\n",
       "      <td>0.250170</td>\n",
       "      <td>0.411860</td>\n",
       "      <td>1.808046</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.261916</td>\n",
       "      <td>-1.104235</td>\n",
       "      <td>-3.085957</td>\n",
       "      <td>-0.276813</td>\n",
       "      <td>2.562142</td>\n",
       "      <td>0.683569</td>\n",
       "      <td>-1.535620</td>\n",
       "      <td>1.238940</td>\n",
       "      <td>-0.985122</td>\n",
       "      <td>3.416550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>300.0</td>\n",
       "      <td>0.681250</td>\n",
       "      <td>0.426850</td>\n",
       "      <td>0.687500</td>\n",
       "      <td>0.492590</td>\n",
       "      <td>0.250110</td>\n",
       "      <td>0.413840</td>\n",
       "      <td>-0.821955</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.261916</td>\n",
       "      <td>-1.104235</td>\n",
       "      <td>-3.085957</td>\n",
       "      <td>-0.276813</td>\n",
       "      <td>2.562142</td>\n",
       "      <td>0.683569</td>\n",
       "      <td>-1.535620</td>\n",
       "      <td>1.238940</td>\n",
       "      <td>-0.985122</td>\n",
       "      <td>3.416550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>300.0</td>\n",
       "      <td>0.681250</td>\n",
       "      <td>0.432410</td>\n",
       "      <td>0.685940</td>\n",
       "      <td>0.505560</td>\n",
       "      <td>0.250110</td>\n",
       "      <td>0.413840</td>\n",
       "      <td>-0.821955</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.261916</td>\n",
       "      <td>-1.104235</td>\n",
       "      <td>-3.085957</td>\n",
       "      <td>-0.276813</td>\n",
       "      <td>2.562142</td>\n",
       "      <td>0.683569</td>\n",
       "      <td>-1.535620</td>\n",
       "      <td>1.238940</td>\n",
       "      <td>-0.985122</td>\n",
       "      <td>3.416550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>300.0</td>\n",
       "      <td>0.873960</td>\n",
       "      <td>0.436110</td>\n",
       "      <td>2.586980</td>\n",
       "      <td>0.465740</td>\n",
       "      <td>0.247190</td>\n",
       "      <td>0.409590</td>\n",
       "      <td>-12.431953</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.261916</td>\n",
       "      <td>-1.104235</td>\n",
       "      <td>-3.085957</td>\n",
       "      <td>-0.276813</td>\n",
       "      <td>2.562142</td>\n",
       "      <td>0.683569</td>\n",
       "      <td>-1.535620</td>\n",
       "      <td>1.238940</td>\n",
       "      <td>-0.985122</td>\n",
       "      <td>3.416550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>300.0</td>\n",
       "      <td>0.873960</td>\n",
       "      <td>0.436110</td>\n",
       "      <td>2.586980</td>\n",
       "      <td>0.465740</td>\n",
       "      <td>0.069350</td>\n",
       "      <td>0.335540</td>\n",
       "      <td>-11.791954</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.261916</td>\n",
       "      <td>-1.104235</td>\n",
       "      <td>-3.085957</td>\n",
       "      <td>-0.276813</td>\n",
       "      <td>2.562142</td>\n",
       "      <td>0.683569</td>\n",
       "      <td>-1.535620</td>\n",
       "      <td>1.238940</td>\n",
       "      <td>-0.985122</td>\n",
       "      <td>3.416550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>438295</th>\n",
       "      <td>438295</td>\n",
       "      <td>1007.0</td>\n",
       "      <td>0.444660</td>\n",
       "      <td>0.359526</td>\n",
       "      <td>0.698782</td>\n",
       "      <td>0.432274</td>\n",
       "      <td>0.407118</td>\n",
       "      <td>0.717636</td>\n",
       "      <td>-5.347173</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.095808</td>\n",
       "      <td>-0.744811</td>\n",
       "      <td>-1.386102</td>\n",
       "      <td>-0.109323</td>\n",
       "      <td>0.396394</td>\n",
       "      <td>-0.211663</td>\n",
       "      <td>-0.469004</td>\n",
       "      <td>-0.248212</td>\n",
       "      <td>-0.755508</td>\n",
       "      <td>36.840069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>438296</th>\n",
       "      <td>438296</td>\n",
       "      <td>1007.0</td>\n",
       "      <td>0.694382</td>\n",
       "      <td>0.500890</td>\n",
       "      <td>0.716677</td>\n",
       "      <td>0.865497</td>\n",
       "      <td>0.407701</td>\n",
       "      <td>0.710676</td>\n",
       "      <td>-6.512259</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.095808</td>\n",
       "      <td>-0.744811</td>\n",
       "      <td>-1.386102</td>\n",
       "      <td>-0.109323</td>\n",
       "      <td>0.396394</td>\n",
       "      <td>-0.211663</td>\n",
       "      <td>-0.469004</td>\n",
       "      <td>-0.248212</td>\n",
       "      <td>-0.755508</td>\n",
       "      <td>36.840069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>438297</th>\n",
       "      <td>438297</td>\n",
       "      <td>1007.0</td>\n",
       "      <td>0.706304</td>\n",
       "      <td>0.504890</td>\n",
       "      <td>0.716677</td>\n",
       "      <td>0.865497</td>\n",
       "      <td>0.497037</td>\n",
       "      <td>0.785391</td>\n",
       "      <td>-6.250959</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.095808</td>\n",
       "      <td>-0.744811</td>\n",
       "      <td>-1.386102</td>\n",
       "      <td>-0.109323</td>\n",
       "      <td>0.396394</td>\n",
       "      <td>-0.211663</td>\n",
       "      <td>-0.469004</td>\n",
       "      <td>-0.248212</td>\n",
       "      <td>-0.755508</td>\n",
       "      <td>36.840069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>438298</th>\n",
       "      <td>438298</td>\n",
       "      <td>1007.0</td>\n",
       "      <td>0.735776</td>\n",
       "      <td>0.518086</td>\n",
       "      <td>0.792718</td>\n",
       "      <td>0.901090</td>\n",
       "      <td>0.404317</td>\n",
       "      <td>0.724950</td>\n",
       "      <td>-6.724377</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.089166</td>\n",
       "      <td>-0.873999</td>\n",
       "      <td>-1.361231</td>\n",
       "      <td>-0.214787</td>\n",
       "      <td>0.020447</td>\n",
       "      <td>-0.107674</td>\n",
       "      <td>-0.310357</td>\n",
       "      <td>0.010008</td>\n",
       "      <td>-0.636303</td>\n",
       "      <td>36.840069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>438299</th>\n",
       "      <td>438299</td>\n",
       "      <td>1007.0</td>\n",
       "      <td>0.757953</td>\n",
       "      <td>0.537008</td>\n",
       "      <td>0.818161</td>\n",
       "      <td>0.716998</td>\n",
       "      <td>0.404227</td>\n",
       "      <td>0.679830</td>\n",
       "      <td>-6.626922</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.089166</td>\n",
       "      <td>-0.873999</td>\n",
       "      <td>-1.361231</td>\n",
       "      <td>-0.214787</td>\n",
       "      <td>0.020447</td>\n",
       "      <td>-0.107674</td>\n",
       "      <td>-0.310357</td>\n",
       "      <td>0.010008</td>\n",
       "      <td>-0.636303</td>\n",
       "      <td>36.840069</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>438300 rows Ã— 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         index  Subject_ID     FPOGX     FPOGY     BPOGX     BPOGY      LPCX  \\\n",
       "0            0       300.0  0.680730  0.422220  0.688020  0.392590  0.250170   \n",
       "1            1       300.0  0.681250  0.426850  0.687500  0.492590  0.250110   \n",
       "2            2       300.0  0.681250  0.432410  0.685940  0.505560  0.250110   \n",
       "3            3       300.0  0.873960  0.436110  2.586980  0.465740  0.247190   \n",
       "4            4       300.0  0.873960  0.436110  2.586980  0.465740  0.069350   \n",
       "...        ...         ...       ...       ...       ...       ...       ...   \n",
       "438295  438295      1007.0  0.444660  0.359526  0.698782  0.432274  0.407118   \n",
       "438296  438296      1007.0  0.694382  0.500890  0.716677  0.865497  0.407701   \n",
       "438297  438297      1007.0  0.706304  0.504890  0.716677  0.865497  0.497037   \n",
       "438298  438298      1007.0  0.735776  0.518086  0.792718  0.901090  0.404317   \n",
       "438299  438299      1007.0  0.757953  0.537008  0.818161  0.716998  0.404227   \n",
       "\n",
       "            LPCY  Normed_LPD  Normed_LPS  ...  Contempt_Evidence  \\\n",
       "0       0.411860    1.808046         0.0  ...          -0.261916   \n",
       "1       0.413840   -0.821955         0.0  ...          -0.261916   \n",
       "2       0.413840   -0.821955         1.0  ...          -0.261916   \n",
       "3       0.409590  -12.431953         0.0  ...          -0.261916   \n",
       "4       0.335540  -11.791954         0.0  ...          -0.261916   \n",
       "...          ...         ...         ...  ...                ...   \n",
       "438295  0.717636   -5.347173         0.0  ...          -1.095808   \n",
       "438296  0.710676   -6.512259         0.0  ...          -1.095808   \n",
       "438297  0.785391   -6.250959         0.0  ...          -1.095808   \n",
       "438298  0.724950   -6.724377         0.0  ...          -1.089166   \n",
       "438299  0.679830   -6.626922         0.0  ...          -1.089166   \n",
       "\n",
       "        Disgust_Evidence  Joy_Evidence  Fear_Evidence  Negative_Evidence  \\\n",
       "0              -1.104235     -3.085957      -0.276813           2.562142   \n",
       "1              -1.104235     -3.085957      -0.276813           2.562142   \n",
       "2              -1.104235     -3.085957      -0.276813           2.562142   \n",
       "3              -1.104235     -3.085957      -0.276813           2.562142   \n",
       "4              -1.104235     -3.085957      -0.276813           2.562142   \n",
       "...                  ...           ...            ...                ...   \n",
       "438295         -0.744811     -1.386102      -0.109323           0.396394   \n",
       "438296         -0.744811     -1.386102      -0.109323           0.396394   \n",
       "438297         -0.744811     -1.386102      -0.109323           0.396394   \n",
       "438298         -0.873999     -1.361231      -0.214787           0.020447   \n",
       "438299         -0.873999     -1.361231      -0.214787           0.020447   \n",
       "\n",
       "        Neutral_Evidence  Positive_Evidence  Sadness_Evidence  \\\n",
       "0               0.683569          -1.535620          1.238940   \n",
       "1               0.683569          -1.535620          1.238940   \n",
       "2               0.683569          -1.535620          1.238940   \n",
       "3               0.683569          -1.535620          1.238940   \n",
       "4               0.683569          -1.535620          1.238940   \n",
       "...                  ...                ...               ...   \n",
       "438295         -0.211663          -0.469004         -0.248212   \n",
       "438296         -0.211663          -0.469004         -0.248212   \n",
       "438297         -0.211663          -0.469004         -0.248212   \n",
       "438298         -0.107674          -0.310357          0.010008   \n",
       "438299         -0.107674          -0.310357          0.010008   \n",
       "\n",
       "        Surprise_Evidence  Normed_Heart_Rate  \n",
       "0               -0.985122           3.416550  \n",
       "1               -0.985122           3.416550  \n",
       "2               -0.985122           3.416550  \n",
       "3               -0.985122           3.416550  \n",
       "4               -0.985122           3.416550  \n",
       "...                   ...                ...  \n",
       "438295          -0.755508          36.840069  \n",
       "438296          -0.755508          36.840069  \n",
       "438297          -0.755508          36.840069  \n",
       "438298          -0.636303          36.840069  \n",
       "438299          -0.636303          36.840069  \n",
       "\n",
       "[438300 rows x 27 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_df_tot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b6d404d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "group_1 = x_train_df_tot[x_train_df_tot.index % 4 == 0]\n",
    "group_2 = x_train_df_tot[x_train_df_tot.index % 4 == 1]\n",
    "group_3 = x_train_df_tot[x_train_df_tot.index % 4 == 2]\n",
    "group_4 = x_train_df_tot[x_train_df_tot.index % 4 == 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "919080ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Subject_ID</th>\n",
       "      <th>FPOGX</th>\n",
       "      <th>FPOGY</th>\n",
       "      <th>BPOGX</th>\n",
       "      <th>BPOGY</th>\n",
       "      <th>LPCX</th>\n",
       "      <th>LPCY</th>\n",
       "      <th>Normed_LPD</th>\n",
       "      <th>Normed_LPS</th>\n",
       "      <th>...</th>\n",
       "      <th>Contempt_Evidence</th>\n",
       "      <th>Disgust_Evidence</th>\n",
       "      <th>Joy_Evidence</th>\n",
       "      <th>Fear_Evidence</th>\n",
       "      <th>Negative_Evidence</th>\n",
       "      <th>Neutral_Evidence</th>\n",
       "      <th>Positive_Evidence</th>\n",
       "      <th>Sadness_Evidence</th>\n",
       "      <th>Surprise_Evidence</th>\n",
       "      <th>Normed_Heart_Rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>300.0</td>\n",
       "      <td>0.873960</td>\n",
       "      <td>0.436110</td>\n",
       "      <td>2.586980</td>\n",
       "      <td>0.465740</td>\n",
       "      <td>0.247190</td>\n",
       "      <td>0.409590</td>\n",
       "      <td>-12.431953</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.261916</td>\n",
       "      <td>-1.104235</td>\n",
       "      <td>-3.085957</td>\n",
       "      <td>-0.276813</td>\n",
       "      <td>2.562142</td>\n",
       "      <td>0.683569</td>\n",
       "      <td>-1.535620</td>\n",
       "      <td>1.238940</td>\n",
       "      <td>-0.985122</td>\n",
       "      <td>3.416550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>300.0</td>\n",
       "      <td>0.025520</td>\n",
       "      <td>0.660190</td>\n",
       "      <td>0.051040</td>\n",
       "      <td>1.321300</td>\n",
       "      <td>0.235370</td>\n",
       "      <td>0.425000</td>\n",
       "      <td>-15.521954</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.261916</td>\n",
       "      <td>-1.104235</td>\n",
       "      <td>-3.085957</td>\n",
       "      <td>-0.276813</td>\n",
       "      <td>2.562142</td>\n",
       "      <td>0.683569</td>\n",
       "      <td>-1.535620</td>\n",
       "      <td>1.238940</td>\n",
       "      <td>-0.985122</td>\n",
       "      <td>7.332649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>300.0</td>\n",
       "      <td>0.413020</td>\n",
       "      <td>0.515740</td>\n",
       "      <td>0.436460</td>\n",
       "      <td>0.163890</td>\n",
       "      <td>0.241500</td>\n",
       "      <td>0.416530</td>\n",
       "      <td>4.168047</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.261916</td>\n",
       "      <td>-1.104235</td>\n",
       "      <td>-3.085957</td>\n",
       "      <td>-0.276813</td>\n",
       "      <td>2.562142</td>\n",
       "      <td>0.683569</td>\n",
       "      <td>-1.535620</td>\n",
       "      <td>1.238940</td>\n",
       "      <td>-0.985122</td>\n",
       "      <td>7.332649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>300.0</td>\n",
       "      <td>1.089060</td>\n",
       "      <td>0.576850</td>\n",
       "      <td>0.470830</td>\n",
       "      <td>0.468520</td>\n",
       "      <td>0.240930</td>\n",
       "      <td>0.415410</td>\n",
       "      <td>3.818047</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.261916</td>\n",
       "      <td>-1.104235</td>\n",
       "      <td>-3.085957</td>\n",
       "      <td>-0.276813</td>\n",
       "      <td>2.562142</td>\n",
       "      <td>0.683569</td>\n",
       "      <td>-1.535620</td>\n",
       "      <td>1.238940</td>\n",
       "      <td>-0.985122</td>\n",
       "      <td>7.332649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>300.0</td>\n",
       "      <td>0.460420</td>\n",
       "      <td>0.470370</td>\n",
       "      <td>0.465620</td>\n",
       "      <td>0.483330</td>\n",
       "      <td>0.240800</td>\n",
       "      <td>0.415830</td>\n",
       "      <td>1.758047</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.261916</td>\n",
       "      <td>-1.104235</td>\n",
       "      <td>-3.085957</td>\n",
       "      <td>-0.276813</td>\n",
       "      <td>2.562142</td>\n",
       "      <td>0.683569</td>\n",
       "      <td>-1.535620</td>\n",
       "      <td>1.238940</td>\n",
       "      <td>-0.985122</td>\n",
       "      <td>7.332649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>438283</th>\n",
       "      <td>438283</td>\n",
       "      <td>1007.0</td>\n",
       "      <td>0.244149</td>\n",
       "      <td>0.467823</td>\n",
       "      <td>0.719069</td>\n",
       "      <td>0.477807</td>\n",
       "      <td>0.405430</td>\n",
       "      <td>0.811491</td>\n",
       "      <td>-5.044336</td>\n",
       "      <td>-0.568045</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.932147</td>\n",
       "      <td>-0.546471</td>\n",
       "      <td>-1.436044</td>\n",
       "      <td>-0.365545</td>\n",
       "      <td>0.605183</td>\n",
       "      <td>-0.036499</td>\n",
       "      <td>-0.500172</td>\n",
       "      <td>-0.020977</td>\n",
       "      <td>-0.890152</td>\n",
       "      <td>11.903822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>438287</th>\n",
       "      <td>438287</td>\n",
       "      <td>1007.0</td>\n",
       "      <td>0.380518</td>\n",
       "      <td>0.780821</td>\n",
       "      <td>0.185547</td>\n",
       "      <td>0.935089</td>\n",
       "      <td>0.405962</td>\n",
       "      <td>0.788045</td>\n",
       "      <td>-5.337355</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.840890</td>\n",
       "      <td>-0.624892</td>\n",
       "      <td>-1.421514</td>\n",
       "      <td>-0.317254</td>\n",
       "      <td>0.555851</td>\n",
       "      <td>-0.085649</td>\n",
       "      <td>-0.493470</td>\n",
       "      <td>-0.047064</td>\n",
       "      <td>-0.832096</td>\n",
       "      <td>11.903822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>438291</th>\n",
       "      <td>438291</td>\n",
       "      <td>1007.0</td>\n",
       "      <td>0.215090</td>\n",
       "      <td>0.257722</td>\n",
       "      <td>0.163359</td>\n",
       "      <td>0.795708</td>\n",
       "      <td>0.405986</td>\n",
       "      <td>0.763318</td>\n",
       "      <td>-7.133323</td>\n",
       "      <td>0.568045</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.828562</td>\n",
       "      <td>-0.682248</td>\n",
       "      <td>-1.460769</td>\n",
       "      <td>-0.291224</td>\n",
       "      <td>0.504762</td>\n",
       "      <td>-0.070599</td>\n",
       "      <td>-0.511447</td>\n",
       "      <td>-0.040697</td>\n",
       "      <td>-0.826277</td>\n",
       "      <td>36.840069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>438295</th>\n",
       "      <td>438295</td>\n",
       "      <td>1007.0</td>\n",
       "      <td>0.444660</td>\n",
       "      <td>0.359526</td>\n",
       "      <td>0.698782</td>\n",
       "      <td>0.432274</td>\n",
       "      <td>0.407118</td>\n",
       "      <td>0.717636</td>\n",
       "      <td>-5.347173</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.095808</td>\n",
       "      <td>-0.744811</td>\n",
       "      <td>-1.386102</td>\n",
       "      <td>-0.109323</td>\n",
       "      <td>0.396394</td>\n",
       "      <td>-0.211663</td>\n",
       "      <td>-0.469004</td>\n",
       "      <td>-0.248212</td>\n",
       "      <td>-0.755508</td>\n",
       "      <td>36.840069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>438299</th>\n",
       "      <td>438299</td>\n",
       "      <td>1007.0</td>\n",
       "      <td>0.757953</td>\n",
       "      <td>0.537008</td>\n",
       "      <td>0.818161</td>\n",
       "      <td>0.716998</td>\n",
       "      <td>0.404227</td>\n",
       "      <td>0.679830</td>\n",
       "      <td>-6.626922</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.089166</td>\n",
       "      <td>-0.873999</td>\n",
       "      <td>-1.361231</td>\n",
       "      <td>-0.214787</td>\n",
       "      <td>0.020447</td>\n",
       "      <td>-0.107674</td>\n",
       "      <td>-0.310357</td>\n",
       "      <td>0.010008</td>\n",
       "      <td>-0.636303</td>\n",
       "      <td>36.840069</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>109575 rows Ã— 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         index  Subject_ID     FPOGX     FPOGY     BPOGX     BPOGY      LPCX  \\\n",
       "3            3       300.0  0.873960  0.436110  2.586980  0.465740  0.247190   \n",
       "7            7       300.0  0.025520  0.660190  0.051040  1.321300  0.235370   \n",
       "11          11       300.0  0.413020  0.515740  0.436460  0.163890  0.241500   \n",
       "15          15       300.0  1.089060  0.576850  0.470830  0.468520  0.240930   \n",
       "19          19       300.0  0.460420  0.470370  0.465620  0.483330  0.240800   \n",
       "...        ...         ...       ...       ...       ...       ...       ...   \n",
       "438283  438283      1007.0  0.244149  0.467823  0.719069  0.477807  0.405430   \n",
       "438287  438287      1007.0  0.380518  0.780821  0.185547  0.935089  0.405962   \n",
       "438291  438291      1007.0  0.215090  0.257722  0.163359  0.795708  0.405986   \n",
       "438295  438295      1007.0  0.444660  0.359526  0.698782  0.432274  0.407118   \n",
       "438299  438299      1007.0  0.757953  0.537008  0.818161  0.716998  0.404227   \n",
       "\n",
       "            LPCY  Normed_LPD  Normed_LPS  ...  Contempt_Evidence  \\\n",
       "3       0.409590  -12.431953    0.000000  ...          -0.261916   \n",
       "7       0.425000  -15.521954    0.000000  ...          -0.261916   \n",
       "11      0.416530    4.168047    0.000000  ...          -0.261916   \n",
       "15      0.415410    3.818047    0.000000  ...          -0.261916   \n",
       "19      0.415830    1.758047    0.000000  ...          -0.261916   \n",
       "...          ...         ...         ...  ...                ...   \n",
       "438283  0.811491   -5.044336   -0.568045  ...          -0.932147   \n",
       "438287  0.788045   -5.337355    0.000000  ...          -0.840890   \n",
       "438291  0.763318   -7.133323    0.568045  ...          -0.828562   \n",
       "438295  0.717636   -5.347173    0.000000  ...          -1.095808   \n",
       "438299  0.679830   -6.626922    0.000000  ...          -1.089166   \n",
       "\n",
       "        Disgust_Evidence  Joy_Evidence  Fear_Evidence  Negative_Evidence  \\\n",
       "3              -1.104235     -3.085957      -0.276813           2.562142   \n",
       "7              -1.104235     -3.085957      -0.276813           2.562142   \n",
       "11             -1.104235     -3.085957      -0.276813           2.562142   \n",
       "15             -1.104235     -3.085957      -0.276813           2.562142   \n",
       "19             -1.104235     -3.085957      -0.276813           2.562142   \n",
       "...                  ...           ...            ...                ...   \n",
       "438283         -0.546471     -1.436044      -0.365545           0.605183   \n",
       "438287         -0.624892     -1.421514      -0.317254           0.555851   \n",
       "438291         -0.682248     -1.460769      -0.291224           0.504762   \n",
       "438295         -0.744811     -1.386102      -0.109323           0.396394   \n",
       "438299         -0.873999     -1.361231      -0.214787           0.020447   \n",
       "\n",
       "        Neutral_Evidence  Positive_Evidence  Sadness_Evidence  \\\n",
       "3               0.683569          -1.535620          1.238940   \n",
       "7               0.683569          -1.535620          1.238940   \n",
       "11              0.683569          -1.535620          1.238940   \n",
       "15              0.683569          -1.535620          1.238940   \n",
       "19              0.683569          -1.535620          1.238940   \n",
       "...                  ...                ...               ...   \n",
       "438283         -0.036499          -0.500172         -0.020977   \n",
       "438287         -0.085649          -0.493470         -0.047064   \n",
       "438291         -0.070599          -0.511447         -0.040697   \n",
       "438295         -0.211663          -0.469004         -0.248212   \n",
       "438299         -0.107674          -0.310357          0.010008   \n",
       "\n",
       "        Surprise_Evidence  Normed_Heart_Rate  \n",
       "3               -0.985122           3.416550  \n",
       "7               -0.985122           7.332649  \n",
       "11              -0.985122           7.332649  \n",
       "15              -0.985122           7.332649  \n",
       "19              -0.985122           7.332649  \n",
       "...                   ...                ...  \n",
       "438283          -0.890152          11.903822  \n",
       "438287          -0.832096          11.903822  \n",
       "438291          -0.826277          36.840069  \n",
       "438295          -0.755508          36.840069  \n",
       "438299          -0.636303          36.840069  \n",
       "\n",
       "[109575 rows x 27 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "group_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "46e6411a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(109575, 27)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "group_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "574a4a98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Subject_ID</th>\n",
       "      <th>FPOGX</th>\n",
       "      <th>FPOGY</th>\n",
       "      <th>BPOGX</th>\n",
       "      <th>BPOGY</th>\n",
       "      <th>LPCX</th>\n",
       "      <th>LPCY</th>\n",
       "      <th>Normed_LPD</th>\n",
       "      <th>Normed_LPS</th>\n",
       "      <th>...</th>\n",
       "      <th>Contempt_Evidence</th>\n",
       "      <th>Disgust_Evidence</th>\n",
       "      <th>Joy_Evidence</th>\n",
       "      <th>Fear_Evidence</th>\n",
       "      <th>Negative_Evidence</th>\n",
       "      <th>Neutral_Evidence</th>\n",
       "      <th>Positive_Evidence</th>\n",
       "      <th>Sadness_Evidence</th>\n",
       "      <th>Surprise_Evidence</th>\n",
       "      <th>Normed_Heart_Rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>300.0</td>\n",
       "      <td>0.680730</td>\n",
       "      <td>0.422220</td>\n",
       "      <td>0.688020</td>\n",
       "      <td>0.392590</td>\n",
       "      <td>0.250170</td>\n",
       "      <td>0.411860</td>\n",
       "      <td>1.808046</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.261916</td>\n",
       "      <td>-1.104235</td>\n",
       "      <td>-3.085957</td>\n",
       "      <td>-0.276813</td>\n",
       "      <td>2.562142</td>\n",
       "      <td>0.683569</td>\n",
       "      <td>-1.535620</td>\n",
       "      <td>1.238940</td>\n",
       "      <td>-0.985122</td>\n",
       "      <td>3.416550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>300.0</td>\n",
       "      <td>0.873960</td>\n",
       "      <td>0.436110</td>\n",
       "      <td>2.586980</td>\n",
       "      <td>0.465740</td>\n",
       "      <td>0.069350</td>\n",
       "      <td>0.335540</td>\n",
       "      <td>-11.791954</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.261916</td>\n",
       "      <td>-1.104235</td>\n",
       "      <td>-3.085957</td>\n",
       "      <td>-0.276813</td>\n",
       "      <td>2.562142</td>\n",
       "      <td>0.683569</td>\n",
       "      <td>-1.535620</td>\n",
       "      <td>1.238940</td>\n",
       "      <td>-0.985122</td>\n",
       "      <td>3.416550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>300.0</td>\n",
       "      <td>0.170830</td>\n",
       "      <td>0.744440</td>\n",
       "      <td>0.460940</td>\n",
       "      <td>0.912040</td>\n",
       "      <td>0.241770</td>\n",
       "      <td>0.421960</td>\n",
       "      <td>-7.441954</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.261916</td>\n",
       "      <td>-1.104235</td>\n",
       "      <td>-3.085957</td>\n",
       "      <td>-0.276813</td>\n",
       "      <td>2.562142</td>\n",
       "      <td>0.683569</td>\n",
       "      <td>-1.535620</td>\n",
       "      <td>1.238940</td>\n",
       "      <td>-0.985122</td>\n",
       "      <td>7.332649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>300.0</td>\n",
       "      <td>0.456770</td>\n",
       "      <td>0.337960</td>\n",
       "      <td>0.469270</td>\n",
       "      <td>0.401850</td>\n",
       "      <td>0.241850</td>\n",
       "      <td>0.415330</td>\n",
       "      <td>4.888046</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.261916</td>\n",
       "      <td>-1.104235</td>\n",
       "      <td>-3.085957</td>\n",
       "      <td>-0.276813</td>\n",
       "      <td>2.562142</td>\n",
       "      <td>0.683569</td>\n",
       "      <td>-1.535620</td>\n",
       "      <td>1.238940</td>\n",
       "      <td>-0.985122</td>\n",
       "      <td>7.332649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>300.0</td>\n",
       "      <td>0.762500</td>\n",
       "      <td>0.515740</td>\n",
       "      <td>0.436460</td>\n",
       "      <td>0.454630</td>\n",
       "      <td>0.240930</td>\n",
       "      <td>0.415410</td>\n",
       "      <td>3.818047</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.261916</td>\n",
       "      <td>-1.104235</td>\n",
       "      <td>-3.085957</td>\n",
       "      <td>-0.276813</td>\n",
       "      <td>2.562142</td>\n",
       "      <td>0.683569</td>\n",
       "      <td>-1.535620</td>\n",
       "      <td>1.238940</td>\n",
       "      <td>-0.985122</td>\n",
       "      <td>7.332649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>438280</th>\n",
       "      <td>438280</td>\n",
       "      <td>1007.0</td>\n",
       "      <td>0.225440</td>\n",
       "      <td>0.258778</td>\n",
       "      <td>0.031497</td>\n",
       "      <td>0.958336</td>\n",
       "      <td>0.385294</td>\n",
       "      <td>0.826367</td>\n",
       "      <td>4.990519</td>\n",
       "      <td>-0.568045</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.910320</td>\n",
       "      <td>-0.502186</td>\n",
       "      <td>-1.474861</td>\n",
       "      <td>-0.377456</td>\n",
       "      <td>0.615698</td>\n",
       "      <td>-0.002588</td>\n",
       "      <td>-0.531048</td>\n",
       "      <td>-0.032666</td>\n",
       "      <td>-0.912763</td>\n",
       "      <td>11.903822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>438284</th>\n",
       "      <td>438284</td>\n",
       "      <td>1007.0</td>\n",
       "      <td>0.265316</td>\n",
       "      <td>0.472497</td>\n",
       "      <td>0.714882</td>\n",
       "      <td>0.478118</td>\n",
       "      <td>0.405255</td>\n",
       "      <td>0.806042</td>\n",
       "      <td>-5.087708</td>\n",
       "      <td>-0.568045</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.932147</td>\n",
       "      <td>-0.546471</td>\n",
       "      <td>-1.436044</td>\n",
       "      <td>-0.365545</td>\n",
       "      <td>0.605183</td>\n",
       "      <td>-0.036499</td>\n",
       "      <td>-0.500172</td>\n",
       "      <td>-0.020977</td>\n",
       "      <td>-0.890152</td>\n",
       "      <td>11.903822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>438288</th>\n",
       "      <td>438288</td>\n",
       "      <td>1007.0</td>\n",
       "      <td>0.381699</td>\n",
       "      <td>0.780299</td>\n",
       "      <td>0.183775</td>\n",
       "      <td>0.913526</td>\n",
       "      <td>0.405899</td>\n",
       "      <td>0.787607</td>\n",
       "      <td>-5.638418</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.840890</td>\n",
       "      <td>-0.624892</td>\n",
       "      <td>-1.421514</td>\n",
       "      <td>-0.317254</td>\n",
       "      <td>0.555851</td>\n",
       "      <td>-0.085649</td>\n",
       "      <td>-0.493470</td>\n",
       "      <td>-0.047064</td>\n",
       "      <td>-0.832096</td>\n",
       "      <td>11.903822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>438292</th>\n",
       "      <td>438292</td>\n",
       "      <td>1007.0</td>\n",
       "      <td>0.215681</td>\n",
       "      <td>0.253518</td>\n",
       "      <td>0.180815</td>\n",
       "      <td>0.857772</td>\n",
       "      <td>0.404297</td>\n",
       "      <td>0.748750</td>\n",
       "      <td>-5.539955</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.828562</td>\n",
       "      <td>-0.682248</td>\n",
       "      <td>-1.460769</td>\n",
       "      <td>-0.291224</td>\n",
       "      <td>0.504762</td>\n",
       "      <td>-0.070599</td>\n",
       "      <td>-0.511447</td>\n",
       "      <td>-0.040697</td>\n",
       "      <td>-0.826277</td>\n",
       "      <td>36.840069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>438296</th>\n",
       "      <td>438296</td>\n",
       "      <td>1007.0</td>\n",
       "      <td>0.694382</td>\n",
       "      <td>0.500890</td>\n",
       "      <td>0.716677</td>\n",
       "      <td>0.865497</td>\n",
       "      <td>0.407701</td>\n",
       "      <td>0.710676</td>\n",
       "      <td>-6.512259</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.095808</td>\n",
       "      <td>-0.744811</td>\n",
       "      <td>-1.386102</td>\n",
       "      <td>-0.109323</td>\n",
       "      <td>0.396394</td>\n",
       "      <td>-0.211663</td>\n",
       "      <td>-0.469004</td>\n",
       "      <td>-0.248212</td>\n",
       "      <td>-0.755508</td>\n",
       "      <td>36.840069</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>109575 rows Ã— 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         index  Subject_ID     FPOGX     FPOGY     BPOGX     BPOGY      LPCX  \\\n",
       "0            0       300.0  0.680730  0.422220  0.688020  0.392590  0.250170   \n",
       "4            4       300.0  0.873960  0.436110  2.586980  0.465740  0.069350   \n",
       "8            8       300.0  0.170830  0.744440  0.460940  0.912040  0.241770   \n",
       "12          12       300.0  0.456770  0.337960  0.469270  0.401850  0.241850   \n",
       "16          16       300.0  0.762500  0.515740  0.436460  0.454630  0.240930   \n",
       "...        ...         ...       ...       ...       ...       ...       ...   \n",
       "438280  438280      1007.0  0.225440  0.258778  0.031497  0.958336  0.385294   \n",
       "438284  438284      1007.0  0.265316  0.472497  0.714882  0.478118  0.405255   \n",
       "438288  438288      1007.0  0.381699  0.780299  0.183775  0.913526  0.405899   \n",
       "438292  438292      1007.0  0.215681  0.253518  0.180815  0.857772  0.404297   \n",
       "438296  438296      1007.0  0.694382  0.500890  0.716677  0.865497  0.407701   \n",
       "\n",
       "            LPCY  Normed_LPD  Normed_LPS  ...  Contempt_Evidence  \\\n",
       "0       0.411860    1.808046    0.000000  ...          -0.261916   \n",
       "4       0.335540  -11.791954    0.000000  ...          -0.261916   \n",
       "8       0.421960   -7.441954   -1.000000  ...          -0.261916   \n",
       "12      0.415330    4.888046    0.000000  ...          -0.261916   \n",
       "16      0.415410    3.818047    1.000000  ...          -0.261916   \n",
       "...          ...         ...         ...  ...                ...   \n",
       "438280  0.826367    4.990519   -0.568045  ...          -0.910320   \n",
       "438284  0.806042   -5.087708   -0.568045  ...          -0.932147   \n",
       "438288  0.787607   -5.638418    0.000000  ...          -0.840890   \n",
       "438292  0.748750   -5.539955    0.000000  ...          -0.828562   \n",
       "438296  0.710676   -6.512259    0.000000  ...          -1.095808   \n",
       "\n",
       "        Disgust_Evidence  Joy_Evidence  Fear_Evidence  Negative_Evidence  \\\n",
       "0              -1.104235     -3.085957      -0.276813           2.562142   \n",
       "4              -1.104235     -3.085957      -0.276813           2.562142   \n",
       "8              -1.104235     -3.085957      -0.276813           2.562142   \n",
       "12             -1.104235     -3.085957      -0.276813           2.562142   \n",
       "16             -1.104235     -3.085957      -0.276813           2.562142   \n",
       "...                  ...           ...            ...                ...   \n",
       "438280         -0.502186     -1.474861      -0.377456           0.615698   \n",
       "438284         -0.546471     -1.436044      -0.365545           0.605183   \n",
       "438288         -0.624892     -1.421514      -0.317254           0.555851   \n",
       "438292         -0.682248     -1.460769      -0.291224           0.504762   \n",
       "438296         -0.744811     -1.386102      -0.109323           0.396394   \n",
       "\n",
       "        Neutral_Evidence  Positive_Evidence  Sadness_Evidence  \\\n",
       "0               0.683569          -1.535620          1.238940   \n",
       "4               0.683569          -1.535620          1.238940   \n",
       "8               0.683569          -1.535620          1.238940   \n",
       "12              0.683569          -1.535620          1.238940   \n",
       "16              0.683569          -1.535620          1.238940   \n",
       "...                  ...                ...               ...   \n",
       "438280         -0.002588          -0.531048         -0.032666   \n",
       "438284         -0.036499          -0.500172         -0.020977   \n",
       "438288         -0.085649          -0.493470         -0.047064   \n",
       "438292         -0.070599          -0.511447         -0.040697   \n",
       "438296         -0.211663          -0.469004         -0.248212   \n",
       "\n",
       "        Surprise_Evidence  Normed_Heart_Rate  \n",
       "0               -0.985122           3.416550  \n",
       "4               -0.985122           3.416550  \n",
       "8               -0.985122           7.332649  \n",
       "12              -0.985122           7.332649  \n",
       "16              -0.985122           7.332649  \n",
       "...                   ...                ...  \n",
       "438280          -0.912763          11.903822  \n",
       "438284          -0.890152          11.903822  \n",
       "438288          -0.832096          11.903822  \n",
       "438292          -0.826277          36.840069  \n",
       "438296          -0.755508          36.840069  \n",
       "\n",
       "[109575 rows x 27 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "group_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "75a8d988",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = group_1.append(group_2).append(group_3).append(group_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "bfa86679",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Subject_ID</th>\n",
       "      <th>FPOGX</th>\n",
       "      <th>FPOGY</th>\n",
       "      <th>BPOGX</th>\n",
       "      <th>BPOGY</th>\n",
       "      <th>LPCX</th>\n",
       "      <th>LPCY</th>\n",
       "      <th>Normed_LPD</th>\n",
       "      <th>Normed_LPS</th>\n",
       "      <th>...</th>\n",
       "      <th>Contempt_Evidence</th>\n",
       "      <th>Disgust_Evidence</th>\n",
       "      <th>Joy_Evidence</th>\n",
       "      <th>Fear_Evidence</th>\n",
       "      <th>Negative_Evidence</th>\n",
       "      <th>Neutral_Evidence</th>\n",
       "      <th>Positive_Evidence</th>\n",
       "      <th>Sadness_Evidence</th>\n",
       "      <th>Surprise_Evidence</th>\n",
       "      <th>Normed_Heart_Rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>300.0</td>\n",
       "      <td>0.680730</td>\n",
       "      <td>0.422220</td>\n",
       "      <td>0.688020</td>\n",
       "      <td>0.392590</td>\n",
       "      <td>0.250170</td>\n",
       "      <td>0.411860</td>\n",
       "      <td>1.808046</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.261916</td>\n",
       "      <td>-1.104235</td>\n",
       "      <td>-3.085957</td>\n",
       "      <td>-0.276813</td>\n",
       "      <td>2.562142</td>\n",
       "      <td>0.683569</td>\n",
       "      <td>-1.535620</td>\n",
       "      <td>1.238940</td>\n",
       "      <td>-0.985122</td>\n",
       "      <td>3.416550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>300.0</td>\n",
       "      <td>0.873960</td>\n",
       "      <td>0.436110</td>\n",
       "      <td>2.586980</td>\n",
       "      <td>0.465740</td>\n",
       "      <td>0.069350</td>\n",
       "      <td>0.335540</td>\n",
       "      <td>-11.791954</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.261916</td>\n",
       "      <td>-1.104235</td>\n",
       "      <td>-3.085957</td>\n",
       "      <td>-0.276813</td>\n",
       "      <td>2.562142</td>\n",
       "      <td>0.683569</td>\n",
       "      <td>-1.535620</td>\n",
       "      <td>1.238940</td>\n",
       "      <td>-0.985122</td>\n",
       "      <td>3.416550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>300.0</td>\n",
       "      <td>0.170830</td>\n",
       "      <td>0.744440</td>\n",
       "      <td>0.460940</td>\n",
       "      <td>0.912040</td>\n",
       "      <td>0.241770</td>\n",
       "      <td>0.421960</td>\n",
       "      <td>-7.441954</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.261916</td>\n",
       "      <td>-1.104235</td>\n",
       "      <td>-3.085957</td>\n",
       "      <td>-0.276813</td>\n",
       "      <td>2.562142</td>\n",
       "      <td>0.683569</td>\n",
       "      <td>-1.535620</td>\n",
       "      <td>1.238940</td>\n",
       "      <td>-0.985122</td>\n",
       "      <td>7.332649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>300.0</td>\n",
       "      <td>0.456770</td>\n",
       "      <td>0.337960</td>\n",
       "      <td>0.469270</td>\n",
       "      <td>0.401850</td>\n",
       "      <td>0.241850</td>\n",
       "      <td>0.415330</td>\n",
       "      <td>4.888046</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.261916</td>\n",
       "      <td>-1.104235</td>\n",
       "      <td>-3.085957</td>\n",
       "      <td>-0.276813</td>\n",
       "      <td>2.562142</td>\n",
       "      <td>0.683569</td>\n",
       "      <td>-1.535620</td>\n",
       "      <td>1.238940</td>\n",
       "      <td>-0.985122</td>\n",
       "      <td>7.332649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>300.0</td>\n",
       "      <td>0.762500</td>\n",
       "      <td>0.515740</td>\n",
       "      <td>0.436460</td>\n",
       "      <td>0.454630</td>\n",
       "      <td>0.240930</td>\n",
       "      <td>0.415410</td>\n",
       "      <td>3.818047</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.261916</td>\n",
       "      <td>-1.104235</td>\n",
       "      <td>-3.085957</td>\n",
       "      <td>-0.276813</td>\n",
       "      <td>2.562142</td>\n",
       "      <td>0.683569</td>\n",
       "      <td>-1.535620</td>\n",
       "      <td>1.238940</td>\n",
       "      <td>-0.985122</td>\n",
       "      <td>7.332649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>438283</th>\n",
       "      <td>438283</td>\n",
       "      <td>1007.0</td>\n",
       "      <td>0.244149</td>\n",
       "      <td>0.467823</td>\n",
       "      <td>0.719069</td>\n",
       "      <td>0.477807</td>\n",
       "      <td>0.405430</td>\n",
       "      <td>0.811491</td>\n",
       "      <td>-5.044336</td>\n",
       "      <td>-0.568045</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.932147</td>\n",
       "      <td>-0.546471</td>\n",
       "      <td>-1.436044</td>\n",
       "      <td>-0.365545</td>\n",
       "      <td>0.605183</td>\n",
       "      <td>-0.036499</td>\n",
       "      <td>-0.500172</td>\n",
       "      <td>-0.020977</td>\n",
       "      <td>-0.890152</td>\n",
       "      <td>11.903822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>438287</th>\n",
       "      <td>438287</td>\n",
       "      <td>1007.0</td>\n",
       "      <td>0.380518</td>\n",
       "      <td>0.780821</td>\n",
       "      <td>0.185547</td>\n",
       "      <td>0.935089</td>\n",
       "      <td>0.405962</td>\n",
       "      <td>0.788045</td>\n",
       "      <td>-5.337355</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.840890</td>\n",
       "      <td>-0.624892</td>\n",
       "      <td>-1.421514</td>\n",
       "      <td>-0.317254</td>\n",
       "      <td>0.555851</td>\n",
       "      <td>-0.085649</td>\n",
       "      <td>-0.493470</td>\n",
       "      <td>-0.047064</td>\n",
       "      <td>-0.832096</td>\n",
       "      <td>11.903822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>438291</th>\n",
       "      <td>438291</td>\n",
       "      <td>1007.0</td>\n",
       "      <td>0.215090</td>\n",
       "      <td>0.257722</td>\n",
       "      <td>0.163359</td>\n",
       "      <td>0.795708</td>\n",
       "      <td>0.405986</td>\n",
       "      <td>0.763318</td>\n",
       "      <td>-7.133323</td>\n",
       "      <td>0.568045</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.828562</td>\n",
       "      <td>-0.682248</td>\n",
       "      <td>-1.460769</td>\n",
       "      <td>-0.291224</td>\n",
       "      <td>0.504762</td>\n",
       "      <td>-0.070599</td>\n",
       "      <td>-0.511447</td>\n",
       "      <td>-0.040697</td>\n",
       "      <td>-0.826277</td>\n",
       "      <td>36.840069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>438295</th>\n",
       "      <td>438295</td>\n",
       "      <td>1007.0</td>\n",
       "      <td>0.444660</td>\n",
       "      <td>0.359526</td>\n",
       "      <td>0.698782</td>\n",
       "      <td>0.432274</td>\n",
       "      <td>0.407118</td>\n",
       "      <td>0.717636</td>\n",
       "      <td>-5.347173</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.095808</td>\n",
       "      <td>-0.744811</td>\n",
       "      <td>-1.386102</td>\n",
       "      <td>-0.109323</td>\n",
       "      <td>0.396394</td>\n",
       "      <td>-0.211663</td>\n",
       "      <td>-0.469004</td>\n",
       "      <td>-0.248212</td>\n",
       "      <td>-0.755508</td>\n",
       "      <td>36.840069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>438299</th>\n",
       "      <td>438299</td>\n",
       "      <td>1007.0</td>\n",
       "      <td>0.757953</td>\n",
       "      <td>0.537008</td>\n",
       "      <td>0.818161</td>\n",
       "      <td>0.716998</td>\n",
       "      <td>0.404227</td>\n",
       "      <td>0.679830</td>\n",
       "      <td>-6.626922</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.089166</td>\n",
       "      <td>-0.873999</td>\n",
       "      <td>-1.361231</td>\n",
       "      <td>-0.214787</td>\n",
       "      <td>0.020447</td>\n",
       "      <td>-0.107674</td>\n",
       "      <td>-0.310357</td>\n",
       "      <td>0.010008</td>\n",
       "      <td>-0.636303</td>\n",
       "      <td>36.840069</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>438300 rows Ã— 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         index  Subject_ID     FPOGX     FPOGY     BPOGX     BPOGY      LPCX  \\\n",
       "0            0       300.0  0.680730  0.422220  0.688020  0.392590  0.250170   \n",
       "4            4       300.0  0.873960  0.436110  2.586980  0.465740  0.069350   \n",
       "8            8       300.0  0.170830  0.744440  0.460940  0.912040  0.241770   \n",
       "12          12       300.0  0.456770  0.337960  0.469270  0.401850  0.241850   \n",
       "16          16       300.0  0.762500  0.515740  0.436460  0.454630  0.240930   \n",
       "...        ...         ...       ...       ...       ...       ...       ...   \n",
       "438283  438283      1007.0  0.244149  0.467823  0.719069  0.477807  0.405430   \n",
       "438287  438287      1007.0  0.380518  0.780821  0.185547  0.935089  0.405962   \n",
       "438291  438291      1007.0  0.215090  0.257722  0.163359  0.795708  0.405986   \n",
       "438295  438295      1007.0  0.444660  0.359526  0.698782  0.432274  0.407118   \n",
       "438299  438299      1007.0  0.757953  0.537008  0.818161  0.716998  0.404227   \n",
       "\n",
       "            LPCY  Normed_LPD  Normed_LPS  ...  Contempt_Evidence  \\\n",
       "0       0.411860    1.808046    0.000000  ...          -0.261916   \n",
       "4       0.335540  -11.791954    0.000000  ...          -0.261916   \n",
       "8       0.421960   -7.441954   -1.000000  ...          -0.261916   \n",
       "12      0.415330    4.888046    0.000000  ...          -0.261916   \n",
       "16      0.415410    3.818047    1.000000  ...          -0.261916   \n",
       "...          ...         ...         ...  ...                ...   \n",
       "438283  0.811491   -5.044336   -0.568045  ...          -0.932147   \n",
       "438287  0.788045   -5.337355    0.000000  ...          -0.840890   \n",
       "438291  0.763318   -7.133323    0.568045  ...          -0.828562   \n",
       "438295  0.717636   -5.347173    0.000000  ...          -1.095808   \n",
       "438299  0.679830   -6.626922    0.000000  ...          -1.089166   \n",
       "\n",
       "        Disgust_Evidence  Joy_Evidence  Fear_Evidence  Negative_Evidence  \\\n",
       "0              -1.104235     -3.085957      -0.276813           2.562142   \n",
       "4              -1.104235     -3.085957      -0.276813           2.562142   \n",
       "8              -1.104235     -3.085957      -0.276813           2.562142   \n",
       "12             -1.104235     -3.085957      -0.276813           2.562142   \n",
       "16             -1.104235     -3.085957      -0.276813           2.562142   \n",
       "...                  ...           ...            ...                ...   \n",
       "438283         -0.546471     -1.436044      -0.365545           0.605183   \n",
       "438287         -0.624892     -1.421514      -0.317254           0.555851   \n",
       "438291         -0.682248     -1.460769      -0.291224           0.504762   \n",
       "438295         -0.744811     -1.386102      -0.109323           0.396394   \n",
       "438299         -0.873999     -1.361231      -0.214787           0.020447   \n",
       "\n",
       "        Neutral_Evidence  Positive_Evidence  Sadness_Evidence  \\\n",
       "0               0.683569          -1.535620          1.238940   \n",
       "4               0.683569          -1.535620          1.238940   \n",
       "8               0.683569          -1.535620          1.238940   \n",
       "12              0.683569          -1.535620          1.238940   \n",
       "16              0.683569          -1.535620          1.238940   \n",
       "...                  ...                ...               ...   \n",
       "438283         -0.036499          -0.500172         -0.020977   \n",
       "438287         -0.085649          -0.493470         -0.047064   \n",
       "438291         -0.070599          -0.511447         -0.040697   \n",
       "438295         -0.211663          -0.469004         -0.248212   \n",
       "438299         -0.107674          -0.310357          0.010008   \n",
       "\n",
       "        Surprise_Evidence  Normed_Heart_Rate  \n",
       "0               -0.985122           3.416550  \n",
       "4               -0.985122           3.416550  \n",
       "8               -0.985122           7.332649  \n",
       "12              -0.985122           7.332649  \n",
       "16              -0.985122           7.332649  \n",
       "...                   ...                ...  \n",
       "438283          -0.890152          11.903822  \n",
       "438287          -0.832096          11.903822  \n",
       "438291          -0.826277          36.840069  \n",
       "438295          -0.755508          36.840069  \n",
       "438299          -0.636303          36.840069  \n",
       "\n",
       "[438300 rows x 27 columns]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7e163b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_augmented = data_train.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c0c66c44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(438300, 27)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_augmented.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7833dbd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train_subj = 37\n",
    "num_augmented_train_subj = num_train_subj + len(y_res_added)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "46e40c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_augmented_reshaped = x_train_augmented.reshape(int(num_augmented_train_subj) * 4, -1, len(related_features) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "96ef08ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "rng = np.random.default_rng()\n",
    "np.random.shuffle(x_train_augmented_reshaped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "8dbbb77a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(180, 2435, 27)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_augmented_reshaped.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d77fb48b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 4.8703000e+04,  3.0900000e+02,  4.3333000e-01, ...,\n",
       "         1.5789080e+00, -7.0753098e-01, -2.0159988e+00],\n",
       "       [ 4.8707000e+04,  3.0900000e+02,  8.6667001e-01, ...,\n",
       "         1.5789080e+00, -7.0753098e-01, -2.0159988e+00],\n",
       "       [ 4.8711000e+04,  3.0900000e+02,  0.0000000e+00, ...,\n",
       "         1.5789080e+00, -7.0753098e-01, -2.0159988e+00],\n",
       "       ...,\n",
       "       [ 5.8431000e+04,  3.0900000e+02,  0.0000000e+00, ...,\n",
       "         2.4614480e-01,  2.1732770e-01,  5.6215973e+00],\n",
       "       [ 5.8435000e+04,  3.0900000e+02,  5.2187997e-01, ...,\n",
       "         2.4614480e-01,  2.1732770e-01,  5.6215973e+00],\n",
       "       [ 5.8439000e+04,  3.0900000e+02,  1.2437500e+00, ...,\n",
       "         2.4614480e-01,  2.1732770e-01,  5.6215973e+00]])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_augmented_reshaped[32]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "294bec2d",
   "metadata": {},
   "source": [
    "## Final Train X and Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "524ad806",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(180, 2435, 27)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_augmented_reshaped.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "027e62e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_x_train = np.zeros((x_train_augmented_reshaped.shape[0], x_train_augmented_reshaped.shape[1],x_train_augmented_reshaped.shape[2]-2))\n",
    "final_y_train = np.zeros(x_train_augmented_reshaped.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c5e082a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, item in enumerate(x_train_augmented_reshaped):\n",
    "    final_y_train[i] = y_train_to_id[int(item[0][1])]\n",
    "    final_x_train[i] = np.delete(item,np.s_[0:2],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "00488a17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(180, 2435, 25)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "1123bed2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(180,)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "076b7a61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(final_y_train==0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "04facaf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "special_value = -9999.99\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Masking(mask_value=special_value, input_shape=(final_x_train.shape[1], final_x_train.shape[2])))\n",
    "model.add(keras.layers.LSTM(256, kernel_regularizer=keras.regularizers.l2(l=0.05)))\n",
    "model.add(keras.layers.Dense(2, kernel_regularizer=keras.regularizers.l2(l=0.05), activation='softmax'))\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', \n",
    "              optimizer=keras.optimizers.Adam(learning_rate=0.01), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "4f021003",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "masking_8 (Masking)          (None, 2435, 25)          0         \n",
      "_________________________________________________________________\n",
      "lstm_8 (LSTM)                (None, 256)               288768    \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 2)                 514       \n",
      "=================================================================\n",
      "Total params: 289,282\n",
      "Trainable params: 289,282\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "931cd165",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(final_y_train==0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "49667b91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "132"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(final_y_train==1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "eed81921",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "6/6 [==============================] - 141s 23s/step - loss: 2.7792 - accuracy: 0.6611\n",
      "Epoch 2/3\n",
      "6/6 [==============================] - 142s 23s/step - loss: 1.1079 - accuracy: 0.9167\n",
      "Epoch 3/3\n",
      "6/6 [==============================] - 145s 24s/step - loss: 0.8695 - accuracy: 0.9833\n"
     ]
    }
   ],
   "source": [
    "weights = {0:4, 1:1}\n",
    "history = model.fit(x = final_x_train, y = final_y_train, class_weight=weights, epochs = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "92eb7f86",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_8_layer_call_fn, lstm_cell_8_layer_call_and_return_conditional_losses, lstm_cell_8_layer_call_fn, lstm_cell_8_layer_call_and_return_conditional_losses, lstm_cell_8_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./Saved_Models/lstm_with_regularization_v6\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./Saved_Models/lstm_with_regularization_v6\\assets\n"
     ]
    }
   ],
   "source": [
    "model.save(\"./Saved_Models/lstm_with_regularization_v6\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a327b4a2",
   "metadata": {},
   "source": [
    "## Make validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "9968fd4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "group_1 = x_valid_df[x_valid_df.index % 4 == 0]\n",
    "group_2 = x_valid_df[x_valid_df.index % 4 == 1]\n",
    "group_3 = x_valid_df[x_valid_df.index % 4 == 2]\n",
    "group_4 = x_valid_df[x_valid_df.index % 4 == 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "03f8a7a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Subject_ID</th>\n",
       "      <th>FPOGX</th>\n",
       "      <th>FPOGY</th>\n",
       "      <th>BPOGX</th>\n",
       "      <th>BPOGY</th>\n",
       "      <th>LPCX</th>\n",
       "      <th>LPCY</th>\n",
       "      <th>Normed_LPD</th>\n",
       "      <th>Normed_LPS</th>\n",
       "      <th>...</th>\n",
       "      <th>Contempt_Evidence</th>\n",
       "      <th>Disgust_Evidence</th>\n",
       "      <th>Joy_Evidence</th>\n",
       "      <th>Fear_Evidence</th>\n",
       "      <th>Negative_Evidence</th>\n",
       "      <th>Neutral_Evidence</th>\n",
       "      <th>Positive_Evidence</th>\n",
       "      <th>Sadness_Evidence</th>\n",
       "      <th>Surprise_Evidence</th>\n",
       "      <th>Normed_Heart_Rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>360383</th>\n",
       "      <td>360384</td>\n",
       "      <td>357</td>\n",
       "      <td>0.51510</td>\n",
       "      <td>0.52222</td>\n",
       "      <td>0.45260</td>\n",
       "      <td>0.51481</td>\n",
       "      <td>0.35771</td>\n",
       "      <td>0.53339</td>\n",
       "      <td>-2.272785</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.683207</td>\n",
       "      <td>-1.598442</td>\n",
       "      <td>-1.426473</td>\n",
       "      <td>-0.672896</td>\n",
       "      <td>0.106662</td>\n",
       "      <td>-0.562831</td>\n",
       "      <td>-0.468420</td>\n",
       "      <td>-0.077918</td>\n",
       "      <td>0.246857</td>\n",
       "      <td>4.572967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360387</th>\n",
       "      <td>360388</td>\n",
       "      <td>357</td>\n",
       "      <td>0.47708</td>\n",
       "      <td>0.52130</td>\n",
       "      <td>0.44635</td>\n",
       "      <td>0.51667</td>\n",
       "      <td>0.35759</td>\n",
       "      <td>0.53300</td>\n",
       "      <td>-1.632784</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.080253</td>\n",
       "      <td>-1.799364</td>\n",
       "      <td>-1.494843</td>\n",
       "      <td>-0.766913</td>\n",
       "      <td>-0.302126</td>\n",
       "      <td>-0.496442</td>\n",
       "      <td>-0.380915</td>\n",
       "      <td>-0.065782</td>\n",
       "      <td>0.380946</td>\n",
       "      <td>4.572967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360391</th>\n",
       "      <td>360392</td>\n",
       "      <td>357</td>\n",
       "      <td>0.45937</td>\n",
       "      <td>0.52685</td>\n",
       "      <td>0.48906</td>\n",
       "      <td>0.52778</td>\n",
       "      <td>0.35885</td>\n",
       "      <td>0.53364</td>\n",
       "      <td>-1.392784</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.372599</td>\n",
       "      <td>-1.606737</td>\n",
       "      <td>-1.463684</td>\n",
       "      <td>-0.704410</td>\n",
       "      <td>0.070525</td>\n",
       "      <td>-0.631562</td>\n",
       "      <td>-0.449554</td>\n",
       "      <td>-0.004153</td>\n",
       "      <td>0.382068</td>\n",
       "      <td>4.572967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360395</th>\n",
       "      <td>360396</td>\n",
       "      <td>357</td>\n",
       "      <td>0.47708</td>\n",
       "      <td>0.52407</td>\n",
       "      <td>0.50052</td>\n",
       "      <td>0.51296</td>\n",
       "      <td>0.35930</td>\n",
       "      <td>0.53440</td>\n",
       "      <td>-1.482784</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.465565</td>\n",
       "      <td>-1.590808</td>\n",
       "      <td>-1.239381</td>\n",
       "      <td>-0.788598</td>\n",
       "      <td>-0.034147</td>\n",
       "      <td>-0.626036</td>\n",
       "      <td>-0.271629</td>\n",
       "      <td>0.024322</td>\n",
       "      <td>0.287926</td>\n",
       "      <td>4.572967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360399</th>\n",
       "      <td>360400</td>\n",
       "      <td>357</td>\n",
       "      <td>0.48021</td>\n",
       "      <td>0.51759</td>\n",
       "      <td>0.47917</td>\n",
       "      <td>0.49259</td>\n",
       "      <td>0.36111</td>\n",
       "      <td>0.53438</td>\n",
       "      <td>-1.152784</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.562609</td>\n",
       "      <td>-1.208939</td>\n",
       "      <td>-1.141587</td>\n",
       "      <td>-0.841349</td>\n",
       "      <td>0.089807</td>\n",
       "      <td>-0.337467</td>\n",
       "      <td>-0.240800</td>\n",
       "      <td>0.171728</td>\n",
       "      <td>0.288487</td>\n",
       "      <td>4.572967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>418803</th>\n",
       "      <td>418804</td>\n",
       "      <td>365</td>\n",
       "      <td>0.38281</td>\n",
       "      <td>0.38704</td>\n",
       "      <td>0.39844</td>\n",
       "      <td>0.38981</td>\n",
       "      <td>0.26615</td>\n",
       "      <td>0.72202</td>\n",
       "      <td>-3.427525</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.842429</td>\n",
       "      <td>-0.154404</td>\n",
       "      <td>-1.076637</td>\n",
       "      <td>-1.826618</td>\n",
       "      <td>2.722756</td>\n",
       "      <td>-1.294892</td>\n",
       "      <td>-0.683817</td>\n",
       "      <td>-2.141173</td>\n",
       "      <td>-2.080912</td>\n",
       "      <td>-5.276138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>418807</th>\n",
       "      <td>418808</td>\n",
       "      <td>365</td>\n",
       "      <td>0.38177</td>\n",
       "      <td>0.37778</td>\n",
       "      <td>0.37865</td>\n",
       "      <td>0.35000</td>\n",
       "      <td>0.26587</td>\n",
       "      <td>0.72156</td>\n",
       "      <td>-4.047524</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.778009</td>\n",
       "      <td>-0.119481</td>\n",
       "      <td>-1.157740</td>\n",
       "      <td>-1.725137</td>\n",
       "      <td>2.680061</td>\n",
       "      <td>-1.360651</td>\n",
       "      <td>-0.751298</td>\n",
       "      <td>-2.250450</td>\n",
       "      <td>-1.972712</td>\n",
       "      <td>-40.433041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>418811</th>\n",
       "      <td>418812</td>\n",
       "      <td>365</td>\n",
       "      <td>0.35573</td>\n",
       "      <td>0.38704</td>\n",
       "      <td>0.33958</td>\n",
       "      <td>0.42037</td>\n",
       "      <td>0.26598</td>\n",
       "      <td>0.72065</td>\n",
       "      <td>-5.057524</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.335703</td>\n",
       "      <td>-0.117248</td>\n",
       "      <td>-1.004888</td>\n",
       "      <td>-1.677470</td>\n",
       "      <td>2.961741</td>\n",
       "      <td>-1.363706</td>\n",
       "      <td>-0.698722</td>\n",
       "      <td>-1.994208</td>\n",
       "      <td>-2.045598</td>\n",
       "      <td>-40.433041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>418815</th>\n",
       "      <td>418816</td>\n",
       "      <td>365</td>\n",
       "      <td>0.37031</td>\n",
       "      <td>0.39815</td>\n",
       "      <td>0.39792</td>\n",
       "      <td>0.38333</td>\n",
       "      <td>0.26668</td>\n",
       "      <td>0.72261</td>\n",
       "      <td>-2.647524</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.407128</td>\n",
       "      <td>-0.156432</td>\n",
       "      <td>-1.473553</td>\n",
       "      <td>-1.841570</td>\n",
       "      <td>2.932494</td>\n",
       "      <td>-1.194771</td>\n",
       "      <td>-0.932807</td>\n",
       "      <td>-1.932029</td>\n",
       "      <td>-1.989926</td>\n",
       "      <td>-40.433041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>418819</th>\n",
       "      <td>418820</td>\n",
       "      <td>365</td>\n",
       "      <td>0.37448</td>\n",
       "      <td>0.39722</td>\n",
       "      <td>0.38646</td>\n",
       "      <td>0.37500</td>\n",
       "      <td>0.26653</td>\n",
       "      <td>0.72194</td>\n",
       "      <td>-2.657524</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.537363</td>\n",
       "      <td>-0.162792</td>\n",
       "      <td>-1.135028</td>\n",
       "      <td>-1.760911</td>\n",
       "      <td>2.925890</td>\n",
       "      <td>-1.501126</td>\n",
       "      <td>-0.740708</td>\n",
       "      <td>-2.024093</td>\n",
       "      <td>-1.986524</td>\n",
       "      <td>-40.433041</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14610 rows Ã— 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         index  Subject_ID    FPOGX    FPOGY    BPOGX    BPOGY     LPCX  \\\n",
       "360383  360384         357  0.51510  0.52222  0.45260  0.51481  0.35771   \n",
       "360387  360388         357  0.47708  0.52130  0.44635  0.51667  0.35759   \n",
       "360391  360392         357  0.45937  0.52685  0.48906  0.52778  0.35885   \n",
       "360395  360396         357  0.47708  0.52407  0.50052  0.51296  0.35930   \n",
       "360399  360400         357  0.48021  0.51759  0.47917  0.49259  0.36111   \n",
       "...        ...         ...      ...      ...      ...      ...      ...   \n",
       "418803  418804         365  0.38281  0.38704  0.39844  0.38981  0.26615   \n",
       "418807  418808         365  0.38177  0.37778  0.37865  0.35000  0.26587   \n",
       "418811  418812         365  0.35573  0.38704  0.33958  0.42037  0.26598   \n",
       "418815  418816         365  0.37031  0.39815  0.39792  0.38333  0.26668   \n",
       "418819  418820         365  0.37448  0.39722  0.38646  0.37500  0.26653   \n",
       "\n",
       "           LPCY  Normed_LPD  Normed_LPS  ...  Contempt_Evidence  \\\n",
       "360383  0.53339   -2.272785         0.0  ...           0.683207   \n",
       "360387  0.53300   -1.632784         0.0  ...           0.080253   \n",
       "360391  0.53364   -1.392784         0.0  ...           0.372599   \n",
       "360395  0.53440   -1.482784         0.0  ...           0.465565   \n",
       "360399  0.53438   -1.152784        -1.0  ...           0.562609   \n",
       "...         ...         ...         ...  ...                ...   \n",
       "418803  0.72202   -3.427525         0.0  ...          -0.842429   \n",
       "418807  0.72156   -4.047524         0.0  ...          -0.778009   \n",
       "418811  0.72065   -5.057524         0.0  ...          -0.335703   \n",
       "418815  0.72261   -2.647524         0.0  ...          -0.407128   \n",
       "418819  0.72194   -2.657524         0.0  ...          -0.537363   \n",
       "\n",
       "        Disgust_Evidence  Joy_Evidence  Fear_Evidence  Negative_Evidence  \\\n",
       "360383         -1.598442     -1.426473      -0.672896           0.106662   \n",
       "360387         -1.799364     -1.494843      -0.766913          -0.302126   \n",
       "360391         -1.606737     -1.463684      -0.704410           0.070525   \n",
       "360395         -1.590808     -1.239381      -0.788598          -0.034147   \n",
       "360399         -1.208939     -1.141587      -0.841349           0.089807   \n",
       "...                  ...           ...            ...                ...   \n",
       "418803         -0.154404     -1.076637      -1.826618           2.722756   \n",
       "418807         -0.119481     -1.157740      -1.725137           2.680061   \n",
       "418811         -0.117248     -1.004888      -1.677470           2.961741   \n",
       "418815         -0.156432     -1.473553      -1.841570           2.932494   \n",
       "418819         -0.162792     -1.135028      -1.760911           2.925890   \n",
       "\n",
       "        Neutral_Evidence  Positive_Evidence  Sadness_Evidence  \\\n",
       "360383         -0.562831          -0.468420         -0.077918   \n",
       "360387         -0.496442          -0.380915         -0.065782   \n",
       "360391         -0.631562          -0.449554         -0.004153   \n",
       "360395         -0.626036          -0.271629          0.024322   \n",
       "360399         -0.337467          -0.240800          0.171728   \n",
       "...                  ...                ...               ...   \n",
       "418803         -1.294892          -0.683817         -2.141173   \n",
       "418807         -1.360651          -0.751298         -2.250450   \n",
       "418811         -1.363706          -0.698722         -1.994208   \n",
       "418815         -1.194771          -0.932807         -1.932029   \n",
       "418819         -1.501126          -0.740708         -2.024093   \n",
       "\n",
       "        Surprise_Evidence  Normed_Heart_Rate  \n",
       "360383           0.246857           4.572967  \n",
       "360387           0.380946           4.572967  \n",
       "360391           0.382068           4.572967  \n",
       "360395           0.287926           4.572967  \n",
       "360399           0.288487           4.572967  \n",
       "...                   ...                ...  \n",
       "418803          -2.080912          -5.276138  \n",
       "418807          -1.972712         -40.433041  \n",
       "418811          -2.045598         -40.433041  \n",
       "418815          -1.989926         -40.433041  \n",
       "418819          -1.986524         -40.433041  \n",
       "\n",
       "[14610 rows x 27 columns]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "group_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "69ccad6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_valid = group_1.append(group_2).append(group_3).append(group_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "19267be3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(58440, 27)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "d30634c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_valid_mat = np.array(data_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "e29c9143",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "9460b168",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_valid_reshaped = x_valid_mat.reshape(len(y_valid) * 4, -1, len(related_features) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "b2390bd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24, 2435, 27)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_valid_reshaped.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "b76bc625",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_x_valid = np.zeros((x_valid_reshaped.shape[0], x_valid_reshaped.shape[1],x_valid_reshaped.shape[2]-2))\n",
    "final_y_valid = np.zeros(x_valid_reshaped.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "84cdd47e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, item in enumerate(x_valid_reshaped):\n",
    "    final_x_valid[i] = np.delete(item,np.s_[0:2],axis=1)\n",
    "    final_y_valid[i] = y_valid_to_id[int(item[0][1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "8cf500a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.zeros(len(final_y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "cfdf2a24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1.,\n",
       "       0., 0., 1., 1., 1., 1., 0.])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_y_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "da1a5756",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "37fc48e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(final_x_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "1a3d5832",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_zero_one = []\n",
    "for x, y in y_pred:\n",
    "    if x >= y: \n",
    "        y_pred_zero_one.append(0)\n",
    "    else:\n",
    "        y_pred_zero_one.append(1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "143a9a55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 0., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1.,\n",
       "       1., 0., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1.,\n",
       "       1., 0., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1., 0., 0.,\n",
       "       0., 1., 0., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1.,\n",
       "       1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0.,\n",
       "       1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1.,\n",
       "       1., 0., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1.,\n",
       "       0., 1., 1., 0., 1., 1., 1., 1., 0., 1.])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "57a7de05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3.6038100e+05,  3.5700000e+02,  5.2604002e-01, ...,\n",
       "        -1.4757470e-02,  3.3129340e-01,  4.5729675e+00],\n",
       "       [ 3.6038500e+05,  3.5700000e+02,  5.1042002e-01, ...,\n",
       "        -7.7918023e-02,  2.4685650e-01,  4.5729675e+00],\n",
       "       [ 3.6038900e+05,  3.5700000e+02,  4.6094000e-01, ...,\n",
       "        -6.5782294e-02,  3.8094649e-01,  4.5729675e+00],\n",
       "       ...,\n",
       "       [ 3.7010900e+05,  3.5700000e+02,  1.1750000e+00, ...,\n",
       "        -1.2997830e+00, -7.2767347e-01,  9.5108665e+01],\n",
       "       [ 3.7011300e+05,  3.5700000e+02,  5.7656002e-01, ...,\n",
       "        -1.3546680e+00, -9.4397902e-01,  9.8297661e+01],\n",
       "       [ 3.7011700e+05,  3.5700000e+02,  9.5051998e-01, ...,\n",
       "        -1.6267240e+00, -7.0488501e-01,  9.8297661e+01]])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_valid_reshaped[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "3aa70889",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_zero_one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "0b06900c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_y_valid.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "12a106e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['accuracy'][:10])\n",
    "\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "id": "462483c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7833333611488342,\n",
       " 0.9444444179534912,\n",
       " 0.9944444298744202,\n",
       " 0.9944444298744202,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0]"
      ]
     },
     "execution_count": 528,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history['accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f242f2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_DumbTo = sub_DumbTo.drop(columns=['Subject_ID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "09748f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_non_zero_nulls_in_cols = sub_DumbTo.isnull().sum().reset_index()\n",
    "num_non_zero_nulls_in_cols.columns = [\"Column\", \"Num_Nulls\"]\n",
    "num_non_zero_nulls_in_cols = num_non_zero_nulls_in_cols[num_non_zero_nulls_in_cols.Num_Nulls != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "004a25a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Column</th>\n",
       "      <th>Num_Nulls</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Column, Num_Nulls]\n",
       "Index: []"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_non_zero_nulls_in_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1e802d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One participants had null values for the last four variables we drop that, so we reduce one from seq_length.shape[0]\n",
    "# we dropped subject_id from related features so we have len(related_features) -1 in reshape dimmentions\n",
    "x = sub_DumbTo.to_numpy()\n",
    "x = x.reshape(int(num_of_subjects), -1, len(related_features) -1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7eaa4c58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape:  (43, 9743, 25)\n"
     ]
    }
   ],
   "source": [
    "print(\"x.shape: \", x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0b91f8fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(x[3][:, 2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4403e11d",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "abf25f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x[:37]\n",
    "y_train = y[:37]\n",
    "x_valid = x[37:]\n",
    "y_valid = y[37:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d628780a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(37, 9743, 25)\n",
      "(37,)\n",
      "(6, 9743, 25)\n",
      "(6,)\n"
     ]
    }
   ],
   "source": [
    "special_value = -9999.99\n",
    "x_train = keras.preprocessing.sequence.pad_sequences(x_train, maxlen = cut_off, padding = \"post\", dtype = 'float64', value = special_value)\n",
    "x_valid = keras.preprocessing.sequence.pad_sequences(x_valid, maxlen = cut_off, padding = \"post\", dtype = 'float64', value = special_value)\n",
    "\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_valid.shape)\n",
    "print(y_valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "671bdcc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_count_df = pd.DataFrame(columns = related_features[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c0ad4855",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FPOGX</th>\n",
       "      <th>FPOGY</th>\n",
       "      <th>BPOGX</th>\n",
       "      <th>BPOGY</th>\n",
       "      <th>LPCX</th>\n",
       "      <th>LPCY</th>\n",
       "      <th>Normed_LPD</th>\n",
       "      <th>Normed_LPS</th>\n",
       "      <th>RPCX</th>\n",
       "      <th>RPCY</th>\n",
       "      <th>...</th>\n",
       "      <th>Contempt_Evidence</th>\n",
       "      <th>Disgust_Evidence</th>\n",
       "      <th>Joy_Evidence</th>\n",
       "      <th>Fear_Evidence</th>\n",
       "      <th>Negative_Evidence</th>\n",
       "      <th>Neutral_Evidence</th>\n",
       "      <th>Positive_Evidence</th>\n",
       "      <th>Sadness_Evidence</th>\n",
       "      <th>Surprise_Evidence</th>\n",
       "      <th>Heart_Rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows Ã— 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [FPOGX, FPOGY, BPOGX, BPOGY, LPCX, LPCY, Normed_LPD, Normed_LPS, RPCX, RPCY, Normed_RPD, Normed_RPS, LPUPILD, RPUPILD, Anger_Evidence, Contempt_Evidence, Disgust_Evidence, Joy_Evidence, Fear_Evidence, Negative_Evidence, Neutral_Evidence, Positive_Evidence, Sadness_Evidence, Surprise_Evidence, Heart_Rate]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 25 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nan_count_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6c82fe2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(x.shape[0]):\n",
    "    nan_count_df.loc[len(nan_count_df)] = sum(np.isnan(x[i]), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c806fcd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nan_ratio = (nan_count_df[['FPOGX', 'FPOGY', 'BPOGX', 'BPOGY', \n",
    "       'LPCX', 'LPCY', 'Normed_LPD', 'Normed_LPS', 'RPCX', 'RPCY', 'Normed_RPD', 'Normed_RPS', \n",
    "       'LPUPILD', 'RPUPILD']]/cut_off) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8994c13b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(43, 9743, 25)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0eec0a03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "masking (Masking)            (None, 9743, 25)          0         \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 64)                23040     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 23,105\n",
      "Trainable params: 23,105\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Masking(mask_value=special_value, input_shape=(cut_off, len(related_features) -1)))\n",
    "model.add(keras.layers.LSTM(64))\n",
    "model.add(keras.layers.Dense(1))\n",
    "\n",
    "model.compile(loss = keras.losses.MeanSquaredError(),\n",
    "              optimizer = keras.optimizers.Adam(learning_rate=0.01))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c26eec01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "2/2 [==============================] - 5s 2s/step - loss: 57.1346 - val_loss: 24.9847\n",
      "Epoch 2/20\n",
      "2/2 [==============================] - 3s 2s/step - loss: 30.3517 - val_loss: 13.5678\n",
      "Epoch 3/20\n",
      "2/2 [==============================] - 4s 2s/step - loss: 17.0741 - val_loss: 5.7431\n",
      "Epoch 4/20\n",
      "2/2 [==============================] - 4s 2s/step - loss: 6.9009 - val_loss: 2.3670\n",
      "Epoch 5/20\n",
      "2/2 [==============================] - 4s 2s/step - loss: 1.4013 - val_loss: 4.3844\n",
      "Epoch 6/20\n",
      "2/2 [==============================] - 4s 2s/step - loss: 1.5780 - val_loss: 6.6259\n",
      "Epoch 7/20\n",
      "2/2 [==============================] - 4s 2s/step - loss: 2.3875 - val_loss: 6.7964\n",
      "Epoch 8/20\n",
      "2/2 [==============================] - 4s 2s/step - loss: 2.0205 - val_loss: 5.7932\n",
      "Epoch 9/20\n",
      "2/2 [==============================] - 4s 2s/step - loss: 1.6364 - val_loss: 5.0519\n",
      "Epoch 10/20\n",
      "2/2 [==============================] - 4s 2s/step - loss: 1.3849 - val_loss: 4.6298\n",
      "Epoch 11/20\n",
      "2/2 [==============================] - 4s 2s/step - loss: 1.2259 - val_loss: 4.3093\n",
      "Epoch 12/20\n",
      "2/2 [==============================] - 4s 2s/step - loss: 1.1442 - val_loss: 4.0188\n",
      "Epoch 13/20\n",
      "2/2 [==============================] - 5s 2s/step - loss: 1.1467 - val_loss: 3.7779\n",
      "Epoch 14/20\n",
      "2/2 [==============================] - 4s 2s/step - loss: 1.1471 - val_loss: 3.5961\n",
      "Epoch 15/20\n",
      "2/2 [==============================] - 4s 2s/step - loss: 1.2049 - val_loss: 3.4831\n",
      "Epoch 16/20\n",
      "2/2 [==============================] - 5s 2s/step - loss: 1.2733 - val_loss: 3.4595\n",
      "Epoch 17/20\n",
      "2/2 [==============================] - 4s 2s/step - loss: 1.2927 - val_loss: 3.4874\n",
      "Epoch 18/20\n",
      "2/2 [==============================] - 4s 2s/step - loss: 1.2553 - val_loss: 3.5667\n",
      "Epoch 19/20\n",
      "2/2 [==============================] - 5s 2s/step - loss: 1.2073 - val_loss: 3.6868\n",
      "Epoch 20/20\n",
      "2/2 [==============================] - 4s 2s/step - loss: 1.1458 - val_loss: 3.8268\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x = x_train, y = y_train, \n",
    "                    validation_data = (x_valid, y_valid),\n",
    "                    epochs = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d884504",
   "metadata": {},
   "outputs": [],
   "source": [
    "def divide_seq(seq, n):\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39593215",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "022a6c52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8dec10c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FPOGX</th>\n",
       "      <th>FPOGY</th>\n",
       "      <th>BPOGX</th>\n",
       "      <th>BPOGY</th>\n",
       "      <th>LPCX</th>\n",
       "      <th>LPCY</th>\n",
       "      <th>Normed_LPD</th>\n",
       "      <th>Normed_LPS</th>\n",
       "      <th>RPCX</th>\n",
       "      <th>RPCY</th>\n",
       "      <th>...</th>\n",
       "      <th>Contempt_Evidence</th>\n",
       "      <th>Disgust_Evidence</th>\n",
       "      <th>Joy_Evidence</th>\n",
       "      <th>Fear_Evidence</th>\n",
       "      <th>Negative_Evidence</th>\n",
       "      <th>Neutral_Evidence</th>\n",
       "      <th>Positive_Evidence</th>\n",
       "      <th>Sadness_Evidence</th>\n",
       "      <th>Surprise_Evidence</th>\n",
       "      <th>Heart_Rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.68073</td>\n",
       "      <td>0.42222</td>\n",
       "      <td>0.68802</td>\n",
       "      <td>0.39259</td>\n",
       "      <td>0.25017</td>\n",
       "      <td>0.41186</td>\n",
       "      <td>1.808046</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.65154</td>\n",
       "      <td>0.40483</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.261916</td>\n",
       "      <td>-1.104235</td>\n",
       "      <td>-3.085957</td>\n",
       "      <td>-0.276813</td>\n",
       "      <td>2.562142</td>\n",
       "      <td>0.683569</td>\n",
       "      <td>-1.53562</td>\n",
       "      <td>1.23894</td>\n",
       "      <td>-0.985122</td>\n",
       "      <td>80.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.68125</td>\n",
       "      <td>0.42685</td>\n",
       "      <td>0.68750</td>\n",
       "      <td>0.49259</td>\n",
       "      <td>0.25011</td>\n",
       "      <td>0.41384</td>\n",
       "      <td>-0.821955</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.65151</td>\n",
       "      <td>0.40529</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.261916</td>\n",
       "      <td>-1.104235</td>\n",
       "      <td>-3.085957</td>\n",
       "      <td>-0.276813</td>\n",
       "      <td>2.562142</td>\n",
       "      <td>0.683569</td>\n",
       "      <td>-1.53562</td>\n",
       "      <td>1.23894</td>\n",
       "      <td>-0.985122</td>\n",
       "      <td>80.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.68125</td>\n",
       "      <td>0.43241</td>\n",
       "      <td>0.68594</td>\n",
       "      <td>0.50556</td>\n",
       "      <td>0.25011</td>\n",
       "      <td>0.41384</td>\n",
       "      <td>-0.821955</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.65146</td>\n",
       "      <td>0.40563</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.261916</td>\n",
       "      <td>-1.104235</td>\n",
       "      <td>-3.085957</td>\n",
       "      <td>-0.276813</td>\n",
       "      <td>2.562142</td>\n",
       "      <td>0.683569</td>\n",
       "      <td>-1.53562</td>\n",
       "      <td>1.23894</td>\n",
       "      <td>-0.985122</td>\n",
       "      <td>80.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.87396</td>\n",
       "      <td>0.43611</td>\n",
       "      <td>2.58698</td>\n",
       "      <td>0.46574</td>\n",
       "      <td>0.24719</td>\n",
       "      <td>0.40959</td>\n",
       "      <td>-12.431953</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.60381</td>\n",
       "      <td>0.47703</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.261916</td>\n",
       "      <td>-1.104235</td>\n",
       "      <td>-3.085957</td>\n",
       "      <td>-0.276813</td>\n",
       "      <td>2.562142</td>\n",
       "      <td>0.683569</td>\n",
       "      <td>-1.53562</td>\n",
       "      <td>1.23894</td>\n",
       "      <td>-0.985122</td>\n",
       "      <td>80.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.87396</td>\n",
       "      <td>0.43611</td>\n",
       "      <td>2.58698</td>\n",
       "      <td>0.46574</td>\n",
       "      <td>0.06935</td>\n",
       "      <td>0.33554</td>\n",
       "      <td>-11.791954</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.57383</td>\n",
       "      <td>0.47479</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.261916</td>\n",
       "      <td>-1.104235</td>\n",
       "      <td>-3.085957</td>\n",
       "      <td>-0.276813</td>\n",
       "      <td>2.562142</td>\n",
       "      <td>0.683569</td>\n",
       "      <td>-1.53562</td>\n",
       "      <td>1.23894</td>\n",
       "      <td>-0.985122</td>\n",
       "      <td>80.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     FPOGX    FPOGY    BPOGX    BPOGY     LPCX     LPCY  Normed_LPD  \\\n",
       "1  0.68073  0.42222  0.68802  0.39259  0.25017  0.41186    1.808046   \n",
       "2  0.68125  0.42685  0.68750  0.49259  0.25011  0.41384   -0.821955   \n",
       "3  0.68125  0.43241  0.68594  0.50556  0.25011  0.41384   -0.821955   \n",
       "4  0.87396  0.43611  2.58698  0.46574  0.24719  0.40959  -12.431953   \n",
       "5  0.87396  0.43611  2.58698  0.46574  0.06935  0.33554  -11.791954   \n",
       "\n",
       "   Normed_LPS     RPCX     RPCY  ...  Contempt_Evidence  Disgust_Evidence  \\\n",
       "1         0.0  0.65154  0.40483  ...          -0.261916         -1.104235   \n",
       "2         0.0  0.65151  0.40529  ...          -0.261916         -1.104235   \n",
       "3         1.0  0.65146  0.40563  ...          -0.261916         -1.104235   \n",
       "4         0.0  0.60381  0.47703  ...          -0.261916         -1.104235   \n",
       "5         0.0  0.57383  0.47479  ...          -0.261916         -1.104235   \n",
       "\n",
       "   Joy_Evidence  Fear_Evidence  Negative_Evidence  Neutral_Evidence  \\\n",
       "1     -3.085957      -0.276813           2.562142          0.683569   \n",
       "2     -3.085957      -0.276813           2.562142          0.683569   \n",
       "3     -3.085957      -0.276813           2.562142          0.683569   \n",
       "4     -3.085957      -0.276813           2.562142          0.683569   \n",
       "5     -3.085957      -0.276813           2.562142          0.683569   \n",
       "\n",
       "   Positive_Evidence  Sadness_Evidence  Surprise_Evidence  Heart_Rate  \n",
       "1           -1.53562           1.23894          -0.985122        80.0  \n",
       "2           -1.53562           1.23894          -0.985122        80.0  \n",
       "3           -1.53562           1.23894          -0.985122        80.0  \n",
       "4           -1.53562           1.23894          -0.985122        80.0  \n",
       "5           -1.53562           1.23894          -0.985122        80.0  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_DumbTo.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1b1d48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b997e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d6c9ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378122bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31dcc45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_df = pd.DataFrame(y)\n",
    "y_df.columns = ['Cont']\n",
    "y_df[\"Dis\"] = 1\n",
    "y_df.loc[y_df.Cont > 5, \"Dis\"] = 2\n",
    "y_df.loc[y_df.Cont < 3, \"Dis\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18341a8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ad4315",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array(y_df.Dis)\n",
    "y_train = y[:37]\n",
    "y_valid = y[37:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d83769",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Masking(mask_value=special_value, input_shape=(cut_off, len(related_features) -1)))\n",
    "model.add(keras.layers.LSTM(32))\n",
    "model.add(keras.layers.Dense(3, activation='softmax'))\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', \n",
    "              optimizer=keras.optimizers.Adam(learning_rate=0.01), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139af2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(x = x_train, y = y_train, \n",
    "                    validation_data = (x_valid, y_valid),\n",
    "                    epochs = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2908e4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_score = model.predict(x_valid)\n",
    "y_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2fd9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c4ac12",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_idx = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24086066",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('classic')\n",
    "fig, ax = plt.subplots()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c782f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x_valid[0][:,feature_idx])\n",
    "plt.plot(x_valid[1][:,feature_idx])\n",
    "plt.plot(x_valid[2][:,feature_idx])\n",
    "plt.plot(x_valid[3][:,feature_idx])\n",
    "plt.plot(x_valid[4][:,feature_idx])\n",
    "plt.plot(x_valid[5][:,feature_idx])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53627b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x_valid[2][:,feature_idx])\n",
    "plt.plot(x_valid[4][:,feature_idx])\n",
    "plt.plot(x_valid[5][:,feature_idx])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716bdf8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf9d1c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079b4c5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f634f1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_dis_pred_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40316184",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_dis_train = tf.keras.utils.to_categorical(y_dis_train, num_classes=3)\n",
    "y_dis_valid = tf.keras.utils.to_categorical(y_dis_valid, num_classes=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e5ceea",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_dis_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36c1678",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = 3\n",
    "precision = dict()\n",
    "recall = dict()\n",
    "average_precision = dict()\n",
    "for i in range(n_classes):\n",
    "    precision[i], recall[i], _ = precision_recall_curve(y_dis_valid[:, i],\n",
    "                                                        y_dis_pred_valid[:, i])\n",
    "    average_precision[i] = average_precision_score(y_dis_valid[:, i], y_dis_pred_valid[:, i])\n",
    "\n",
    "# A \"micro-average\": quantifying score on all classes jointly\n",
    "precision[\"micro\"], recall[\"micro\"], _ = precision_recall_curve(y_dis_valid.ravel(),\n",
    "     y_dis_pred_valid.ravel())\n",
    "average_precision[\"micro\"] = average_precision_score(y_dis_valid,  y_dis_pred_valid,\n",
    "                                                     average=\"micro\")\n",
    "print('Average precision score, micro-averaged over all classes: {0:0.2f}'\n",
    "      .format(average_precision[\"micro\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39682431",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "721a5827",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda7c4ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3ae403",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8b3986ce",
   "metadata": {},
   "source": [
    "## Sklearn Example for Percision Recall Curve\n",
    "[link](https://scikit-learn.org/stable/auto_examples/model_selection/plot_precision_recall.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d589e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# setup plot details\n",
    "colors = cycle(['navy', 'turquoise', 'darkorange', 'cornflowerblue', 'teal'])\n",
    "\n",
    "plt.figure(figsize=(7, 8))\n",
    "f_scores = np.linspace(0.2, 0.8, num=4)\n",
    "lines = []\n",
    "labels = []\n",
    "for f_score in f_scores:\n",
    "    x = np.linspace(0.01, 1)\n",
    "    y = f_score * x / (2 * x - f_score)\n",
    "    l, = plt.plot(x[y >= 0], y[y >= 0], color='gray', alpha=0.2)\n",
    "    plt.annotate('f1={0:0.1f}'.format(f_score), xy=(0.9, y[45] + 0.02))\n",
    "\n",
    "lines.append(l)\n",
    "labels.append('iso-f1 curves')\n",
    "l, = plt.plot(recall[\"micro\"], precision[\"micro\"], color='gold', lw=2)\n",
    "lines.append(l)\n",
    "labels.append('micro-average Precision-recall (area = {0:0.2f})'\n",
    "              ''.format(average_precision[\"micro\"]))\n",
    "\n",
    "for i, color in zip(range(n_classes), colors):\n",
    "    l, = plt.plot(recall[i], precision[i], color=color, lw=2)\n",
    "    lines.append(l)\n",
    "    labels.append('Precision-recall for class {0} (area = {1:0.2f})'\n",
    "                  ''.format(i, average_precision[i]))\n",
    "\n",
    "fig = plt.gcf()\n",
    "fig.subplots_adjust(bottom=0.25)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Extension of Precision-Recall curve to multi-class')\n",
    "plt.legend(lines, labels, loc=(0, -.38), prop=dict(size=14))\n",
    "\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae97c9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c9eff6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060b905f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf93438b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77237f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_dis_pred_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d40a024",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_naive = np.full(y_dis_valid.shape, 2)\n",
    "accuracy_score(y_dis_valid, y_pred_naive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b70be5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7ab884",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_dis_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a6aa34",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Masking(mask_value=special_value, input_shape=(cut_off, len(related_features) -1)))\n",
    "model.add(keras.layers.LSTM(32))\n",
    "model.add(keras.layers.Dense(3, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7869774a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer=keras.optimizers.Adam(learning_rate=0.01), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3042da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(x = x_train, y = y_dis_train, \n",
    "                    validation_data = (x_valid, y_dis_valid),\n",
    "                    epochs = 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604a3917",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_dis_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7acb857e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_dis_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cab9476",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
